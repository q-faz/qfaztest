from fastapi import FastAPI, APIRouter, File, UploadFile, HTTPException
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
# from motor.motor_asyncio import AsyncIOMotorClient  # Comentado para deploy mais r√°pido
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime
import pandas as pd
import numpy as np
import tempfile
import json
import io
import re

ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# MongoDB connection (comentado para deploy mais r√°pido)
# mongo_url = os.environ.get('MONGO_URL', '')
# client = AsyncIOMotorClient(mongo_url) if mongo_url else None
# db = client[os.environ.get('DB_NAME', '')] if client else None

# Create the main app without a prefix
app = FastAPI()

# Health check endpoint for Docker
@app.get("/health")
async def health_check():
    """Health check endpoint para verificar se a aplica√ß√£o est√° funcionando"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "service": "Q-FAZ Backend",
        "version": "1.0.0"
    }

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")

# Models
class ProcessingJob(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    status: str = "processing"  # processing, completed, failed
    message: str = ""
    processed_records: int = 0
    total_records: int = 0
    created_at: datetime = Field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None
    result_file: Optional[str] = None

class ReportSummary(BaseModel):
    bank_name: str
    total_records: int
    duplicates_removed: int
    status_distribution: Dict[str, int]
    mapped_records: int = 0
    unmapped_records: int = 0

# ===== MAPEAMENTOS COMPLETOS MELHORADOS =====

# Status normalization - Mapeamento COMPLETO
STATUS_MAPPING = {
    # ===== PAGO variants ===== (proposta finalizada e paga)
    "pago": "PAGO", 
    "paga": "PAGO", 
    "integrada": "PAGO", 
    "integrado": "PAGO",
    "finalizado": "PAGO", 
    "finalizada": "PAGO", 
    "finalizada / paga": "PAGO",
    "emitido": "PAGO", 
    "quitado": "PAGO", 
    "quitada": "PAGO", 
    "concluido": "PAGO", 
    "conclu√≠da": "PAGO", 
    "liberado": "PAGO", 
    "liberada": "PAGO",
    "desembolsado": "PAGO",
    "desembolsada": "PAGO",
    "valor desembolsado para a conta do cliente opera√ß√£o conclu√≠da.": "PAGO",
    "proposta integrada": "PAGO", 
    "int": "PAGO",
    "prova de vida aprovada / aguardando pagamento": "PAGO",
    "aprovado": "PAGO",
    "aprovada": "PAGO",
    "pago ao cliente": "PAGO",
    "credito liberado": "PAGO",
    
    # ===== CANCELADO variants ===== (proposta cancelada/reprovada)
    "cancelado": "CANCELADO", 
    "cancelada": "CANCELADO", 
    "reprovado": "CANCELADO", 
    "reprovada": "CANCELADO", 
    "rejeitado": "CANCELADO", 
    "rejeitada": "CANCELADO",
    "negado": "CANCELADO", 
    "negada": "CANCELADO", 
    "recusado": "CANCELADO",
    "recusada": "CANCELADO",
    "expirado": "CANCELADO", 
    "expirada": "CANCELADO", 
    "inv√°lido": "CANCELADO", 
    "inv√°lida": "CANCELADO",
    "invalido": "CANCELADO",
    "invalida": "CANCELADO",
    "cancelado devido a proposta expirada.": "CANCELADO",
    "cancelado permanentemente": "CANCELADO",
    "opera√ß√£o cancelada para desaverba√ß√£o.": "CANCELADO",
    "reprovada - mesa de formaliza√ß√£o": "CANCELADO",
    "reprovada - mesa de formalizacao": "CANCELADO",
    "proposta expirada": "CANCELADO",
    "rep": "CANCELADO", 
    "can": "CANCELADO",
    "reprovada por averbadora": "CANCELADO",
    "reprovada pelo banco": "CANCELADO",
    "nao aprovado": "CANCELADO",
    "n√£o aprovado": "CANCELADO",
    "desistencia": "CANCELADO",
    "desist√™ncia": "CANCELADO",
    "cliente desistiu": "CANCELADO",
    
    # ===== AGUARDANDO variants ===== (proposta em processamento/pendente)
    "digitada / aguardando formaliza√ß√£o": "AGUARDANDO",
    "digitada / aguardando formaliza√á√Éo": "AGUARDANDO",
    "digitada / aguardando formalizacao": "AGUARDANDO",
    "emitido/aguardando averba√ß√£o": "AGUARDANDO",
    "emitido/aguardando averbacao": "AGUARDANDO",
    "emitido/aguardando averba√£¬ß√£¬£o": "AGUARDANDO",
    "criada / aguardando link de formaliza√ß√£o": "AGUARDANDO",
    "criada / aguardando link de formalizacao": "AGUARDANDO",
    "aguardando saldo cip": "AGUARDANDO",
    "aguardando portabilidade": "AGUARDANDO",
    "aguardando prova de vida / assinatura": "AGUARDANDO",
    "aguardando prova de vida": "AGUARDANDO",
    "aguardando assinatura": "AGUARDANDO",
    "ajustar troco": "AGUARDANDO",
    "andamento": "AGUARDANDO", 
    "and": "AGUARDANDO", 
    "em andamento": "AGUARDANDO",
    "pendente": "AGUARDANDO", 
    "pendencia": "AGUARDANDO",
    "pend√™ncia": "AGUARDANDO",
    "aguardando pagamento": "AGUARDANDO",
    "aguardando": "AGUARDANDO",
    "pend√™ncia autoriza√ß√£o": "AGUARDANDO",
    "pendencia autorizacao": "AGUARDANDO",
    "pend√™ncia / documenta√ß√£o": "AGUARDANDO",
    "pendencia / documentacao": "AGUARDANDO",
    "pendente documentacao": "AGUARDANDO",
    "pendente documenta√ß√£o": "AGUARDANDO",
    "link de prova de vida e coleta de assinatura enviado para o cliente.": "AGUARDANDO",
    "pend√™ncia de autoriza√ß√£o da qitech para pagamento.": "AGUARDANDO",
    "pendente formalizacao": "AGUARDANDO",
    "pendente formaliza√ß√£o": "AGUARDANDO",
    "aguardar aumento margem": "AGUARDANDO",
    "digitada": "AGUARDANDO",
    "em aberto": "AGUARDANDO",
    "aberto": "AGUARDANDO",
    "aberta": "AGUARDANDO",
    "formaliza√ß√£o": "AGUARDANDO",
    "formalizacao": "AGUARDANDO",
    "checagem - mesa formaliza√ß√£o": "AGUARDANDO",
    "checagem - mesa formalizacao": "AGUARDANDO",
    "aguarda form dig web": "AGUARDANDO",
    "analise corban": "AGUARDANDO",
    "an√°lise corban": "AGUARDANDO",
    "em analise": "AGUARDANDO",
    "em an√°lise": "AGUARDANDO",
    "analise": "AGUARDANDO",
    "an√°lise": "AGUARDANDO",
    "processando": "AGUARDANDO",
    "em processamento": "AGUARDANDO",
    "cadastrada": "AGUARDANDO",
    "cadastrado": "AGUARDANDO",
    "nova": "AGUARDANDO",
    "novo": "AGUARDANDO",
    "enviado": "AGUARDANDO",
    "enviada": "AGUARDANDO",
    "aguardando averbacao": "AGUARDANDO",
    "aguardando averba√ß√£o": "AGUARDANDO"
}

# Tipos de opera√ß√£o padronizados
OPERATION_TYPES = {
    "MARGEM LIVRE (NOVO)": "MARGEM LIVRE (NOVO)",
    "MARGEM LIVRE": "MARGEM LIVRE (NOVO)", 
    "margem livre (novo)": "MARGEM LIVRE (NOVO)",
    "PORTABILIDADE": "PORTABILIDADE",
    "PORTABILIDADE + REFIN": "PORTABILIDADE + REFIN",
    "REFINANCIAMENTO": "REFINANCIAMENTO",
    "REFINANCIAMENTO DA PORTABILIDADE": "REFINANCIAMENTO DA PORTABILIDADE",
    "EMPR√âSTIMO COMPLEMENTAR": "EMPR√âSTIMO COMPLEMENTAR",
    "Saque FGTS": "MARGEM LIVRE (NOVO)",
    "Consignado FGTS": "MARGEM LIVRE (NOVO)",
    "Consignado INSS": "MARGEM LIVRE (NOVO)",
    "Portabilidade + Refin": "PORTABILIDADE + REFIN",
    "Refinanciamento": "REFINANCIAMENTO",
    "Cart√£o c/ saque": "CART√ÉO C/ SAQUE",
    "Cart√£o c/ saque complementar √† vista": "CART√ÉO C/ SAQUE COMPLEMENTAR √Ä VISTA"
}

# Global storage for processing state
processing_jobs = {}
storm_data_global = {}

# üåç FUN√á√ïES GLOBAIS DE FORMATA√á√ÉO (aplicadas a TODOS os bancos)

def format_cpf_global(cpf_str):
    """Formata CPF para o padr√£o brasileiro: 000.000.000-00
    IMPORTANTE: N√ÉO formata c√≥digos de usu√°rio que cont√™m underscore!
    """
    if not cpf_str:
        return ""
    
    cpf_clean = str(cpf_str).strip()
    
    # üö´ N√ÉO FORMATAR c√≥digos de usu√°rio que cont√™m underscore!
    # Ex: "39891947807_901064" deve manter formato original sem underscore
    if '_' in cpf_clean:
        # √â um c√≥digo de usu√°rio, n√£o CPF - remover underscore e retornar
        return cpf_clean.replace('_', '')
    
    # Remover tudo que n√£o √© n√∫mero
    cpf_numbers = ''.join(filter(str.isdigit, cpf_clean))
    
    # Verificar se tem 11 d√≠gitos
    if len(cpf_numbers) != 11:
        # Se n√£o tem 11 d√≠gitos, retornar original
        return cpf_clean
    
    # Formatar: 000.000.000-00
    cpf_formatted = f"{cpf_numbers[0:3]}.{cpf_numbers[3:6]}.{cpf_numbers[6:9]}-{cpf_numbers[9:11]}"
    return cpf_formatted

def format_value_brazilian(value_str):
    """Formata valores monet√°rios para o padr√£o brasileiro: 1.255,00"""
    if not value_str or str(value_str).strip() in ['', 'nan', 'None', 'null', 'NaN']:
        return "0,00"
    
    try:
        # Limpar o valor (remover espa√ßos, moeda, etc.)
        clean_value = str(value_str).strip().replace('R$', '').replace(' ', '').replace('\xa0', '')
        
        # Se est√° vazio ap√≥s limpeza
        if not clean_value or clean_value == '0':
            return "0,00"
        
        # Se j√° est√° no formato brasileiro correto (X.XXX,XX), manter
        if ',' in clean_value:
            parts = clean_value.split(',')
            if len(parts) == 2 and len(parts[1]) == 2:
                # Verificar se parte inteira tem pontos como separador de milhar
                if '.' in parts[0] or parts[0].isdigit():
                    return clean_value
        
        # Remover pontos que s√£o separadores de milhar no formato brasileiro
        # mas manter o √∫ltimo ponto se for decimal
        if ',' not in clean_value and '.' in clean_value:
            # Formato americano: 1234.56 ou 1,234.56
            clean_value = clean_value.replace(',', '')  # Remove v√≠rgulas (separador de milhar americano)
            parts = clean_value.split('.')
            integer_part = parts[0]
            decimal_part = parts[1][:2] if len(parts) > 1 else "00"
        elif ',' in clean_value:
            # Formato brasileiro: 1.234,56 ou j√° est√° com v√≠rgula decimal
            parts = clean_value.replace('.', '').split(',')  # Remove pontos, split por v√≠rgula
            integer_part = parts[0]
            decimal_part = parts[1][:2] if len(parts) > 1 else "00"
        else:
            # Sem decimal, assumir valor inteiro
            integer_part = clean_value.replace('.', '').replace(',', '')
            decimal_part = "00"
        
        # Garantir que decimal tenha 2 d√≠gitos
        if len(decimal_part) == 1:
            decimal_part += "0"
        elif len(decimal_part) == 0:
            decimal_part = "00"
        
        # Converter para float
        float_value = float(f"{integer_part}.{decimal_part}")
        
        # Formatar no padr√£o brasileiro: 1.255,00
        if float_value >= 1000:
            # Valores >= 1000: usar ponto para milhar
            formatted = f"{float_value:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
        else:
            # Valores < 1000: apenas v√≠rgula decimal
            formatted = f"{float_value:.2f}".replace('.', ',')
        
        return formatted
        
    except (ValueError, TypeError) as e:
        logging.warning(f"‚ö†Ô∏è Erro ao formatar valor '{value_str}': {e}")
        return str(value_str).strip()  # Retornar original se houver erro

def format_percentage_brazilian(percentage_str):
    """Formata percentuais para o padr√£o brasileiro: 1,85%"""
    if not percentage_str or str(percentage_str).strip() in ['', 'nan', 'None', 'null', 'NaN']:
        return "0,00%"
    
    try:
        # Limpar o valor (remover %, espa√ßos, etc.)
        clean_value = str(percentage_str).strip().replace('%', '').replace(' ', '')
        
        if not clean_value or clean_value == '0':
            return "0,00%"
        
        # Converter para float
        percentage_float = float(clean_value.replace(',', '.'))
        
        # Formatar no padr√£o brasileiro: X,XX%
        formatted = f"{percentage_float:.2f}".replace('.', ',')
        return f"{formatted}%"
        
    except (ValueError, TypeError) as e:
        logging.warning(f"‚ö†Ô∏è Erro ao formatar percentual '{percentage_str}': {e}")
        return f"{str(percentage_str).strip()}%"

def clean_special_characters(text):
    """
    Remove ou substitui caracteres especiais problem√°ticos que quebram o processamento
    Trata problemas de encoding e normaliza texto para processamento seguro
    """
    if not text or pd.isna(text):
        return ""
    
    text_str = str(text).strip()

    # Primeiro tentar corrigir casos de mojibake comum (ex: 'CR√Ø¬ø¬ΩDITO' -> 'CR√âDITO')
    def fix_mojibake(s: str) -> str:
        # Tentar desfazer m√∫ltiplas camadas de codifica√ß√£o (at√© 3 itera√ß√µes)
        candidate = s
        for _ in range(3):
            try:
                decoded = candidate.encode('latin-1', errors='replace').decode('utf-8', errors='replace')
                # Se a decodifica√ß√£o produziu mais caracteres v√°lidos, adotar
                if len(re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø]", decoded)) > len(re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø]", candidate)):
                    candidate = decoded
                    continue
            except Exception:
                pass

            try:
                decoded2 = candidate.encode('utf-8', errors='replace').decode('latin-1', errors='replace')
                if len(re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø]", decoded2)) > len(re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø]", candidate)):
                    candidate = decoded2
                    continue
            except Exception:
                pass

            # Se nenhuma melhoria, parar
            break

        best = candidate

        # Substitui√ß√µes simples e seguras residuais
        safe_replacements = {'‚Äì': '-', '‚Äî': '-', '‚Ä¢': '-', '‚Ä¶': '...', '\u00A0': ' ', '\t': ' ', '\r': ''}
        for k, v in safe_replacements.items():
            best = best.replace(k, v)

        residual = {'√É¬°': '√°', '√É¬©': '√©', '√É¬™': '√™', '√É¬ß': '√ß', '√É¬£': '√£', '√É¬µ': '√µ', '√É¬∫': '√∫', '√É¬≥': '√≥', '√É¬≠': '√≠'}
        for k, v in residual.items():
            if k in best:
                best = best.replace(k, v)

        # Hotfix: Mapeamentos espec√≠ficos para casos comuns conhecidos
        hotfix_map = {
            'CR√Ø¬ø¬ΩDITO DO TRABALHADOR': 'CR√âDITO DO TRABALHADOR',
            'CR√Ø¬ø¬ΩDITO': 'CR√âDITO',
            'CR√É‚Ä∞DITO DO TRABALHADOR': 'CR√âDITO DO TRABALHADOR',
            'CR√É‚Ä∞DITO': 'CR√âDITO',
            'TRABALHADOR√Ø¬ø¬Ω': 'TRABALHADOR',
            'Cart√Ø¬ø¬Ωo': 'Cart√£o',
            'opera√Ø¬ø¬Ω√Ø¬ø¬Ωo': 'opera√ß√£o',
            'situa√Ø¬ø¬Ω√Ø¬ø¬Ωo': 'situa√ß√£o',
            '√≥rg√Ø¬ø¬Ωo': '√≥rg√£o',
            '√ìRG√Ø¬ø¬ΩO': '√ìRG√ÉO',
            'org√Ø¬ø¬Ωo': '√≥rg√£o',
            'ORG√Ø¬ø¬ΩO': '√ìRG√ÉO',
            '√Ø¬ø¬ΩRG√ÉO': '√ìRG√ÉO',
            '√Ø¬ø¬Ωrg√£o': '√≥rg√£o',
            'empr√©stimo': 'empr√©stimo',  # j√° correto
            'financiamento': 'financiamento'  # j√° correto
        }
        
        # Aplicar mapeamentos diretos primeiro
        for broken, fixed in hotfix_map.items():
            if broken in best:
                best = best.replace(broken, fixed)
        
        # Sequ√™ncias gen√©ricas comuns
        generic_fixes = {
            '√Ø¬ø¬Ω': '√â',  # replacement char comum para √â
            '√É¬©': '√©',   # mojibake comum para √©
            '√É¬°': '√°',   # mojibake comum para √°
            '√É¬ß': '√ß',   # mojibake comum para √ß
            '√É¬£': '√£',   # mojibake comum para √£
            '√É¬µ': '√µ'    # mojibake comum para √µ
        }
        
        for broken, fixed in generic_fixes.items():
            best = best.replace(broken, fixed)
        
        # Remover caracteres de replacement residual
        best = best.replace('\ufffd', '')
        return best

    text_str = fix_mojibake(text_str)
    
    if not text_str:
        return ""
    
    # Dicion√°rio de substitui√ß√µes para caracteres problem√°ticos comuns
    char_replacements = {
        # Caracteres de controle e especiais problem√°ticos
        '^': '',
        '~': '',
        '`': '',
        '¬¥': '',
        '¬®': '',
        '¬∞': '',
        '¬∫': '',
        '¬™': '',
        # Aspas problem√°ticas
        '"': '"',
        '"': '"', 
        ''': "'",
        ''': "'",
        # S√≠mbolos matem√°ticos problem√°ticos  
        '¬±': '+/-',
        '√ó': 'x',
        '√∑': '/',
        # Outros s√≠mbolos problem√°ticos
        '¬ß': 'paragrafo',
        '¬∂': '',
        '‚Ä†': '',
        '‚Ä°': '',
        '‚Ä¢': '-',
        '‚Ä¶': '...',
        '‚Äì': '-',
        '‚Äî': '-',
        # Caracteres de moeda problem√°ticos
        '¬¢': 'centavos',
        '¬£': 'libras',
        '¬•': 'yen',
        '‚Ç¨': 'euro',
    }
    
    # Aplicar substitui√ß√µes
    cleaned_text = text_str
    for old_char, new_char in char_replacements.items():
        cleaned_text = cleaned_text.replace(old_char, new_char)
    
    # Remover caracteres de controle (ASCII 0-31 exceto \t, \n, \r)
    import re
    cleaned_text = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', cleaned_text)
    
    # Normalizar m√∫ltiplos espa√ßos em um s√≥
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    
    # Logging para debug quando h√° mudan√ßas significativas
    if len(text_str) != len(cleaned_text) or text_str != cleaned_text:
        logging.info(f"üßπ Texto limpo: '{text_str[:50]}...' ‚Üí '{cleaned_text[:50]}...'")
    
    return cleaned_text

def apply_character_cleaning_to_dataframe(df: pd.DataFrame, filename: str = "") -> pd.DataFrame:
    """
    Aplica limpeza de caracteres especiais em todas as colunas de texto do DataFrame
    """
    if df.empty:
        return df
    
    cleaned_columns = 0
    for column in df.columns:
        if df[column].dtype == 'object':  # Colunas de texto
            df[column] = df[column].astype(str).apply(clean_special_characters)
            cleaned_columns += 1
    
    if cleaned_columns > 0 and filename:
        logging.info(f"‚úÖ Limpeza aplicada ao arquivo {filename}: {cleaned_columns} colunas processadas")
    
    return df

def load_organ_mapping():
    """Carrega o mapeamento de √≥rg√£os do arquivo CSV atualizado - MELHORADO sem depend√™ncia de usu√°rio"""
    try:
        # Ler o arquivo de mapeamento atualizado usando caminho relativo
        csv_path = os.path.join(os.path.dirname(__file__), 'relat_orgaos.csv')
        
        # Verificar se arquivo existe antes de tentar ler
        if not os.path.exists(csv_path):
            logging.warning(f"Arquivo relat_orgaos.csv n√£o encontrado em: {csv_path}")
            logging.warning(f"Diret√≥rio atual: {os.getcwd()}")
            logging.warning(f"Arquivos no diret√≥rio backend: {os.listdir(os.path.dirname(__file__))}")
            return {}, {}, {}, {}
            
        # Tentar diferentes encodings com tratamento robusto de caracteres especiais
        encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-16']
        df = None
        
        for encoding in encodings_to_try:
            try:
                df = pd.read_csv(csv_path, encoding=encoding, sep=';')
                logging.info(f"‚úÖ Arquivo CSV carregado com encoding: {encoding}")
                break
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Erro ao ler CSV com encoding {encoding}: {e}")
                continue
        
        if df is None:
            # Fallback final 
            try:
                df = pd.read_csv(csv_path, encoding='utf-8', sep=';')
                logging.warning("‚ö†Ô∏è Arquivo CSV carregado com encoding UTF-8 como fallback")
            except Exception as e:
                logging.error(f"‚ùå Erro cr√≠tico ao ler CSV: {e}")
                return {}, {}, {}, {}
        
        # Formato REAL do arquivo: BANCO;ORG√ÉO STORM;TABELA BANCO;CODIGO TABELA STORM;OPERA√á√ÉO STORM;TAXA STORM
        # NOTA: Campo USUARIO DIGITADOR STORM foi removido para evitar problemas futuros com mudan√ßas de usu√°rio
        mapping = {}
        detailed_mapping = {}  # Mapeamento por BANCO|ORG√ÉO|OPERA√á√ÉO (m√∫ltiplas op√ß√µes)
        tabela_mapping = {}     # Mapeamento por BANCO|ORG√ÉO|OPERA√á√ÉO|TABELA (mais espec√≠fico)
        bank_organ_mapping = {} # Mapeamento por BANCO|ORG√ÉO (mais gen√©rico para fallback)
        
        for _, row in df.iterrows():
            banco = ' '.join(str(row.get('BANCO', '')).strip().upper().split())
            # FIX ENCODING: Usar √çNDICE em vez de nome da coluna para evitar problemas de encoding
            # Coluna 0: BANCO
            # Coluna 1: ORG√ÉO
            # Coluna 2: TABELA BANCO
            # Coluna 3: CODIGO TABELA STORM
            # Coluna 4: OPERA√á√ÉO STORM
            # Coluna 5: TAXA STORM
            orgao = ' '.join(str(row.iloc[1] if len(row) > 1 else '').strip().upper().split())
            # CR√çTICO: Normalizar tabela removendo TODOS os espa√ßos extras (incluindo espa√ßos iniciais)
            tabela_banco_raw = str(row.iloc[2] if len(row) > 2 else '').strip()
            tabela_banco = ' '.join(tabela_banco_raw.split())  # Remove espa√ßos extras completamente
            codigo_tabela = str(row.iloc[3] if len(row) > 3 else '').strip()
            operacao_storm = str(row.iloc[4] if len(row) > 4 else '').strip()
            taxa_storm = str(row.iloc[5] if len(row) > 5 else '').strip()
            
            if banco and banco != 'NAN' and codigo_tabela and codigo_tabela != 'NAN':
                # Mapeamento simples (primeira ocorr√™ncia por hierarquia)
                if banco not in mapping:
                    mapping[banco] = {}
                if orgao and orgao != 'NAN':
                    if orgao not in mapping[banco]:
                        mapping[banco][orgao] = {}
                    if operacao_storm and operacao_storm != 'NAN':
                        if operacao_storm not in mapping[banco][orgao]:
                            mapping[banco][orgao][operacao_storm] = codigo_tabela
                
                # Mapeamento detalhado por BANCO|ORG√ÉO|OPERA√á√ÉO (guarda todas as op√ß√µes)
                key = f"{banco}|{orgao}|{operacao_storm.upper()}"
                if key not in detailed_mapping:
                    detailed_mapping[key] = []
                detailed_mapping[key].append({
                    'codigo_tabela': codigo_tabela,
                    'tabela_banco': tabela_banco,
                    'orgao_storm': orgao,  # ORG√ÉO padronizado Storm
                    'operacao_storm': operacao_storm,
                    'taxa_storm': taxa_storm
                })
                
                # Mapeamento por TABELA (mais espec√≠fico e confi√°vel)
                if tabela_banco and tabela_banco != 'NAN':
                    # CR√çTICO: Salvar a chave com tabela em UPPERCASE para matching consistente
                    tabela_key = f"{banco}|{orgao}|{operacao_storm.upper()}|{tabela_banco.upper()}"
                    tabela_mapping[tabela_key] = {
                        'codigo_tabela': codigo_tabela,
                        'tabela_banco': tabela_banco,  # Manter original para exibi√ß√£o
                        'orgao_storm': orgao,  # ORG√ÉO padronizado Storm
                        'operacao_storm': operacao_storm,
                        'taxa_storm': taxa_storm
                    }
                    
                    # üîç DEBUG: Log primeiras 3 linhas FACTA
                    if 'FACTA' in banco and len([k for k in tabela_mapping.keys() if 'FACTA' in k]) <= 3:
                        logging.info(f"üîç DEBUG TABELA_MAPPING FACTA: Chave='{tabela_key}'")
                        logging.info(f"   Valores: codigo={codigo_tabela}, orgao={orgao}, operacao={operacao_storm}, taxa={taxa_storm}")
                
                # Mapeamento gen√©rico por BANCO|ORG√ÉO (para fallback quando opera√ß√£o n√£o bate exatamente)
                bank_organ_key = f"{banco}|{orgao}"
                if bank_organ_key not in bank_organ_mapping:
                    bank_organ_mapping[bank_organ_key] = []
                bank_organ_mapping[bank_organ_key].append({
                    'codigo_tabela': codigo_tabela,
                    'tabela_banco': tabela_banco,
                    'orgao_storm': orgao,
                    'operacao_storm': operacao_storm,
                    'taxa_storm': taxa_storm
                })
        
        logging.info(f"Mapeamento carregado: {len(mapping)} bancos, {len(detailed_mapping)} combina√ß√µes banco+orgao+operacao, {len(tabela_mapping)} por tabela espec√≠fica, {len(bank_organ_mapping)} por banco+orgao")
        
        # üîç DEBUG: Mostrar primeiras chaves FACTA no TABELA_MAPPING
        facta_keys = [k for k in tabela_mapping.keys() if 'FACTA' in k]
        if facta_keys:
            logging.info(f"üîç DEBUG: Primeiras {min(3, len(facta_keys))} chaves FACTA no TABELA_MAPPING:")
            for key in facta_keys[:3]:
                logging.info(f"   Chave: '{key}'")
                logging.info(f"   Dados: {tabela_mapping[key]}")
        
        return mapping, detailed_mapping, tabela_mapping, bank_organ_mapping
    except Exception as e:
        logging.error(f"Erro ao carregar mapeamento de √≥rg√£os: {str(e)}")
        return {}, {}, {}, {}

# Carregar mapeamento global - ATUALIZADO sem depend√™ncia de usu√°rio
try:
    ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING = load_organ_mapping()
    logging.info("‚úÖ Mapeamento de √≥rg√£os carregado com sucesso!")
except Exception as e:
    logging.error(f"‚ùå Erro ao carregar mapeamento: {e}")
    ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING = {}, {}, {}, {}

def reload_organ_mapping():
    """Recarrega o mapeamento de √≥rg√£os para pegar novos c√≥digos de tabela adicionados"""
    global ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING
    try:
        logging.info("üîÑ Recarregando mapeamento de √≥rg√£os...")
        ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING = load_organ_mapping()
        logging.info("‚úÖ Mapeamento recarregado com sucesso!")
        return True
    except Exception as e:
        logging.error(f"‚ùå Erro ao recarregar mapeamento: {str(e)}")
        return False

def read_file_optimized(file_content: bytes, filename: str) -> pd.DataFrame:
    """Leitura otimizada de arquivos com m√∫ltiplas tentativas e melhor detec√ß√£o de separadores"""
    file_ext = filename.lower().split('.')[-1]
    
    try:
        if file_ext == 'csv':
            # Tentar diferentes encodings e separadores com tratamento robusto
            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-16']
            for encoding in encodings_to_try:
                for sep in [';', ',', '\t', '|']:
                    try:
                        df = pd.read_csv(
                            io.BytesIO(file_content), 
                            encoding=encoding, 
                            sep=sep,
                            low_memory=False,
                            na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                            dtype=str  # Manter tudo como string inicialmente
                        )
                        
                        # Verificar se temos m√∫ltiplas colunas ou se precisa dividir
                        if len(df.columns) == 1 and sep != ';':
                            # Tentar dividir a √∫nica coluna por diferentes separadores
                            first_col = df.columns[0]
                            if ';' in first_col or ',' in first_col or '\t' in first_col:
                                continue  # Tentar pr√≥ximo separador
                        
                        if len(df.columns) > 1 or (len(df.columns) == 1 and len(df) > 0):
                            logging.info(f"Arquivo lido com encoding {encoding} e separador '{sep}', {len(df.columns)} colunas")
                            return apply_character_cleaning_to_dataframe(df, filename)
                            
                    except (UnicodeDecodeError, pd.errors.ParserError, Exception) as e:
                        continue
            
            # Se chegou aqui, tentar √∫ltimo recurso: detectar automaticamente o separador
            try:
                content_str = file_content.decode('utf-8', errors='ignore')
                # Verificar qual separador aparece mais nas primeiras linhas
                first_lines = content_str.split('\n')[:5]
                separators = {';': 0, ',': 0, '\t': 0, '|': 0}
                
                for line in first_lines:
                    for sep in separators:
                        separators[sep] += line.count(sep)
                
                best_sep = max(separators, key=separators.get)
                if separators[best_sep] > 0:
                    df = pd.read_csv(
                        io.BytesIO(file_content), 
                        encoding='utf-8', 
                        sep=best_sep,
                        low_memory=False,
                        na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                        dtype=str,
                        on_bad_lines='skip'
                    )
                    logging.info(f"Arquivo lido com separador auto-detectado '{best_sep}', {len(df.columns)} colunas")
                    return df
            except Exception as e:
                logging.error(f"Erro na detec√ß√£o autom√°tica: {str(e)}")
            
            raise ValueError("N√£o foi poss√≠vel ler o arquivo CSV com nenhum separador")
            
        elif file_ext in ['xlsx', 'xls']:
            # Verificar se √© arquivo PAULISTA - precisa pular primeiras linhas
            filename_lower = filename.lower()
            is_paulista = any(indicator in filename_lower for indicator in ['paulista', 'af5eebb7'])
            
            logging.info(f"üîç Arquivo: {filename_lower}, √â PAULISTA? {is_paulista}")
            
            if is_paulista:
                logging.info(f"üè¶ Detectado arquivo PAULISTA: {filename}, aplicando leitura especial...")
                try:
                    # PAULISTA: pular primeiras 2 linhas, usar linha 3 como cabe√ßalho
                    df = pd.read_excel(
                        io.BytesIO(file_content),
                        skiprows=2,  # Pula logo e linha vazia
                        na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                        dtype=str
                    )
                    logging.info(f"üè¶ PAULISTA lido com skip=2: {len(df.columns)} colunas, primeiras: {list(df.columns)[:5]}")
                    return apply_character_cleaning_to_dataframe(df, filename)
                except Exception as e:
                    logging.error(f"‚ùå Erro na leitura especial PAULISTA: {str(e)}")
                    # Fallback para leitura normal
            
            # Tentar ler com diferentes configura√ß√µes
            try:
                # Primeiro tentar leitura normal
                df = pd.read_excel(
                    io.BytesIO(file_content),
                    na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                    dtype=str
                )
                
                # Se o DataFrame est√° vazio ou tem s√≥ NaN, tentar pular linhas
                if df.empty or df.dropna(how='all').empty:
                    raise ValueError("DataFrame vazio ap√≥s primeira tentativa")
                
                # PAULISTA DETECTION: Verificar se tem "Rela√ß√£o de Propostas" nas primeiras linhas
                if not df.empty and len(df) > 0:
                    first_few_rows = df.head(3).astype(str)
                    content_text = ' '.join(first_few_rows.values.flatten()).lower()
                    
                    if 'rela√ß√£o de propostas' in content_text or 'anal√≠tico' in content_text:
                        logging.info(f"üè¶ PAULISTA detectado por conte√∫do! Aplicando leitura especial...")
                        try:
                            # Recarregar pulando primeiras linhas
                            df_paulista = pd.read_excel(
                                io.BytesIO(file_content),
                                skiprows=2,  # Pula logo e "Rela√ß√£o de Propostas"
                                na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                                dtype=str
                            )
                            logging.info(f"üè¶ PAULISTA relido: {len(df_paulista.columns)} colunas: {list(df_paulista.columns)[:5]}")
                            return apply_character_cleaning_to_dataframe(df_paulista, filename)
                        except Exception as e:
                            logging.error(f"‚ùå Erro na releitura PAULISTA: {str(e)}")
                
                # Verificar se a primeira linha parece ser cabe√ßalho de metadados
                # (ex: "Relat√≥rio de...", "Banco:", etc.)
                if not df.empty:
                    first_row = df.iloc[0].astype(str).str.lower()
                    metadata_indicators = ['relat√≥rio', 'relatorio', 'banco:', 'data:', 'per√≠odo', 
                                          'periodo', 'extra√ß√£o', 'extracao', 'total:', 'p√°gina']
                    
                    # Se encontrar indicadores de metadados, tentar pular linhas
                    if any(indicator in ' '.join(first_row.values) for indicator in metadata_indicators):
                        logging.info("Detectado cabe√ßalho de metadados, tentando pular linhas...")
                        
                        # Tentar pular de 1 a 10 linhas
                        for skip_rows in range(1, 11):
                            try:
                                df_attempt = pd.read_excel(
                                    io.BytesIO(file_content),
                                    skiprows=skip_rows,
                                    na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                                    dtype=str
                                )
                                
                                # Verificar se agora temos dados v√°lidos
                                if not df_attempt.empty and len(df_attempt.columns) > 3:
                                    # Verificar se tem mais dados que a tentativa anterior
                                    valid_rows = df_attempt.dropna(how='all')
                                    if len(valid_rows) > 0:
                                        logging.info(f"Excel lido pulando {skip_rows} linhas, {len(df_attempt.columns)} colunas")
                                        return apply_character_cleaning_to_dataframe(df_attempt, filename)
                            except:
                                continue
                
                # Se chegou aqui, usar o DataFrame original
                logging.info(f"Excel lido normalmente, {len(df.columns)} colunas")
                return apply_character_cleaning_to_dataframe(df, filename)
                
            except Exception as e:
                # √öltima tentativa: ler todas as sheets e pegar a primeira com dados
                logging.warning(f"Tentativa normal falhou: {str(e)}, tentando ler todas as sheets...")
                try:
                    excel_file = pd.ExcelFile(io.BytesIO(file_content))
                    for sheet_name in excel_file.sheet_names:
                        try:
                            df = pd.read_excel(
                                io.BytesIO(file_content),
                                sheet_name=sheet_name,
                                na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                                dtype=str
                            )
                            if not df.empty and len(df.columns) > 1:
                                logging.info(f"Excel lido da sheet '{sheet_name}', {len(df.columns)} colunas")
                                return apply_character_cleaning_to_dataframe(df, filename)
                        except:
                            continue
                except Exception as sheet_error:
                    logging.error(f"Erro ao ler sheets: {str(sheet_error)}")
                
                raise ValueError(f"N√£o foi poss√≠vel ler o arquivo Excel: {str(e)}")
        elif file_ext in ['txt']:
            # TXT pode ser CSV com separadores variados
            try:
                # Tentar como CSV primeiro
                content_str = file_content.decode('utf-8')
            except:
                try:
                    content_str = file_content.decode('latin-1')
                except:
                    content_str = file_content.decode('cp1252')
            
            # Detectar separador
            first_lines = content_str.split('\n')[:5]
            separators = {';': 0, ',': 0, '\t': 0, '|': 0}
            
            for line in first_lines:
                for sep in separators:
                    separators[sep] += line.count(sep)
            
            best_sep = max(separators, key=separators.get)
            if separators[best_sep] > 0:
                df = pd.read_csv(
                    io.BytesIO(file_content), 
                    encoding='utf-8' if file_content.decode('utf-8', errors='ignore') else 'latin-1',
                    sep=best_sep,
                    low_memory=False,
                    na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                    dtype=str,
                    on_bad_lines='skip'
                )
                logging.info(f"Arquivo TXT lido com separador '{best_sep}', {len(df.columns)} colunas")
                return df
            else:
                raise ValueError("Arquivo TXT sem separadores reconhec√≠veis")
        else:
            raise ValueError(f"Formato n√£o suportado: {file_ext}. Formatos aceitos: CSV, XLSX, XLS, TXT")
            
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Erro ao ler arquivo {filename}: {str(e)}")

def detect_bank_type_enhanced(df: pd.DataFrame, filename: str) -> str:
    """Detec√ß√£o melhorada de tipo de banco baseada na estrutura real dos arquivos"""
    filename_lower = filename.lower()
    df_columns = [str(col).lower().strip() for col in df.columns]
    
    logging.info(f"Detectando banco para arquivo: {filename}")
    logging.info(f"Colunas encontradas: {df_columns[:10]}...")  # Mostrar apenas primeiras 10
    
    # Detec√ß√£o por nome do arquivo - mais confi√°vel
    if any(indicator in filename_lower for indicator in ['storm', 'contratos', 'digitados']):
        return "STORM"
    elif 'averbai' in filename_lower:
        return "AVERBAI"
    elif any(indicator in filename_lower for indicator in ['digio', 'wfsic', 'wfi']):
        logging.info(f"‚úÖ DIGIO detectado por nome do arquivo: {filename}")
        return "DIGIO"
    elif 'prata' in filename_lower:
        return "PRATA"
    elif 'vctex' in filename_lower:
        return "VCTEX"
    elif 'daycoval' in filename_lower:
        return "DAYCOVAL"
    
    # Detec√ß√£o por estrutura de colunas espec√≠fica
    
    # Verificar se √© Storm
    storm_indicators = ['ade', 'banco empr√©stimo', 'status do contrato']
    storm_matches = sum(1 for indicator in storm_indicators if any(indicator in col for col in df_columns))
    if storm_matches >= 2:
        return "STORM"
    
    # Verificar se √© AVERBAI (tem colunas espec√≠ficas como Id, IdTableComissao)
    averbai_indicators = ['id', 'idtablecomissao', 'tipoproduto', 'loginconsultor']
    averbai_matches = sum(1 for indicator in averbai_indicators if any(indicator in col for col in df_columns))
    if averbai_matches >= 2:
        return "AVERBAI"
    
    # Verificar se √© DIGIO (melhorada - mais espec√≠fica)
    if len(df.columns) > 50 and sum(1 for col in df_columns if 'unnamed:' in col) > 20:
        # Verificar dados espec√≠ficos do Digio em m√∫ltiplas linhas
        if not df.empty:
            all_data = ""
            # Verificar primeiras 5 linhas para ser mais preciso
            for i in range(min(5, len(df))):
                row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_data += " " + row_data
                
            logging.info(f"üîç DIGIO check - dados: {all_data[:200]}...")
            
            # Indicadores √∫nicos do DIGIO (expandidos para melhor detec√ß√£o)
            digio_unique_indicators = [
                'banco digio', 'digio s.a', 'digio s/a', 'digio bank', 'digio',
                'propostas cadastradas', 'relat√≥rio de propostas', 'relatorio de propostas',
                'sic - v.', 'mprlprcd', 'proc.:', 'sist.:', 'cont.:',
                'tipo: por data', 'por data de libera√ß√£o', 'periodo:',
                'tkt', 'ticket', 'ativo', 'cancelado', 'pago', 'liberado'
            ]
            found_digio_indicators = [ind for ind in digio_unique_indicators if ind in all_data]
            
            # Verificar tamb√©m na estrutura de colunas
            digio_column_patterns = ['unnamed:', 'proposta', 'tipo_operacao', 'data_cadastro', 'situacao', 'cpf_cliente']
            digio_col_count = sum(1 for pattern in digio_column_patterns if any(pattern in str(col).lower() for col in df_columns))
            
            if found_digio_indicators:
                logging.info(f"‚úÖ DIGIO detectado por indicadores: {found_digio_indicators}")
                return "DIGIO"
                
            # Verificar estrutura t√≠pica do DIGIO (muitas colunas Unnamed)
            unnamed_count = sum(1 for col in df_columns if 'unnamed:' in str(col).lower())
            if unnamed_count >= 50:  # DIGIO tem ~105 colunas Unnamed
                # Verificar se N√ÉO √© DAYCOVAL
                daycoval_exclusive_indicators = ['banco daycoval', 'qfz solucoes', 'tp. opera√ß√£o', 'daycoval']
                found_daycoval_indicators = [ind for ind in daycoval_exclusive_indicators if ind in all_data]
                
                if not found_daycoval_indicators:
                    logging.info(f"‚úÖ DIGIO detectado por estrutura ({unnamed_count} colunas Unnamed)")
                    return "DIGIO"
    
    # Verificar se √© PRATA (tem colunas espec√≠ficas)
    prata_indicators = ['corban master', 'n√∫mero da proposta', 'prata digital', 'shake de morango']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        prata_matches = sum(1 for indicator in prata_indicators if indicator in first_row_data)
        if prata_matches >= 1:
            return "PRATA"
    
    # Verificar se √© VCTEX (tem colunas espec√≠ficas)
    vctex_indicators = ['corban master', 'n√∫mero do contrato', "it's solucoes", 'tabela vamo']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        vctex_matches = sum(1 for indicator in vctex_indicators if indicator in first_row_data)
        if vctex_matches >= 1:
            return "VCTEX"
    
    # Verificar se √© DAYCOVAL (melhorada)
    # 1. Por nome do arquivo
    if 'daycoval' in filename_lower:
        logging.info(f"‚úÖ DAYCOVAL detectado por nome do arquivo: {filename}")
        return "DAYCOVAL"
    
    # 2. Por estrutura de colunas espec√≠ficas - FORMATO CSV CORRETO
    daycoval_csv_indicators = ['proposta', 'data cadastro', 'banco', 'orgao', 'codigo tabela', 'tipo de operacao', 'numero parcelas']
    daycoval_csv_matches = sum(1 for indicator in daycoval_csv_indicators if any(indicator in col for col in df_columns))
    if daycoval_csv_matches >= 5:
        logging.info(f"‚úÖ DAYCOVAL detectado por colunas CSV: {daycoval_csv_matches} matches")
        return "DAYCOVAL"
    
    # 3. Por estrutura de colunas antigas (Unnamed)
    daycoval_column_indicators = ['cliente', 'cpf/cnpj', 'matr√≠cula', 'dt.cad.', 'dt.base', 'vlr.oper', 'prz. em meses', 'tx.am']
    daycoval_col_matches = sum(1 for indicator in daycoval_column_indicators if any(indicator in col for col in df_columns))
    if daycoval_col_matches >= 5:
        logging.info(f"‚úÖ DAYCOVAL detectado por colunas antigas: {daycoval_col_matches} matches")
        return "DAYCOVAL"
    
    # 3. Por estrutura Unnamed + conte√∫do
    unnamed_count = sum(1 for col in df_columns if 'unnamed:' in col)
    logging.info(f"üîç DAYCOVAL Check - Colunas: {len(df.columns)}, Unnamed: {unnamed_count}")
    
    if len(df.columns) > 20 and unnamed_count > 15:
        # Verificar dados espec√≠ficos do Daycoval em m√∫ltiplas linhas
        if not df.empty:
            all_data = ""
            for i in range(min(5, len(df))):
                row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_data += " " + row_data
            
            logging.info(f"üîç DAYCOVAL primeiras linhas: {all_data[:300]}")
            
            # Indicadores √∫nicos do DAYCOVAL (n√£o confundem com DIGIO)
            daycoval_unique_indicators = ['banco daycoval', 'qfz solucoes', 'tp. opera√ß√£o', 'detalhado']
            found_daycoval_indicators = [ind for ind in daycoval_unique_indicators if ind in all_data]
            
            # Verificar se N√ÉO tem indicadores do DIGIO
            digio_exclusive_indicators = ['banco digio', 'digio s.a', 'tkt', 'status: ativo', 'status: cancelado', 'status: pago']
            found_digio_indicators = [ind for ind in digio_exclusive_indicators if ind in all_data]
            
            if found_daycoval_indicators and not found_digio_indicators:
                logging.info(f"‚úÖ DAYCOVAL detectado! Indicadores √∫nicos: {found_daycoval_indicators}")
                return "DAYCOVAL"
            else:
                logging.info(f"‚ö†Ô∏è DAYCOVAL n√£o detectado - indicadores DAYCOVAL: {found_daycoval_indicators}, indicadores DIGIO: {found_digio_indicators}")
    
    # Detec√ß√£o por nome do arquivo SANTANDER (prioridade)
    if 'santander' in filename_lower:
        logging.info(f"‚úÖ SANTANDER detectado por nome do arquivo")
        return "SANTANDER"
    
    # Verificar se √© SANTANDER por colunas espec√≠ficas
    # Colunas reais do SANTANDER: COD, COD. BANCO, CPF, CLIENTE, CONVENIO, PRODUTO, STATUS, etc.
    santander_column_indicators = ['cod. banco', 'convenio', 'produto', 'qtde parcelas', 'valor bruto', 'valor liquido', 'cod digitador']
    santander_col_matches = sum(1 for indicator in santander_column_indicators if any(indicator in col for col in df_columns))
    
    if santander_col_matches >= 4:
        logging.info(f"‚úÖ SANTANDER detectado por colunas ({santander_col_matches} matches)")
        return "SANTANDER"
    
    # Verificar se tem "SANTANDER" nos dados (formato relat√≥rio final)
    if not df.empty:
        banco_col = next((col for col in df.columns if 'banco' in str(col).lower()), None)
        if banco_col and any('SANTANDER' in str(val).upper() for val in df[banco_col].dropna().head(10)):
            logging.info(f"‚úÖ SANTANDER detectado por conte√∫do da coluna BANCO")
            return "SANTANDER"
    
    # Verificar se √© CREFAZ (PRIORIDADE ALTA - antes do MERCANTIL)
    # 1. Por nome do arquivo
    if 'crefaz' in filename_lower:
        logging.info(f"‚úÖ CREFAZ detectado por nome do arquivo: {filename}")
        return "CREFAZ"
    
    # 2. Por colunas espec√≠ficas do CREFAZ
    crefaz_column_indicators = ['data cadastro', 'n√∫mero da proposta', 'cpf', 'cliente', 'cidade', 'valor liberado', 'prazo', 'status', 'agente', 'cod opera√ß√£o', 'produto']
    crefaz_col_matches = sum(1 for indicator in crefaz_column_indicators if any(indicator in col for col in df_columns))
    if crefaz_col_matches >= 5:
        logging.info(f"‚úÖ CREFAZ detectado por colunas ({crefaz_col_matches} matches)")
        return "CREFAZ"
    
    # 3. Por conte√∫do espec√≠fico (indicadores √∫nicos de energia/boleto)
    if not df.empty:
        # Verificar nas primeiras 3 linhas para indicadores espec√≠ficos do CREFAZ
        all_data = ""
        for i in range(min(3, len(df))):
            row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
            all_data += " " + row_data
        
        # Indicadores √∫nicos do CREFAZ (energia, boleto, etc.)
        crefaz_unique_indicators = ['crefaz', 'energia', 'boleto', 'cpfl', 'cosern', 'celpe', 'enel', 'ener', 'bol', 'luz', 'fatura']
        found_crefaz_indicators = [ind for ind in crefaz_unique_indicators if ind in all_data]
        
        # Verificar se N√ÉO tem indicadores exclusivos do MERCANTIL
        mercantil_exclusive_indicators = ['banco mercantil do brasil', 'credfranco', 'bmb', 'codigocorrespondente', 'nomecorrespondente']
        found_mercantil_indicators = [ind for ind in mercantil_exclusive_indicators if ind in all_data]
        
        if found_crefaz_indicators and not found_mercantil_indicators:
            logging.info(f"‚úÖ CREFAZ detectado por conte√∫do √∫nico: {found_crefaz_indicators}")
            return "CREFAZ"

    # Verificar se √© MERCANTIL (Banco Mercantil do Brasil) - AP√ìS CREFAZ
    # 1. Por nome do arquivo
    if 'mercantil' in filename_lower or 'bmb' in filename_lower or 'credfranco' in filename_lower:
        logging.info(f"‚úÖ MERCANTIL detectado por nome do arquivo: {filename}")
        return "MERCANTIL"
    
    # 2. Por estrutura de colunas espec√≠ficas (FORMATO REAL DO MERCANTIL)
    # Indicadores principais do formato real
    mercantil_indicators = ['numeroproposta', 'codigoconvenio', 'nomeconvenio', 'codigoproduto', 'nomeproduto', 'modalidadecredito', 'situacaoproposta']
    mercantil_matches = sum(1 for indicator in mercantil_indicators if any(indicator in col for col in df_columns))
    
    # Indicadores espec√≠ficos √∫nicos do Mercantil (baseado no arquivo real)
    mercantil_unique = ['codigocorrespondente', 'nomecorrespondente', 'cnpjcorrespondente', 'codigosubstabelecido', 'nomesubstabelecido', 'cpfagentecertificado']
    mercantil_unique_matches = sum(1 for indicator in mercantil_unique if any(indicator in col for col in df_columns))
    
    # Indicadores de campos extensos t√≠picos do Mercantil
    mercantil_extended = ['valorliberacaosimulado', 'bancoliberacaocliente', 'agencialiberacaocliente', 'contaliberacaocliente', 'digitacontaliberacaocliente']
    mercantil_extended_matches = sum(1 for indicator in mercantil_extended if any(indicator in col for col in df_columns))
    
    if mercantil_matches >= 4:
        logging.info(f"‚úÖ MERCANTIL detectado por colunas principais ({mercantil_matches}/7 matches)")
        return "MERCANTIL"
    elif mercantil_unique_matches >= 3:
        logging.info(f"‚úÖ MERCANTIL detectado por colunas √∫nicas ({mercantil_unique_matches}/6 matches)")
        return "MERCANTIL"
    elif mercantil_extended_matches >= 2:
        logging.info(f"‚úÖ MERCANTIL detectado por colunas extensas ({mercantil_extended_matches}/5 matches)")
        return "MERCANTIL"
    
    # 3. Por conte√∫do espec√≠fico do MERCANTIL (mais restrito)
    if not df.empty:
        # Verificar nas primeiras 5 linhas por indicadores espec√≠ficos do Mercantil
        all_data = ""
        for i in range(min(5, len(df))):
            row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
            all_data += " " + row_data
        
        # Indicadores espec√≠ficos do MERCANTIL (removido 'qfz solucoes' para evitar conflito)
        mercantil_content_indicators = ['banco mercantil do brasil', 'credfranco', 'bmb', 'mercantil']
        found_content_indicators = [ind for ind in mercantil_content_indicators if ind in all_data]
        
        # Verificar se N√ÉO tem indicadores do CREFAZ
        crefaz_conflict_indicators = ['crefaz', 'energia', 'boleto', 'cpfl', 'enel', 'ener', 'bol']
        found_crefaz_conflicts = [ind for ind in crefaz_conflict_indicators if ind in all_data]
        
        if found_content_indicators and not found_crefaz_conflicts:
            logging.info(f"‚úÖ MERCANTIL detectado por conte√∫do espec√≠fico: {found_content_indicators}")
            return "MERCANTIL"
    
    # Verificar se √© QUERO MAIS CREDITO (PRIORIDADE ALTA - antes do Paulista)
    # 1. Por nome do arquivo
    if 'quero' in filename_lower and 'mais' in filename_lower:
        logging.info("üéØ QUERO MAIS detectado por nome do arquivo")
        return "QUERO_MAIS"
    
    # 2. Por estrutura de colunas Unnamed espec√≠ficas
    if len(df.columns) > 40 and sum(1 for col in df_columns if 'unnamed:' in col) > 30:
        # Verificar indicadores espec√≠ficos do QUERO MAIS (ANTES do Paulista!)
        quero_mais_indicators = ['capital consig', 'quero mais credito', 'relat√≥rio de produ√ß√£o', 'promotora', 'grupo qfz', 'cpf correspondente', 'conv√™nio correspondente']
        if not df.empty:
            # Verificar nas primeiras 5 linhas para maior precis√£o
            all_data = ""
            for i in range(min(5, len(df))):
                row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_data += " " + row_data
            
            logging.info(f"üîç QUERO MAIS check - dados: {all_data[:200]}...")
            
            # Indicadores √∫nicos do QUERO MAIS (n√£o confundem com PAULISTA)
            quero_mais_unique = ['capital consig', 'quero mais', 'promotora', 'grupo qfz', 'cpf correspondente']
            found_quero_indicators = [ind for ind in quero_mais_unique if ind in all_data]
            
            # Verificar se N√ÉO tem indicadores do PAULISTA
            paulista_exclusive = ['banco paulista', 'rela√ß√£o de propostas', 'esp√©cie benef√≠cio', 'anal√≠tico']
            found_paulista_indicators = [ind for ind in paulista_exclusive if ind in all_data]
            
            if found_quero_indicators and not found_paulista_indicators:
                logging.info(f"‚úÖ QUERO MAIS detectado! Indicadores √∫nicos: {found_quero_indicators}")
                return "QUERO_MAIS"
            else:
                logging.info(f"‚ö†Ô∏è QUERO MAIS n√£o detectado - indicadores QUERO: {found_quero_indicators}, PAULISTA: {found_paulista_indicators}")
    
    # Verificar se √© BANCO PAN (tem estrutura espec√≠fica de cart√£o)
    pan_indicators = ['n¬∫ proposta', 'n¬∫ opera√ß√£o', 'tipo de opera√ß√£o', 'c√≥digo do produto', 'nome do produto']
    pan_matches = sum(1 for indicator in pan_indicators if any(indicator in col for col in df_columns))
    if pan_matches >= 3:
        return "PAN"
    
    # Verificar se √© C6 BANK (melhorada)
    # 1. Por nome do arquivo
    if 'c6' in filename_lower:
        return "C6"
    
    # 2. Por indicadores nas colunas
    c6_column_indicators = ['nome entidade', 'numero do contrato', 'proposta', 'data da operacao']
    c6_column_matches = sum(1 for indicator in c6_column_indicators if any(indicator in col for col in df_columns))
    if c6_column_matches >= 3:
        return "C6"
    
    # 3. Por conte√∫do dos dados
    c6_indicators = ['c6 bank', 'c6 consignado', 'banco c6']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        if any(indicator in first_row_data for indicator in c6_indicators):
            return "C6"
    
    # Verificar se √© FACTA92 (melhorada)
    # 1. Por nome do arquivo
    if 'facta' in filename_lower or 'relat√≥riovista' in filename_lower.replace(' ', ''):
        return "FACTA92"
    
    # 2. Por colunas espec√≠ficas do FACTA92
    facta_indicators = ['codigo', 'data_cadastro', 'data_registro', 'proposta', 'convenio', 'averbador', 'tipo_operacao', 'tipo_tabela']
    facta_matches = sum(1 for indicator in facta_indicators if any(indicator in col for col in df_columns))
    if facta_matches >= 4:
        return "FACTA92"
    
    # Verificar se √© PAULISTA (detec√ß√£o melhorada)
    # 1. Por nome do arquivo
    if filename and 'paulista' in filename.lower():
        return "PAULISTA"
    
    # 2. Por colunas espec√≠ficas do Paulista
    paulista_column_indicators = ['n¬∫ proposta', 'contrato', 'data captura', 'cpf/cnpj proponente', 'nome do proponente', 'matr√≠cula']
    paulista_col_matches = sum(1 for indicator in paulista_column_indicators if any(indicator in col for col in df_columns))
    if paulista_col_matches >= 4:
        return "PAULISTA"
    
    # 3. Por indicadores na primeira linha
    paulista_indicators = ['banco paulista', 'rela√ß√£o de propostas', 'anal√≠tico', 'esp√©cie benef√≠cio']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        paulista_matches = sum(1 for indicator in paulista_indicators if indicator in first_row_data)
        if paulista_matches >= 2:
            return "PAULISTA"
    
    # 3. Por estrutura de colunas Unnamed espec√≠ficas do Paulista (MELHORADA)
    paulista_columns = ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 14', 'Unnamed: 18']
    paulista_col_matches = sum(1 for col in paulista_columns if col in df_columns)
    if paulista_col_matches >= 5 and len(df_columns) > 20:  # Paulista tem muitas colunas
        # Verificar se tem dados que parecem do Paulista em qualquer linha
        if not df.empty:
            # Procurar em todas as linhas por palavras-chave do Paulista
            all_data = ""
            for i in range(min(5, len(df))):  # Verificar at√© 5 primeiras linhas
                row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_data += " " + row_data
            
            logging.info(f"üîç PAULISTA check - dados: {all_data[:200]}...")
            
            # Indicadores √∫nicos do PAULISTA (n√£o confundem com QUERO MAIS)
            paulista_unique_indicators = ['banco paulista', 'rela√ß√£o de propostas', 'esp√©cie benef√≠cio', 'anal√≠tico']
            found_paulista_indicators = [ind for ind in paulista_unique_indicators if ind in all_data]
            
            # Verificar se N√ÉO tem indicadores do QUERO MAIS
            quero_mais_exclusive = ['capital consig', 'quero mais', 'promotora', 'grupo qfz', 'cpf correspondente']
            found_quero_indicators = [ind for ind in quero_mais_exclusive if ind in all_data]
            
            # PAULISTA s√≥ se tem indicadores √∫nicos E n√£o tem indicadores do QUERO MAIS
            if found_paulista_indicators and not found_quero_indicators:
                logging.info(f"‚úÖ PAULISTA detectado! Indicadores √∫nicos: {found_paulista_indicators}")
                return "PAULISTA"
            elif found_paulista_indicators and found_quero_indicators:
                logging.warning(f"üîÑ Conflito PAULISTA vs QUERO MAIS - priorizando QUERO MAIS: {found_quero_indicators}")
                return "QUERO_MAIS"  # Em caso de d√∫vida, priorizar QUERO MAIS
            else:
                logging.info(f"‚ö†Ô∏è PAULISTA n√£o detectado - indicadores PAULISTA: {found_paulista_indicators}, QUERO: {found_quero_indicators}")
                
            # Fallback: se tem estrutura Unnamed mas n√£o tem indicadores √∫nicos claros
            generic_keywords = ['inss', 'aposentad', 'pens√£o', 'consignado', 'benefici', 'cpf', 'proposta']
            keyword_matches = sum(1 for word in generic_keywords if word in all_data)
            
            # S√≥ usar fallback se n√£o conflitar com QUERO MAIS
            if keyword_matches >= 3 and not found_quero_indicators:
                logging.info(f"üìä PAULISTA assumido por estrutura + keywords gen√©ricos (sem conflito QUERO MAIS)")
                return "PAULISTA"
    
    # Verificar se √© TOTALCASH (tem estrutura espec√≠fica)
    totalcash_indicators = ['totalcash', 'total cash']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        if any(indicator in first_row_data for indicator in totalcash_indicators):
            return "TOTALCASH"
    
    # Verificar se √© BRB (Banco de Bras√≠lia)
    brb_indicators = ['id card', 'nome do cliente', 'benef√≠cio', 'cpf do benefici√°rio', 'data da proposta', 'data da pagamento', 'n¬∫ contrato']
    brb_matches = sum(1 for indicator in brb_indicators if any(indicator in col for col in df_columns))
    if brb_matches >= 4:
        # Confirmar com dados
        if not df.empty:
            first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
            if 'brb' in first_row_data or 'banco de bras√≠lia' in first_row_data or 'q-faz' in first_row_data:
                return "BRB"
    
    # Verificar se √© QUALIBANKING (melhorada)
    # 1. Por nome do arquivo
    if 'quali' in filename_lower or 'qualibanking' in filename_lower:
        return "QUALIBANKING"
    
    # 2. Por colunas espec√≠ficas
    qualibanking_indicators = ['c√≥digo', 'tipo', 'etapa', 'nome do produto', 'nome da tabela', 'c√≥digo da tabela', 'tipo de produto', 'tipo de opera√ß√£o', 'data de cadastro', 'valor da opera√ß√£o']
    qualibanking_matches = sum(1 for indicator in qualibanking_indicators if any(indicator in col for col in df_columns))
    if qualibanking_matches >= 5:
        return "QUALIBANKING"
    
    # 3. Por padr√£o do n√∫mero de contrato (QUA0000...)
    if not df.empty:
        for col in df.columns:
            if 'contrato' in str(col).lower() or 'c√≥digo' in str(col).lower():
                try:
                    sample_val = str(df[col].iloc[0]).upper()
                    if sample_val.startswith('QUA'):
                        return "QUALIBANKING"
                except:
                    continue
    
    # 4. Por conte√∫do dos dados
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        if 'qualibanking' in first_row_data or 'quali' in first_row_data:
            return "QUALIBANKING"
    

    # Verificar se √© AMIGOZ
    amigoz_indicators = ['nr proposta', 'id banksoft', 'vulnerabilidade', 'aceite cliente vulneravel', 'grau de escolaridade', 'tipo de cart√£o']
    amigoz_matches = sum(1 for indicator in amigoz_indicators if any(indicator in col for col in df_columns))
    if amigoz_matches >= 3:
        # Confirmar com dados
        if not df.empty:
            first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
            if 'amigoz' in first_row_data or 'cart√£o benef√≠cio' in first_row_data or 'cart√£o consignado' in first_row_data:
                return "AMIGOZ"
    
    # Detec√ß√£o adicional por conte√∫do dos dados
    if not df.empty:
        sample_data = str(df.iloc[0]).lower() if len(df) > 0 else ""
        
        if 'averbai' in sample_data or 'fixo 30' in sample_data:
            return "AVERBAI"
        elif 'digio' in sample_data or 'tkt' in sample_data:
            return "DIGIO"
        elif 'prata digital' in sample_data or 'shake de morango' in sample_data:
            return "PRATA"
        elif 'vctex' in sample_data or "it's solucoes" in sample_data:
            return "VCTEX"
        elif 'daycoval' in sample_data:
            return "DAYCOVAL"
    
    # Detec√ß√£o por padr√µes de colunas
    if any('proposta' in col for col in df_columns):
        # Se tem muitas colunas unnamed, pode ser Digio ou Daycoval
        if sum(1 for col in df_columns if 'unnamed:' in col) > 20:
            # Distinguir entre DIGIO e DAYCOVAL pela primeira linha
            if not df.empty:
                first_row_content = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
                if 'daycoval' in first_row_content or 'nr.prop' in first_row_content or 'tp. opera√ß√£o' in first_row_content:
                    return "DAYCOVAL"
                else:
                    return "DIGIO"
            return "DIGIO"  # Default se n√£o conseguir distinguir
        elif sum(1 for col in df_columns if 'unnamed:' in col) > 10:
            return "DAYCOVAL"
        # Se tem ID, provavelmente √© Averbai
        elif any('id' in col for col in df_columns):
            return "AVERBAI"
        # Se tem "n√∫mero da proposta", √© Prata
        elif any('n√∫mero da proposta' in col for col in df_columns):
            return "PRATA"
        # Se tem "n√∫mero do contrato", √© VCTEX
        elif any('n√∫mero do contrato' in col for col in df_columns):
            return "VCTEX"
    
    # Se n√£o conseguiu detectar, tentar por estrutura geral
    logging.warning(f"N√£o foi poss√≠vel detectar automaticamente o tipo de banco para: {filename}")
    logging.warning(f"N√∫mero de colunas: {len(df.columns)}")
    logging.warning(f"Colunas Unnamed: {sum(1 for col in df_columns if 'unnamed:' in col)}")
    
    # √öltima tentativa: an√°lise de conte√∫do mais espec√≠fica
    if not df.empty and len(df.columns) > 1:
        # Verificar conte√∫do da primeira linha n√£o vazia
        for idx, row in df.iterrows():
            row_content = ' '.join([str(val).lower() for val in row.values if pd.notna(val) and str(val).strip()])
            if row_content and len(row_content) > 10:  # Linha com conte√∫do substantivo
                if 'banco digio' in row_content:
                    return "DIGIO"
                elif 'banco daycoval' in row_content:
                    return "DAYCOVAL"
                elif 'banco paulista' in row_content or 'paulista' in row_content:
                    return "PAULISTA"
                elif 'prata digital' in row_content:
                    return "PRATA"
                elif "it's solucoes" in row_content:
                    return "VCTEX"
                elif 'averbai' in row_content or 'saque fgts' in row_content:
                    return "AVERBAI"
                break
        
        # Tentativa final para Paulista: estrutura Unnamed + palavras-chave
        if len(df_columns) > 20 and sum(1 for col in df_columns if 'unnamed:' in col) > 15:
            # Procurar palavras-chave do Paulista em qualquer parte do DataFrame
            all_text = ""
            for i in range(min(5, len(df))):
                row_text = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_text += " " + row_text
            
            paulista_keywords = ['inss', 'aposentad', 'pens√£o', 'consignado', 'benefici', 'cpf', 'proposta', 'contrato']
            keyword_matches = sum(1 for word in paulista_keywords if word in all_text)
            
            if keyword_matches >= 2:
                return "PAULISTA"
    
    raise HTTPException(status_code=400, detail=f"Tipo de banco n√£o reconhecido para: {filename}. Estrutura: {len(df.columns)} colunas, {sum(1 for col in df_columns if 'unnamed:' in col)} colunas 'Unnamed'. Primeiras colunas: {df_columns[:5]}")

def process_storm_data_enhanced(df: pd.DataFrame) -> Dict[str, str]:
    """Processamento aprimorado dos dados da Storm"""
    storm_proposals = {}
    
    logging.info(f"Processando Storm com colunas: {list(df.columns)}")
    
    for _, row in df.iterrows():
        proposta = ""
        status = ""
        
        # Tentar diferentes formatos de colunas (case-insensitive)
        for idx, col in enumerate(df.columns):
            col_lower = str(col).lower()
            
            # Procurar coluna de proposta/ADE
            if any(term in col_lower for term in ['ade', 'proposta']) and not proposta:
                proposta = str(row.iloc[idx]).strip()
                
            # Procurar coluna de status
            if any(term in col_lower for term in ['status', 'situacao']) and not status:
                status = str(row.iloc[idx]).strip().lower()
        
        # Fallback: usar posi√ß√µes fixas se n√£o encontrou pelos nomes
        if not proposta and len(df.columns) >= 1:
            proposta = str(row.iloc[0]).strip()
        if not status and len(df.columns) >= 3:
            status = str(row.iloc[2]).strip().lower()
        elif not status and len(df.columns) >= 2:
            status = str(row.iloc[1]).strip().lower()
        
        # Validar se proposta √© num√©rica (ADE v√°lido)
        if proposta and proposta != 'nan' and proposta not in ['ADE', 'ade', 'PROPOSTA']:
            # Limpar proposta (remover caracteres n√£o num√©ricos exceto d√≠gitos)
            proposta_clean = ''.join(c for c in proposta if c.isdigit())
            if proposta_clean and len(proposta_clean) >= 4:  # ADE deve ter pelo menos 4 d√≠gitos
                normalized_status = STATUS_MAPPING.get(status, status.upper() if status else "AGUARDANDO")
                storm_proposals[proposta_clean] = normalized_status
                logging.info(f"Proposta processada: {proposta_clean} -> {normalized_status}")
    
    logging.info(f"Storm processada: {len(storm_proposals)} propostas")
    return storm_proposals

def normalize_operation_for_matching(operation: str) -> str:
    """Normaliza opera√ß√£o para compara√ß√£o flex√≠vel (remove case sensitivity e preposi√ß√µes)"""
    if not operation:
        return ""
    
    # Normalizar b√°sico
    normalized = ' '.join(operation.strip().split())
    
    # Converter para lowercase para compara√ß√£o
    normalized_lower = normalized.lower()
    
    # Mapear varia√ß√µes conhecidas para forma can√¥nica
    operation_mappings = {
        'refinanciamento da portabilidade': 'REFINANCIAMENTO DA PORTABILIDADE',
        'refinanciamento de portabilidade': 'REFINANCIAMENTO DA PORTABILIDADE', 
        'refin portabilidade': 'REFINANCIAMENTO DA PORTABILIDADE',
        'portabilidade': 'PORTABILIDADE',
        'refinanciamento': 'REFINANCIAMENTO',
        'margem livre': 'MARGEM LIVRE',
        'margem livre (novo)': 'MARGEM LIVRE (NOVO)',
        'novo': 'MARGEM LIVRE (NOVO)'
    }
    
    # Buscar mapeamento exato primeiro
    if normalized_lower in operation_mappings:
        return operation_mappings[normalized_lower]
    
    # Buscar por palavras-chave
    if 'portabilidade' in normalized_lower and 'refin' in normalized_lower:
        return 'REFINANCIAMENTO DA PORTABILIDADE'
    elif 'portabilidade' in normalized_lower:
        return 'PORTABILIDADE'
    elif 'refinanciamento' in normalized_lower:
        return 'REFINANCIAMENTO'
    elif 'margem' in normalized_lower:
        return 'MARGEM LIVRE (NOVO)'
    
    # Se n√£o encontrou, retorna normalizado em uppercase
    return normalized.upper()

def apply_mapping_daycoval_corrected(organ: str, operation_type: str) -> dict:
    """
    üîß DAYCOVAL - Mapeamento direto por √ìrg√£o + Opera√ß√£o
    DAYCOVAL n√£o tem c√≥digo da tabela no arquivo, ent√£o mapeia por √≥rg√£o e opera√ß√£o
    """
    organ_normalized = organ.upper().strip()
    operation_normalized = operation_type.upper().strip()
    
    logging.info(f"üîß DAYCOVAL Mapeamento - ORGAO: '{organ_normalized}', OPERACAO: '{operation_normalized}'")
    
    # Mapear para os padr√µes do CSV do DAYCOVAL
    # C√≥digos espec√≠ficos encontrados no CSV: 803463, 801307, 805994, 821121, 231880
    
    if organ_normalized in ["INSS"]:
        if "PORTABILIDADE" in operation_normalized and "REFIN" in operation_normalized:
            # Refinanciamento da Portabilidade - INSS
            return {
                "codigo_tabela": "805994",  # Padr√£o DAYCOVAL no CSV
                "banco_storm": "BANCO DAYCOVAL",
                "orgao_storm": "INSS",
                "operacao_storm": "Refinanciamento da Portabilidade",
                "taxa_storm": "2.14"  # Taxa padr√£o DAYCOVAL
            }
        elif "PORTABILIDADE" in operation_normalized:
            # Portabilidade + Refin - INSS
            return {
                "codigo_tabela": "803463",  # C√≥digo comum DAYCOVAL
                "banco_storm": "BANCO DAYCOVAL",
                "orgao_storm": "INSS",
                "operacao_storm": "Portabilidade + Refin",
                "taxa_storm": "2.14"
            }
        elif "REFINANCIAMENTO" in operation_normalized:
            # Refinanciamento - INSS
            return {
                "codigo_tabela": "801307",  # Outro c√≥digo DAYCOVAL
                "banco_storm": "BANCO DAYCOVAL",
                "orgao_storm": "INSS", 
                "operacao_storm": "Refinanciamento",
                "taxa_storm": "2.14"
            }
        else:
            # Margem Livre Novo - INSS
            return {
                "codigo_tabela": "821121",  # C√≥digo DAYCOVAL padr√£o
                "banco_storm": "BANCO DAYCOVAL",
                "orgao_storm": "INSS",
                "operacao_storm": "Margem Livre (Novo)",
                "taxa_storm": "2.14"
            }
            
    elif organ_normalized in ["SPPREV"]:
        # SPPREV - usar c√≥digo espec√≠fico
        return {
            "codigo_tabela": "231880",  # C√≥digo DAYCOVAL para SPPREV
            "banco_storm": "BANCO DAYCOVAL", 
            "orgao_storm": "SPPREV",
            "operacao_storm": operation_type,  # Manter opera√ß√£o original
            "taxa_storm": "2.14"
        }
        
    elif organ_normalized in ["EDUCACAO"]:
        # Educa√ß√£o - usar c√≥digo geral
        return {
            "codigo_tabela": "803463",  # C√≥digo geral DAYCOVAL
            "banco_storm": "BANCO DAYCOVAL",
            "orgao_storm": "EDUCACAO", 
            "operacao_storm": operation_type,
            "taxa_storm": "2.14"
        }
    
    # Default - INSS Margem Livre
    logging.warning(f"‚ö†Ô∏è DAYCOVAL Mapeamento n√£o espec√≠fico para {organ_normalized} + {operation_normalized}, usando default INSS")
    return {
        "codigo_tabela": "803463",  # C√≥digo mais comum no CSV
        "banco_storm": "BANCO DAYCOVAL",
        "orgao_storm": "INSS", 
        "operacao_storm": "Margem Livre (Novo)",
        "taxa_storm": "2.14"
    }

def _detect_santander_orgao(row: dict) -> str:
    """Detectar √≥rg√£o do SANTANDER baseado no CONVENIO/PRODUTO"""
    convenio = str(row.get('CONVENIO', '')).strip().upper()
    produto = str(row.get('PRODUTO', '')).strip().upper()
    
    if 'PREF' in convenio or 'PREFEITURA' in convenio or 'AGUDOS' in convenio or 'RANCHARIA' in convenio:
        return 'PREF. DE AGUDOS - SP'  # Padr√£o para prefeituras
    elif 'INSS' in convenio or 'INSS' in produto:
        return 'INSS'
    elif 'SEGURO' in convenio or 'SEGURO' in produto:
        return 'INSS'  # Seguro vinculado ao INSS
    else:
        return 'INSS'  # Default

def _get_santander_operation_type(row: dict) -> str:
    """Extrair tipo de opera√ß√£o do SANTANDER baseado no PRODUTO"""
    produto = str(row.get('PRODUTO', '')).strip().upper()
    
    if 'NOVO' in produto and 'REFIN' in produto:
        return 'Margem Livre (Novo)'  # Priorizar NOVO quando ambos est√£o presentes
    elif 'REFIN' in produto:
        return 'Refinanciamento'
    elif 'NOVO' in produto:
        return 'Margem Livre (Novo)'
    elif 'SEGURO' in produto:
        return 'Margem Livre (Novo)'  # Seguro √© geralmente opera√ß√£o nova
    else:
        return 'Margem Livre (Novo)'  # Default

def _extract_santander_codigo_tabela(produto_str: str) -> str:
    """Extrair c√≥digo tabela do campo PRODUTO do SANTANDER"""
    if not produto_str:
        return ""
    
    produto_str = str(produto_str).strip()
    
    # Padr√£o: "21387 - 810021387 - 1 OFERTA NOVO COM SEGURO"
    # Queremos extrair o c√≥digo do meio (810021387)
    import re
    
    # Buscar padr√£o n√∫mero - n√∫mero - texto
    pattern = r'(\d+)\s*-\s*(\d+)\s*-'
    match = re.search(pattern, produto_str)
    if match:
        return match.group(2)  # Segundo n√∫mero √© o c√≥digo
    
    # Se n√£o encontrar o padr√£o, buscar n√∫meros individuais
    numbers = re.findall(r'\d+', produto_str)
    if len(numbers) >= 2:
        # Pegar o maior n√∫mero (provavelmente o c√≥digo)
        return max(numbers, key=len)
    elif len(numbers) == 1:
        return numbers[0]
    
    return ""

def apply_mapping_santander_direct_code(codigo_tabela: str) -> dict:
    """Mapeamento direto para BANCO SANTANDER por c√≥digo tabela extra√≠do"""
    try:
        if not codigo_tabela or not codigo_tabela.isdigit():
            return {}
        
        # Procurar c√≥digo no mapeamento
        for key, details in TABELA_MAPPING.items():
            if details.get('codigo_tabela') == codigo_tabela and 'BANCO SANTANDER' in key:
                logging.info(f"‚úÖ SANTANDER c√≥digo {codigo_tabela}: {details.get('orgao_storm')} | {details.get('operacao_storm')} | {details.get('taxa_storm')}")
                return {
                    'orgao_storm': details.get('orgao_storm', ''),
                    'operacao_storm': details.get('operacao_storm', ''),
                    'taxa_storm': details.get('taxa_storm', ''),
                    'codigo_tabela': codigo_tabela
                }
        
        logging.warning(f"‚ö†Ô∏è SANTANDER c√≥digo {codigo_tabela}: N√£o encontrado no mapeamento")
        return {}
        
    except Exception as e:
        logging.error(f"‚ùå Erro no mapeamento direto Santander: {e}")
        return {}

def apply_mapping_averbai_corrected(organ: str, operation_type: str, tabela: str = "") -> dict:
    """Corre√ß√£o espec√≠fica para AVERBAI - evita c√≥digos 1005/1016 trocados com 994/992"""
    try:
        # Filtrar apenas registros AVERBAI do mapeamento
        averbai_candidates = []
        
        for key, details in TABELA_MAPPING.items():
            parts = key.split('|')
            if len(parts) == 4 and parts[0].upper() == 'AVERBAI':
                averbai_candidates.append((key, details))
        
        # Normalizar entradas
        organ_normalized = ' '.join(organ.strip().upper().split()) if organ else ""
        operation_normalized = normalize_operation_for_matching(operation_type)
        tabela_normalized = ' '.join(tabela.strip().upper().split()) if tabela else ""
        
        logging.info(f"üîé AVERBAI CORRIGIDO - Buscando: {organ_normalized} | {operation_normalized} | '{tabela_normalized}'")
        
        # Criar lista de candidatos com scoring inteligente
        scored_candidates = []
        
        for key, details in averbai_candidates:
            parts = key.split('|')
            csv_banco, csv_orgao, csv_operacao, csv_tabela = parts
            
            # Normalizar dados do CSV
            csv_orgao_norm = ' '.join(csv_orgao.upper().split())
            csv_operacao_norm = ' '.join(csv_operacao.upper().split())
            csv_tabela_norm = ' '.join(csv_tabela.upper().split())
            
            # Score inicial: 0
            total_score = 0
            match_details = []
            
            # 1. √ìRG√ÉO deve ser EXATO (obrigat√≥rio)
            if csv_orgao_norm == organ_normalized:
                total_score += 1000  # Score alto para √≥rg√£o correto
                match_details.append("ORGAO_EXATO")
            else:
                continue  # Pular se √≥rg√£o n√£o bate
            
            # 2. OPERA√á√ÉO (muito importante) - MELHORADA para distinguir 1005 vs 1016
            if csv_operacao_norm == operation_normalized:
                total_score += 500  # Score alto para opera√ß√£o exata
                match_details.append("OPERACAO_EXATA")
            elif operation_normalized in csv_operacao_norm or csv_operacao_norm in operation_normalized:
                # CORRE√á√ÉO ESPEC√çFICA: Distinguir "Refinanciamento da Portabilidade" vs "Refinanciamento Da Portabilidade"
                # Problema: 1016 com "Refinanciamento Da Portabilidade" estava sobrepondo 1005 com "Refinanciamento da Portabilidade"
                
                # Verificar se √© match de case sensitivity (da vs Da)
                if csv_operacao_norm.replace("DA", "da") == operation_normalized.replace("DA", "da"):
                    # Match exato ignorando case de "da/Da" - dar score alto mas menor que exato
                    total_score += 450  # Score alto mas menor que opera√ß√£o exata
                    match_details.append("OPERACAO_CASE_SIMILAR")
                else:
                    total_score += 200  # Score m√©dio para opera√ß√£o parcial
                    match_details.append("OPERACAO_PARCIAL")
            else:
                # Verificar palavras-chave comuns
                op_words_input = set(operation_normalized.split())
                op_words_csv = set(csv_operacao_norm.split())
                common_op_words = op_words_input.intersection(op_words_csv)
                if common_op_words:
                    total_score += len(common_op_words) * 50
                    match_details.append(f"OPERACAO_PALAVRAS({len(common_op_words)})")
                else:
                    continue  # Pular se opera√ß√£o n√£o tem nada a ver
            
            # 3. TABELA (decisivo para desempate) - PRIORIDADE ABSOLUTA
            if tabela_normalized and csv_tabela_norm:
                if csv_tabela_norm == tabela_normalized:
                    total_score += 2000  # MATCH PERFEITO - PRIORIDADE ABSOLUTA (dobrado)
                    match_details.append("TABELA_EXATA")
                else:
                    # An√°lise por palavras mais sofisticada
                    tabela_words = set(w for w in tabela_normalized.split() if len(w) > 2)
                    csv_words = set(w for w in csv_tabela_norm.split() if len(w) > 2)
                    
                    if tabela_words and csv_words:
                        common_words = tabela_words.intersection(csv_words)
                        
                        if tabela_words == csv_words:
                            total_score += 800  # Mesmo conjunto de palavras
                            match_details.append("TABELA_PALAVRAS_IDENTICAS")
                        elif len(common_words) >= len(tabela_words) * 0.8:  # 80% das palavras
                            total_score += 600
                            match_details.append(f"TABELA_ALTA_SIMILARIDADE({len(common_words)}/{len(tabela_words)})")
                        elif len(common_words) >= len(tabela_words) * 0.5:  # 50% das palavras
                            total_score += 400
                            match_details.append(f"TABELA_MEDIA_SIMILARIDADE({len(common_words)}/{len(tabela_words)})")
                        elif common_words:
                            total_score += len(common_words) * 100
                            match_details.append(f"TABELA_ALGUMAS_PALAVRAS({len(common_words)})")
                    
                    # Bonus para substring match
                    if tabela_normalized in csv_tabela_norm or csv_tabela_norm in tabela_normalized:
                        total_score += 200
                        match_details.append("TABELA_SUBSTRING")
            
            # 4. Bonus para taxas mais altas (desempate)
            try:
                taxa_str = details.get('taxa_storm', '0%')
                taxa_float = float(taxa_str.replace('%', '').replace(',', '.'))
                total_score += taxa_float * 10  # Bonus pequeno para taxa mais alta
                match_details.append(f"TAXA_BONUS({taxa_float})")
            except:
                pass
            
            # 5. Prioriza√ß√£o de c√≥digos - CORRE√á√ÉO ESPEC√çFICA para 1005 vs 1016
            try:
                codigo_str = details.get('codigo_tabela', '0')
                codigo_int = int(codigo_str)
                
                # CORRE√á√ÉO ESPEC√çFICA: Quando n√£o h√° tabela espec√≠fica, priorizar c√≥digos menores
                # Problema: 1016 (taxa 1,85%) estava ganhando de 1005 (taxa 1,80%) sem tabela
                if not tabela_normalized or len(tabela_normalized) == 0:
                    # Sem tabela espec√≠fica: priorizar c√≥digos menores (mais estabelecidos)
                    if codigo_int <= 1005:  # C√≥digos "antigos" t√™m prioridade
                        total_score += 100
                        match_details.append(f"CODIGO_ESTABELECIDO({codigo_int})")
                    else:
                        # C√≥digos novos t√™m menor prioridade quando n√£o h√° tabela espec√≠fica
                        total_score += 50
                        match_details.append(f"CODIGO_NOVO_SEM_TABELA({codigo_int})")
                else:
                    # Com tabela espec√≠fica: c√≥digos maiores podem ter ligeira vantagem
                    if codigo_int >= 1000:
                        total_score += codigo_int * 0.05  # Reduzido de 0.1 para 0.05
                        match_details.append(f"CODIGO_NOVO({codigo_int})")
            except:
                pass
            
            # Adicionar candidato se tem score m√≠nimo
            if total_score >= 1000:  # Pelo menos √≥rg√£o correto
                scored_candidates.append({
                    'details': details,
                    'score': total_score,
                    'match_info': ' + '.join(match_details),
                    'tabela_csv': csv_tabela_norm
                })
        
        # Ordenar candidatos por score (maior primeiro)
        scored_candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # Log dos candidatos
        logging.info(f"üìä AVERBAI - Encontrados {len(scored_candidates)} candidatos:")
        for i, candidate in enumerate(scored_candidates[:3]):  # Top 3
            codigo = candidate['details'].get('codigo_tabela', 'N/A')
            taxa = candidate['details'].get('taxa_storm', 'N/A')
            logging.info(f"   {i+1}. Score {candidate['score']}: C√≥digo {codigo} | Taxa {taxa}")
            logging.info(f"      Match: {candidate['match_info']}")
        
        if not scored_candidates:
            logging.error(f"‚ùå AVERBAI - Nenhum candidato encontrado para {organ_normalized} | {operation_normalized}")
            return {}
        
        # Retornar o melhor candidato
        best = scored_candidates[0]
        result = best['details']
        
        logging.info(f"‚úÖ AVERBAI RESULTADO CORRIGIDO: C√≥digo {result.get('codigo_tabela')} | Score {best['score']}")
        
        return result
        
    except Exception as e:
        logging.error(f"‚ùå Erro no mapeamento AVERBAI corrigido: {str(e)}")
        return {}

def apply_mapping(bank_name: str, organ: str, operation_type: str, usuario: str = "", tabela: str = "") -> dict:
    """Aplica mapeamento autom√°tico MELHORADO com corre√ß√£o espec√≠fica para AVERBAI"""
    try:
        # Normalizar nomes para busca (remover espa√ßos extras e converter para uppercase)
        bank_normalized = ' '.join(bank_name.strip().upper().split()) if bank_name else ""
        organ_normalized = ' '.join(organ.strip().upper().split()) if organ else ""
        operation_normalized = normalize_operation_for_matching(operation_type)
        
        # Normalizar tabela (UPPER para matching consistente com chaves do CSV)
        tabela_normalized = ' '.join(tabela.strip().upper().split()) if tabela else ""
        
        # CORRE√á√ÉO ESPEC√çFICA PARA AVERBAI - usar fun√ß√£o especializada
        if bank_normalized == "AVERBAI":
            logging.info(f"üîß AVERBAI detectado - usando corre√ß√£o espec√≠fica")
            return apply_mapping_averbai_corrected(organ, operation_type, tabela)
            
        # CORRE√á√ÉO ESPEC√çFICA PARA DAYCOVAL - usar fun√ß√£o especializada
        if bank_normalized == "BANCO DAYCOVAL" or bank_normalized == "DAYCOVAL":
            logging.info(f"üîß DAYCOVAL detectado - usando corre√ß√£o espec√≠fica por √≥rg√£o+opera√ß√£o")
            return apply_mapping_daycoval_corrected(organ, operation_type)
            
        # CORRE√á√ÉO ESPEC√çFICA PARA SANTANDER - usar mapeamento direto por c√≥digo
        if bank_normalized == "BANCO SANTANDER" or bank_normalized == "SANTANDER":
            # Para Santander, usar o par√¢metro tabela como c√≥digo_tabela
            if tabela_normalized and tabela_normalized.isdigit():
                logging.info(f"üè¶ SANTANDER detectado - usando mapeamento direto por c√≥digo: {tabela_normalized}")
                return apply_mapping_santander_direct_code(tabela_normalized)
            else:
                logging.warning(f"‚ö†Ô∏è SANTANDER sem c√≥digo v√°lido ({tabela_normalized}), usando busca tradicional")
        
            # Log b√°sico para VCTEX
            if bank_normalized == "BANCO VCTEX":
                print(f"üîç VCTEX: BANCO={bank_normalized} | ORGAO={organ_normalized} | OPERACAO={operation_normalized} | TABELA={tabela_normalized}")
                logging.warning(f"üîç VCTEX: BANCO={bank_normalized} | ORGAO={organ_normalized} | OPERACAO={operation_normalized} | TABELA={tabela_normalized}")
        
        logging.info(f"üîç Buscando mapeamento: BANCO={bank_normalized} | ORGAO={organ_normalized} | OPERACAO={operation_normalized} | TABELA={tabela_normalized}")
        
        # PRIORIDADE 1: Busca EXATA por BANCO + ORG√ÉO + OPERA√á√ÉO + TABELA (mais espec√≠fico e confi√°vel)
        if tabela_normalized:
            best_match = None
            best_match_score = 0
            
            # Log detalhado para AVERBAI e VCTEX
            is_averbai = bank_normalized == "AVERBAI"
            is_vctex = bank_normalized == "BANCO VCTEX"
            if is_averbai:
                logging.info(f"üîé AVERBAI - Iniciando busca por tabela: '{tabela_normalized}' (len={len(tabela_normalized)})")
            elif is_vctex:
                print(f"üîé VCTEX: '{tabela_normalized}'")
                logging.warning(f"üîé VCTEX: '{tabela_normalized}'")
            
            for key, details in TABELA_MAPPING.items():
                parts = key.split('|')
                if len(parts) == 4:
                    key_banco, key_orgao, key_operacao, key_tabela = parts
                    # Normalizar keys removendo espa√ßos extras
                    key_banco_norm = ' '.join(key_banco.upper().split())
                    key_orgao_norm = ' '.join(key_orgao.upper().split())
                    key_operacao_norm = ' '.join(key_operacao.upper().split())
                    key_tabela_norm = ' '.join(key_tabela.upper().split())
                    
                    # Debug para VCTEX
                    if is_vctex and 'VCTEX' in key:
                        print(f"üîéüî• VCTEX - Testando chave: '{key}'")
                        print(f"   üî• Key Tabela: '{key_tabela_norm}' vs Busca: '{tabela_normalized}'")
                        logging.warning(f"üîéüî• VCTEX - Testando chave: '{key}'")
                        logging.warning(f"   üî• Key Tabela: '{key_tabela_norm}' vs Busca: '{tabela_normalized}'")
                    
                    # Busca EXATA para banco
                    if bank_normalized != key_banco_norm:
                        if is_vctex and 'VCTEX' in key:
                            print(f"   ‚ùåüî• BANCO n√£o match: '{bank_normalized}' != '{key_banco_norm}'")
                            logging.warning(f"   ‚ùåüî• BANCO n√£o match: '{bank_normalized}' != '{key_banco_norm}'")
                        continue
                    
                    # Busca FLEX√çVEL para √≥rg√£o (pode variar ligeiramente)
                    organ_match = (
                        organ_normalized == key_orgao_norm or
                        organ_normalized in key_orgao_norm or 
                        key_orgao_norm in organ_normalized
                    )
                    
                    if not organ_match:
                        continue
                    
                    # Para tabela, usar matching inteligente com diferentes n√≠veis de precis√£o
                    match_score = 0
                    
                    # üî¢ PRIORIDADE M√ÅXIMA: Match exato de C√ìDIGO NUM√âRICO no in√≠cio
                    # Ex: busca "61700" deve bater EXATO em "61700 - CLT..." e n√£o em "60763 - CLT..."
                    if tabela_normalized.isdigit() and key_tabela_norm.startswith(tabela_normalized + ' '):
                        match_score = 10  # Match de c√≥digo exato (M√ÅXIMA PRIORIDADE)
                    elif tabela_normalized == key_tabela_norm:
                        match_score = 5  # Match exato (melhor)
                    else:
                        # An√°lise por palavras para casos com formata√ß√£o diferente
                        tabela_words = set(tabela_normalized.split())
                        key_words = set(key_tabela_norm.split())
                        
                        # Remover palavras muito curtas que s√£o ru√≠do
                        tabela_words_filtered = {w for w in tabela_words if len(w) > 2}
                        key_words_filtered = {w for w in key_words if len(w) > 2}
                        
                        # Verificar se todas as palavras significativas batem
                        if tabela_words_filtered and key_words_filtered:
                            if tabela_words_filtered == key_words_filtered:
                                match_score = 4  # Mesmo conjunto de palavras, ordem diferente
                            elif tabela_words_filtered.issubset(key_words_filtered):
                                match_score = 3  # Tabela do banco cont√©m todas as palavras do CSV
                            elif key_words_filtered.issubset(tabela_words_filtered):
                                match_score = 3  # CSV cont√©m todas as palavras da tabela do banco
                            else:
                                # Calcular palavras em comum
                                common_words = tabela_words_filtered.intersection(key_words_filtered)
                                if len(common_words) >= min(2, len(tabela_words_filtered) // 2):
                                    match_score = 2  # Pelo menos metade das palavras batem
                        
                        # Fallback: matching por substring (menos confi√°vel)
                        if match_score == 0:
                            if tabela_normalized in key_tabela_norm or key_tabela_norm in tabela_normalized:
                                match_score = 1
                    
                    # Guardar melhor match
                    if match_score > best_match_score:
                        best_match_score = match_score
                        best_match = details
                        best_match_key = key_tabela_norm
                        
                        # Log detalhado para AVERBAI
                        if is_averbai:
                            logging.info(f"  ‚ú® MELHOR MATCH at√© agora: score={match_score}, tabela_csv='{key_tabela_norm}', codigo={details.get('codigo_tabela', 'N/A')}, taxa={details.get('taxa_storm', 'N/A')}")
            
            if best_match:
                if is_averbai:
                    logging.info(f"‚úÖ AVERBAI - Resultado FINAL: score={best_match_score}, key='{best_match_key}', Codigo={best_match['codigo_tabela']}, Taxa={best_match['taxa_storm']}, Operacao={best_match['operacao_storm']}")
                else:
                    logging.info(f"‚úÖ Mapeamento por TABELA (score={best_match_score}): Codigo={best_match['codigo_tabela']} | Taxa={best_match['taxa_storm']} | Operacao={best_match['operacao_storm']}")
                return best_match
            
            # Log se tabela n√£o foi encontrada
            if is_averbai:
                logging.error(f"‚ùå AVERBAI - TABELA N√ÉO ENCONTRADA: '{tabela_normalized}' - Tentando fallback gen√©rico")
            else:
                logging.warning(f"‚ö†Ô∏è Tabela '{tabela_normalized}' n√£o encontrada, tentando fallback gen√©rico")
        
        # PRIORIDADE 2: Busca por BANCO + ORG√ÉO + OPERA√á√ÉO (usa DETAILED_MAPPING)
        detail_key_candidates = []
        
        for bank_key, organs in ORGAN_MAPPING.items():
            bank_key_norm = ' '.join(bank_key.upper().split())
            # Busca EXATA para banco
            if bank_normalized == bank_key_norm:
                for organ_key, operations in organs.items():
                    organ_key_norm = ' '.join(organ_key.upper().split())
                    # Busca FLEX√çVEL para √≥rg√£o
                    organ_match = (
                        organ_normalized == organ_key_norm or
                        organ_normalized in organ_key_norm or 
                        organ_key_norm in organ_normalized
                    )
                    if organ_match:
                        for op_key, table_code in operations.items():
                            op_key_norm = ' '.join(op_key.upper().split())
                            # Busca FLEX√çVEL para opera√ß√£o
                            operation_match = (
                                operation_normalized == op_key_norm or
                                operation_normalized in op_key_norm or 
                                op_key_norm in operation_normalized
                            )
                            if operation_match:
                                detail_key = f"{bank_key}|{organ_key}|{op_key}"
                                detail_key_candidates.append(detail_key)
        
        # Processar candidatos do mapeamento detalhado com PRIORIZA√á√ÉO INTELIGENTE
        if detail_key_candidates:
            best_candidate = None
            best_score = 0
            
            for detail_key in detail_key_candidates:
                options = DETAILED_MAPPING.get(detail_key, [])
                if options:
                    details = options[0]  # Usar primeira op√ß√£o da lista
                    
                    # Calcular score de especificidade da chave
                    parts = detail_key.split('|')
                    if len(parts) >= 3:
                        operation_part = parts[2]
                        
                        # Score baseado em especificidade
                        score = 0
                        
                        # 1. Match exato de opera√ß√£o (case sensitive) tem prioridade m√°xima
                        if operation_part == operation_type:
                            score += 1000
                        
                        # 2. Match de opera√ß√£o normalizada
                        elif operation_part.upper() == operation_normalized:
                            score += 500
                        
                        # 3. Opera√ß√µes mais espec√≠ficas (mais palavras) t√™m prioridade
                        word_count = len(operation_part.split())
                        score += word_count * 50
                        
                        # 4. Taxa mais alta tem ligeira prioridade (desempate)
                        taxa = details.get('taxa_storm', '0%').replace('%', '').replace(',', '.')
                        try:
                            taxa_float = float(taxa)
                            score += taxa_float
                        except:
                            pass
                        
                        # Log detalhado para AVERBAI
                        if bank_normalized == "AVERBAI":
                            logging.info(f"üèÜ Candidato: '{detail_key}' | Score={score} | C√≥digo={details.get('codigo_tabela')} | Taxa={details.get('taxa_storm')}")
                        
                        if score > best_score:
                            best_score = score
                            best_candidate = (detail_key, details)
            
            if best_candidate:
                detail_key, details = best_candidate
                logging.info(f"‚úÖ MELHOR CANDIDATO (score={best_score}): {detail_key} -> Codigo={details['codigo_tabela']} | Taxa={details['taxa_storm']}")
                return details
        
        # PRIORIDADE 3: Busca gen√©rica por BANCO + ORG√ÉO (fallback mais amplo)
        bank_organ_key = f"{bank_normalized}|{organ_normalized}"
        if bank_organ_key in BANK_ORGAN_MAPPING:
            options = BANK_ORGAN_MAPPING[bank_organ_key]
            if options:
                # Tentar encontrar a opera√ß√£o mais compat√≠vel
                best_option = None
                best_op_score = 0
                
                for option in options:
                    op_storm = option.get('operacao_storm', '').upper()
                    # Calcular compatibilidade da opera√ß√£o com prioriza√ß√£o inteligente
                    if operation_normalized == op_storm:
                        op_score = 100  # Match exato - m√°xima prioridade
                    elif operation_normalized in op_storm:
                        # Opera√ß√£o buscada est√° contida na opera√ß√£o do Storm
                        # Dar prioridade a opera√ß√µes mais espec√≠ficas (mais palavras)
                        word_count_bonus = len(op_storm.split()) * 5
                        op_score = 50 + word_count_bonus  # Substring match + bonus por especificidade
                    elif op_storm in operation_normalized:
                        # Opera√ß√£o Storm est√° contida na opera√ß√£o buscada
                        op_score = 40  # Substring match reverso
                    elif any(word in op_storm for word in operation_normalized.split()) or any(word in operation_normalized for word in op_storm.split()):
                        # Palavras em comum
                        common_words = len(set(operation_normalized.split()) & set(op_storm.split()))
                        op_score = 10 + (common_words * 2)  # Base + bonus por palavra comum
                    else:
                        op_score = 0
                    
                    # Log detalhado para AVERBAI debug
                    if bank_normalized == "AVERBAI" and operation_normalized and op_score > 0:
                        logging.info(f"üîç AVERBAI Score: '{operation_normalized}' vs '{op_storm}' = {op_score} (C√≥digo: {option.get('codigo_tabela', 'N/A')})")
                    
                    if op_score > best_op_score:
                        best_op_score = op_score
                        best_option = option
                
                if best_option:
                    logging.info(f"‚úÖ Mapeamento GEN√âRICO por BANCO+ORGAO: Codigo={best_option['codigo_tabela']} | Taxa={best_option['taxa_storm']} | Operacao={best_option['operacao_storm']}")
                    return best_option
                else:
                    # Se n√£o encontrou por opera√ß√£o, usar a primeira op√ß√£o
                    first_option = options[0]
                    logging.warning(f"‚ö†Ô∏è Usando primeira op√ß√£o gen√©rica para {bank_normalized}+{organ_normalized}: Codigo={first_option['codigo_tabela']} | Taxa={first_option['taxa_storm']}")
                    return first_option
        
        # Se chegou at√© aqui, n√£o encontrou nenhum mapeamento
        logging.error(f"‚ùå NENHUM mapeamento encontrado para: {bank_normalized} -> {organ_normalized} -> {operation_normalized}")
        return {}
        
    except Exception as e:
        logging.error(f"‚ùå Erro no mapeamento: {str(e)}")
        return {}

def normalize_bank_data(df: pd.DataFrame, bank_type: str) -> pd.DataFrame:
    """Normaliza dados do banco para estrutura padr√£o usando mapeamento correto baseado no arquivo"""
    # Garantir acesso √†s vari√°veis globais
    global ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING
    
    normalized_data = []
    
    logging.info(f"=" * 100)
    logging.info(f"üîß INICIANDO normalize_bank_data para {bank_type} com {len(df)} registros")
    logging.info(f"   Colunas dispon√≠veis: {list(df.columns)}")
    
    # Debug espec√≠fico para PAULISTA
    if bank_type == "PAULISTA":
        logging.error(f"üè¶ NORMALIZE_BANK_DATA: PAULISTA com {len(df)} linhas")
        for i in range(min(3, len(df))):
            row_data = df.iloc[i].to_dict()
            logging.error(f"   Linha {i}: Unnamed:0='{row_data.get('Unnamed: 0', 'N/A')}'")
    
    logging.info(f"=" * 100)
    
    # VALIDA√á√ÉO: Remover linhas completamente vazias
    df = df.dropna(how='all')
    
    # VALIDA√á√ÉO: Verificar se ainda tem dados
    if df.empty:
        logging.error(f"‚ùå {bank_type}: DataFrame vazio ap√≥s remover linhas vazias")
        return pd.DataFrame()
    
    # VALIDA√á√ÉO: Verificar se tem pelo menos 3 colunas
    if len(df.columns) < 3:
        logging.error(f"‚ùå {bank_type}: Muito poucas colunas ({len(df.columns)}) - Colunas: {list(df.columns)}")
        return pd.DataFrame()
    
    # üßπ LIMPEZA DE CARACTERES ESPECIAIS: Aplicar em todas as colunas de texto
    logging.info(f"üßπ {bank_type}: Iniciando limpeza de caracteres especiais...")
    text_columns_cleaned = 0
    
    for column in df.columns:
        if df[column].dtype == 'object':  # Colunas de texto
            original_values = df[column].astype(str)
            df[column] = df[column].astype(str).apply(clean_special_characters)
            
            # Contar quantas c√©lulas foram alteradas
            changes_count = sum(1 for old, new in zip(original_values, df[column]) if old != new)
            if changes_count > 0:
                text_columns_cleaned += 1
                logging.info(f"üßπ {bank_type}: Coluna '{column}' - {changes_count} c√©lulas limpas")
    
    if text_columns_cleaned > 0:
        logging.info(f"üßπ {bank_type}: Limpeza conclu√≠da - {text_columns_cleaned} colunas processadas")
    else:
        logging.info(f"üßπ {bank_type}: Nenhum caractere especial problem√°tico encontrado")
    

    logging.info(f"‚úÖ DataFrame passou valida√ß√µes - {len(df)} registros, {len(df.columns)} colunas")
    
    logging.info(f"Ap√≥s limpeza: {len(df)} registros v√°lidos com {len(df.columns)} colunas")
    
    for idx, row in df.iterrows():
        logging.info(f"üîç PROCESSANDO linha {idx}: {dict(row)}")
        
        # Pular linhas que s√£o claramente cabe√ßalhos ou metadados
        row_str = ' '.join([str(val).lower() for val in row.values if pd.notna(val)])
        
        # DEBUG: Log para PAULISTA mostrando a row_str constru√≠da
        if bank_type == "PAULISTA":
            logging.info(f"üîç PAULISTA linha {idx}: row_str = '{row_str[:100]}...'")
        
        # Detectar linhas de metadados/cabe√ßalho
        metadata_indicators = [
            'relat√≥rio', 'relatorio', 'total de registros', 'total:', 'p√°gina',
            'data de emiss√£o', 'data de extra√ß√£o', 'banco:', 'per√≠odo',
            'nome do banco', 'agencia:', 'conta:', 'saldo:'
        ]
        
        # Detectar linhas de cabe√ßalho espec√≠ficas do BANCO PAULISTA
        paulista_header_indicators = [
            'n¬∫ proposta', 'numero proposta', 'data captura', 'banco paulista',
            'cpf/cnpj proponente', 'nome do proponente', 'valor solicitado',
            'quant. parcelas', 'usu√°rio digitador', 'usuario digitador',
            'rela√ß√£o de propostas', 'anal√≠tico', 'relat√≥rio', 'relatorio'
        ]
        
        # Verificar se deve pular linha de cabe√ßalho
        is_header = any(indicator in row_str for indicator in metadata_indicators + paulista_header_indicators)
        
        if bank_type == "PAULISTA":
            # Log bem detalhado para PAULISTA
            primeira_col = row.get('Unnamed: 0', '')
            logging.error(f"üîç PAULISTA linha {idx}: Primeira coluna = '{primeira_col}'")
            logging.error(f"üîç PAULISTA linha {idx}: row_str = '{row_str[:50]}...'")
            logging.error(f"üîç PAULISTA linha {idx}: √â cabe√ßalho? {is_header}")
            if is_header:
                matched_indicators = [ind for ind in metadata_indicators + paulista_header_indicators if ind in row_str]
                logging.error(f"üìã PAULISTA: Indicadores encontrados: {matched_indicators}")
        
        if is_header:
            if bank_type == "PAULISTA":
                logging.error(f"üìã PAULISTA: Pulando linha de cabe√ßalho: {row_str[:50]}...")
            else:
                logging.debug(f"Pulando linha de cabe√ßalho/metadados: {row_str[:100]}")
            continue
        else:
            # Esta linha N√ÉO √© cabe√ßalho - vai processar
            if bank_type == "PAULISTA":
                logging.error(f"‚úÖ PAULISTA linha {idx}: VAI PROCESSAR - Primeira coluna: '{row.get('Unnamed: 0', '')}')")
        
        normalized_row = {}
        
        logging.debug(f"üîß Normalizando linha {idx} para banco: {bank_type}")
        
        if bank_type == "AVERBAI":
            # Mapeamento AVERBAI - Baseado na estrutura REAL do map_relat_atualizados.txt
            # Detectar tipo de opera√ß√£o do campo TipoProduto ou outro campo
            tipo_produto = str(row.get('TipoProduto', '')).strip()
            tipo_operacao_averbai = "Margem Livre (Novo)"  # padr√£o
            orgao_averbai = ""
            
            # Identificar ORGAO e tipo de opera√ß√£o baseado nos campos do arquivo
            tipo_produto_upper = tipo_produto.upper()
            
            # Detectar tipo de opera√ß√£o PRIMEIRO (isso afeta o √≥rg√£o)
            if 'PORTABILIDADE' in tipo_produto_upper and 'REFIN' in tipo_produto_upper:
                tipo_operacao_averbai = "Refinanciamento Da Portabilidade"  # ‚úÖ Corrigido para mai√∫sculo "Da"
            elif 'PORTABILIDADE' in tipo_produto_upper:
                tipo_operacao_averbai = "Portabilidade"
            elif 'REFIN' in tipo_produto_upper:
                tipo_operacao_averbai = "Refinanciamento"
            else:
                tipo_operacao_averbai = "Margem Livre (Novo)"
            
            # Detectar ORGAO - CR√çTICO: Portabilidade/Refinanciamento s√£o sempre INSS no CSV!
            if tipo_operacao_averbai in ["Portabilidade", "Refinanciamento Da Portabilidade", "Refinanciamento"]:
                # Portabilidade e Refinanciamento est√£o cadastrados como INSS no CSV
                orgao_averbai = 'INSS'
            elif 'INSS' in tipo_produto_upper or 'SAQUE INSS' in tipo_produto_upper:
                orgao_averbai = 'INSS'
            elif 'FGTS' in tipo_produto_upper or 'SAQUE FGTS' in tipo_produto_upper:
                orgao_averbai = 'FGTS'
            else:
                orgao_averbai = 'FGTS'  # Default
            
            # Normalizar nome da tabela AVERBAI removendo espa√ßos extras e varia√ß√µes
            tabela_raw = str(row.get('Tabela', '')).strip()
            # Normalizar: remover espa√ßos extras no in√≠cio/fim e m√∫ltiplos espa√ßos internos
            tabela_normalizada = ' '.join(tabela_raw.split()) if tabela_raw else ""
            
            # üéØ SOLU√á√ÉO DEFINITIVA: Usar c√≥digo direto do arquivo AVERBAI!
            # Campo IdTableComissao j√° tem o c√≥digo correto (1005, 1016, 994, 992, etc)
            codigo_tabela_direto = str(row.get('IdTableComissao', '')).strip()
            cpf_cliente = str(row.get('CpfCliente', '')).strip()
            
            # ÔøΩ FUN√á√ÉO para formatar CPF no padr√£o brasileiro
            def format_cpf(cpf_str):
                """Formata CPF para o padr√£o 000.000.000-00"""
                if not cpf_str:
                    return ""
                
                # Remover tudo que n√£o √© n√∫mero
                cpf_numbers = ''.join(filter(str.isdigit, str(cpf_str)))
                
                # Verificar se tem 11 d√≠gitos
                if len(cpf_numbers) != 11:
                    logging.warning(f"‚ö†Ô∏è CPF inv√°lido (n√£o tem 11 d√≠gitos): '{cpf_str}' -> '{cpf_numbers}'")
                    return cpf_str  # Retornar original se inv√°lido
                
                # Formatar: 000.000.000-00
                cpf_formatted = f"{cpf_numbers[0:3]}.{cpf_numbers[3:6]}.{cpf_numbers[6:9]}-{cpf_numbers[9:11]}"
                return cpf_formatted
            
            # Formatar CPF
            cpf_cliente = format_cpf(cpf_cliente)
            
            # ÔøΩüí∞ FUN√á√ÉO para formatar valores no padr√£o brasileiro
            def format_brazilian_currency(value_str):
                """Converte valores para formato brasileiro: 1.500,39 ou 87,58"""
                if not value_str or str(value_str).strip() in ['', 'nan', 'None', '0']:
                    return "0,00"
                
                try:
                    # Limpar o valor (remover espa√ßos, moeda, etc.)
                    clean_value = str(value_str).strip().replace('R$', '').replace(' ', '')
                    
                    # Se j√° est√° no formato brasileiro, manter
                    if ',' in clean_value and clean_value.count(',') == 1:
                        parts = clean_value.split(',')
                        if len(parts[1]) == 2:  # Duas casas decimais ap√≥s v√≠rgula
                            return clean_value
                    
                    # Converter do formato americano (ponto decimal)
                    if '.' in clean_value:
                        # Separar parte inteira e decimal
                        parts = clean_value.split('.')
                        integer_part = parts[0]
                        decimal_part = parts[1][:2] if len(parts) > 1 else "00"  # Max 2 decimais
                    else:
                        # Sem decimal, assumir valor inteiro
                        integer_part = clean_value
                        decimal_part = "00"
                    
                    # Garantir que decimal tenha 2 d√≠gitos
                    if len(decimal_part) == 1:
                        decimal_part += "0"
                    elif len(decimal_part) == 0:
                        decimal_part = "00"
                    
                    # Converter para float para formatar
                    float_value = float(f"{integer_part}.{decimal_part}")
                    
                    # Formatar no padr√£o brasileiro
                    if float_value >= 1000:
                        # Valores altos: 1.500,39
                        formatted = f"{float_value:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
                    else:
                        # Valores baixos: 87,58
                        formatted = f"{float_value:.2f}".replace('.', ',')
                    
                    return formatted
                    
                except (ValueError, TypeError) as e:
                    logging.warning(f"‚ö†Ô∏è AVERBAI: Erro ao formatar valor '{value_str}': {e}")
                    return str(value_str)  # Retornar original se houver erro
            
            # üîç LOG DEBUG: Campos importantes do AVERBAI
            logging.info(f"üîç AVERBAI Debug - Id: {row.get('Id', 'N/A')}, IdTableComissao: '{codigo_tabela_direto}', CpfCliente: '{cpf_cliente}', NomeCliente: '{row.get('NomeCliente', 'N/A')}'")
            
            # Buscar dados do c√≥digo no CSV para pegar √≥rg√£o e taxa corretos
            orgao_final = orgao_averbai  # Default baseado no TipoProduto
            taxa_final = ""
            operacao_final = tipo_operacao_averbai
            
            if codigo_tabela_direto and codigo_tabela_direto.isdigit():
                # Procurar informa√ß√µes do c√≥digo no CSV
                for key, details in TABELA_MAPPING.items():
                    if details.get('codigo_tabela', '') == codigo_tabela_direto:
                        # Usar dados oficiais do CSV
                        orgao_csv = details.get('orgao_storm', '')
                        taxa_csv = details.get('taxa_storm', '')
                        operacao_csv = details.get('operacao_storm', '')
                        
                        if orgao_csv:
                            orgao_final = orgao_csv
                        if taxa_csv:
                            taxa_final = taxa_csv
                        if operacao_csv:
                            operacao_final = operacao_csv
                            
                        logging.info(f"‚úÖ AVERBAI c√≥digo {codigo_tabela_direto}: {orgao_final} | {operacao_final} | {taxa_final} | CPF: {cpf_cliente}")
                        break
                else:
                    # C√≥digo n√£o encontrado no CSV - usar detec√ß√£o autom√°tica
                    logging.warning(f"‚ö†Ô∏è AVERBAI c√≥digo {codigo_tabela_direto}: N√£o encontrado no CSV, usando detec√ß√£o autom√°tica")
            else:
                # Sem c√≥digo - usar nome da tabela como antes
                codigo_tabela_direto = tabela_normalizada
                logging.warning(f"‚ö†Ô∏è AVERBAI sem IdTableComissao, usando nome da tabela: '{tabela_normalizada}'")
            
            # üí∞ Formatar valores no padr√£o brasileiro
            valor_operacao_br = format_brazilian_currency(row.get('ValorOperacao', ''))
            valor_liberado_br = format_brazilian_currency(row.get('ValorLiquido', ''))
            valor_parcela_br = format_brazilian_currency(row.get('ValorParcela', ''))
            
            # üìä Organizar taxa conforme tabela (garantir formato percentual)
            taxa_formatada = taxa_final
            if taxa_formatada and '%' not in taxa_formatada:
                # Se n√£o tem %, adicionar
                try:
                    # Tentar converter para float e formatar
                    taxa_num = float(taxa_formatada.replace(',', '.'))
                    taxa_formatada = f"{taxa_num:.2f}%".replace('.', ',')
                except:
                    taxa_formatada = f"{taxa_formatada}%"
            
            logging.info(f"üí∞ AVERBAI Proposta {row.get('Id', 'N/A')}: Valores formatados - OPERA√á√ÉO: {valor_operacao_br}, LIBERADO: {valor_liberado_br}, PARCELA: {valor_parcela_br}, TAXA: {taxa_formatada}")
            
            normalized_row = {
                "PROPOSTA": str(row.get('Id', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data', '')).strip(),
                "BANCO": "AVERBAI",
                "ORGAO": orgao_final,  # √ìrg√£o correto do CSV ou detectado
                "TIPO_OPERACAO": operacao_final,  # Opera√ß√£o correta do CSV ou detectada
                "NUMERO_PARCELAS": str(row.get('Prazo', '')).strip(),
                "VALOR_OPERACAO": valor_operacao_br,  # üí∞ FORMATO BRASILEIRO
                "VALOR_LIBERADO": valor_liberado_br,  # üí∞ FORMATO BRASILEIRO
                "USUARIO_BANCO": str(row.get('LoginConsultor', '')).strip(),
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('DataFinaliza√ß√£o', '')).strip() if 'DataFinaliza√ß√£o' in df.columns else "",
                "CPF": cpf_cliente,  # CPF j√° extra√≠do e validado
                "NOME": str(row.get('NomeCliente', '')).strip(),
                "DATA_NASCIMENTO": str(row.get('DataNascimento', '')).strip() if 'DataNascimento' in df.columns else "",
                "TELEFONE": "",    # AVERBAI n√£o tem dados de telefone
                "ENDERECO": "",    # AVERBAI n√£o tem dados de endere√ßo
                "BAIRRO": "",      # AVERBAI n√£o tem dados de bairro
                "CEP": "",         # AVERBAI n√£o tem dados de CEP
                "UF": "",          # AVERBAI n√£o tem dados de UF
                "VALOR_PARCELAS": valor_parcela_br,  # üí∞ FORMATO BRASILEIRO
                "CODIGO_TABELA": codigo_tabela_direto,  # üéØ C√ìDIGO DIRETO DO ARQUIVO!
                "TAXA": taxa_formatada,  # üìä TAXA ORGANIZADA CONFORME TABELA
                "OBSERVACOES": str(row.get('Observa√ß√µes', row.get('Observacoes', row.get('Obs', '')))).strip()
            }
            
        elif bank_type == "DIGIO":
            # Mapeamento BANCO DIGIO S.A. - Suporte para duas estruturas:
            # 1. Estrutura com colunas Unnamed (arquivo XLS original do banco)
            # 2. Estrutura com cabe√ßalhos nomeados (arquivo CSV exportado)
            
            # Verificar se √© estrutura com Unnamed ou com cabe√ßalhos nomeados
            # Checar se a maioria das colunas s√£o Unnamed
            unnamed_count = sum(1 for col in row.index if 'unnamed:' in str(col).lower())
            total_count = len(row.index)
            has_unnamed_structure = unnamed_count > (total_count * 0.5)  # Mais de 50% Unnamed
            
            logging.info(f"üîç DIGIO estrutura: {unnamed_count} Unnamed de {total_count} colunas ({unnamed_count/total_count*100:.1f}%)")
            
            # üîç DIGIO: Se tem 100+ colunas Unnamed, aceitar - √© a estrutura normal
            if unnamed_count == total_count and total_count >= 100:
                logging.info(f"‚úÖ DIGIO: Estrutura normal detectada - {total_count} colunas Unnamed")
            
            if not has_unnamed_structure:
                # Estrutura com cabe√ßalhos nomeados (CSV exportado do banco)
                logging.info("üîç DIGIO: Detectada estrutura CSV com cabe√ßalhos nomeados")
                
                # Mapeamento baseado na estrutura real do CSV DIGIO
                proposta = str(row.get('PROPOSTA', '')).strip()
                tipo_operacao = str(row.get('TIPO_OPERACAO', '')).strip()
                data_cadastro = str(row.get('DATA_CADASTRO', '')).strip()
                situacao = str(row.get('SITUACAO_PROPOSTA', '')).strip()
                data_lancamento = str(row.get('DATA_LANCAMENTO', '')).strip()
                nome_orgao_raw = str(row.get('NOME_ORGAO', '')).strip()
                usuario_digitador_raw = str(row.get('DESCR_USU_DIGITADOR', '')).strip()
                cpf_cliente = str(row.get('CPF_CLIENTE', '')).strip()
                
                # üîç DIGIO: Pular linhas de cabe√ßalho (identificar pelo conte√∫do)
                row_values = ' '.join([str(val) for val in row.values if pd.notna(val)]).upper()
                if ('BANCO DIGIO' in row_values or 'RELAT√ìRIO' in row_values or 'RELATORIO' in row_values or
                    'PROPOSTAS CADASTRADAS' in row_values or 'PROC.:' in row_values or
                    proposta == 'PROPOSTA' or  # Header da tabela
                    not proposta or proposta.upper() in ['PROPOSTA', 'NAN']):
                    logging.info(f"‚è≠Ô∏è DIGIO: Pulando linha de cabe√ßalho - Conte√∫do: {row_values[:100]}")
                    continue
                
                # ‚úÖ DIGIO: Validar se proposta √© n√∫mero v√°lido 
                if (not proposta or not proposta.replace('.', '').isdigit()):
                    logging.debug(f"‚è≠Ô∏è DIGIO: Pulando linha - proposta inv√°lida: '{proposta}'")
                    continue
                
                logging.info(f"‚úÖ DIGIO: Proposta v√°lida encontrada: {proposta}")
                
                # üîß DIGIO: Extrair campos da estrutura CSV real
                usuario_digitador = usuario_digitador_raw if usuario_digitador_raw else ""
                nome_cliente = str(row.get('NOMECLI', '')).strip()
                data_nascimento = str(row.get('DATA_NASCIMENTO', '')).strip()
                qtd_parcelas = str(row.get('QTD_PARCELAS', '')).strip()
                vlr_parcela = str(row.get('VALOR_PARCELA', '')).strip()
                vlr_financiado = str(row.get('VLR FINANCIADO', '')).strip()
                vlr_lib1 = str(row.get('VLR_LIB1', '')).strip()
                
                # C√≥digo e nome de conv√™nio
                cod_convenio = str(row.get('COD_CONVENIO', '')).strip()
                nome_convenio = str(row.get('NOME_CONVENIO', '')).strip()
                
                nome_tabela_para_busca = cod_convenio
                
            else:
                # Estrutura com Unnamed (XLS original do banco)
                logging.info("üîç DIGIO: Detectada estrutura com colunas Unnamed (XLS original)")
                proposta = str(row.get('Unnamed: 3', '')).strip()
                tipo_operacao = str(row.get('Unnamed: 4', '')).strip()
                data_cadastro = str(row.get('Unnamed: 8', '')).strip()
                situacao = str(row.get('Unnamed: 9', '')).strip()
                data_lancamento = str(row.get('Unnamed: 13', '')).strip()
                nome_orgao_raw = str(row.get('Unnamed: 25', '')).strip()
                usuario_digitador_raw = str(row.get('Unnamed: 29', '')).strip()
                cpf_cliente = str(row.get('Unnamed: 31', '')).strip()
                
                # üîç DIGIO: Pular linhas de cabe√ßalho baseado no MAP
                # Verificar se tem "BANCO DIGIO", "RELAT√ìRIO", "PROPOSTA" (como header), etc.
                row_values = ' '.join([str(val) for val in row.values if pd.notna(val)]).upper()
                
                # Linha √© cabe√ßalho se:
                # 1. Cont√©m "BANCO DIGIO", "RELAT√ìRIO" 
                # 2. Proposta = "PROPOSTA" (header da tabela de dados)
                # 3. Campos s√£o nomes de colunas
                if ('BANCO DIGIO' in row_values or 'RELAT√ìRIO' in row_values or 'RELATORIO' in row_values or
                    proposta == 'PROPOSTA' or  # Header da tabela de dados
                    usuario_digitador_raw in ['DESCR_USU_DIGITADOR'] or
                    cpf_cliente in ['CPF_CLIENTE'] or
                    nome_orgao_raw in ['NOME_ORGAO']):
                    logging.info(f"‚è≠Ô∏è DIGIO: Pulando linha de cabe√ßalho detectada - Proposta='{proposta}', Conte√∫do: {row_values[:100]}...")
                    continue
                
                # ‚úÖ DIGIO: Validar se proposta √© n√∫mero v√°lido (estrutura Unnamed)
                proposta_str = str(proposta).strip()
                if (not proposta_str or proposta_str.lower() in ['nan', 'none', ''] or 
                    not proposta_str.replace('.', '').isdigit()):
                    logging.debug(f"‚è≠Ô∏è DIGIO: Pulando linha - proposta inv√°lida: '{proposta}' (n√£o √© n√∫mero)")
                    continue
                
                logging.info(f"‚úÖ DIGIO: Proposta v√°lida encontrada: {proposta}")
                
                # üîß DIGIO: Manter underscore do usu√°rio digitador no formato original
                # Exemplo: "02579846158_202902" (manter como est√°)
                usuario_digitador = usuario_digitador_raw if usuario_digitador_raw else ""
                nome_cliente = str(row.get('Unnamed: 32', '')).strip()
                data_nascimento = str(row.get('Unnamed: 33', '')).strip()
                qtd_parcelas = str(row.get('Unnamed: 48', '')).strip()
                vlr_parcela = str(row.get('Unnamed: 49', '')).strip()
                vlr_financiado = str(row.get('Unnamed: 50', '')).strip()
                
                # DIGIO: Extrair c√≥digos e nomes de conv√™nio (Unnamed: 53 e 54)
                cod_convenio = str(row.get('Unnamed: 53', '')).strip()  # Ex: 002035, 001717
                nome_convenio = str(row.get('Unnamed: 54', '')).strip()  # Ex: "PORT+REFIN VINCULADO-1-96X-1,39 A 1,85-T"
                
                vlr_lib1 = str(row.get('Unnamed: 59', '')).strip()
                
                # DIGIO: Manter COD_CONVENIO original (ex: 002035 mant√©m como 002035)
                # N√ÉO remover zeros √† esquerda - usar c√≥digo exato do arquivo
                # cod_convenio j√° est√° correto como string original
                
                nome_tabela_para_busca = nome_convenio if nome_convenio else cod_convenio
            
            # Log para debug do DIGIO - EXPANDIDO
            logging.info(f"üîç DIGIO campos principais: Proposta='{proposta}', TipoOp='{tipo_operacao}', Orgao='{nome_orgao_raw}'")
            logging.info(f"üîç DIGIO tabela: COD_CONVENIO='{cod_convenio}' | NOME_CONVENIO='{nome_convenio}'")
            logging.info(f"üîç DIGIO valores: QtdParc={qtd_parcelas}, VlrFinanc={vlr_financiado}, VlrLib={vlr_lib1}")
            logging.info(f"üîç DIGIO cliente: CPF='{cpf_cliente}', Nome='{nome_cliente}', Usuario='{usuario_digitador}'")
            logging.info(f"üîç DIGIO estrutura: has_unnamed={has_unnamed_structure}, unnamed_count={unnamed_count}/{total_count}")
            
            # ‚úÖ DIGIO: Valida√ß√µes m√≠nimas para prosseguir com o processamento
            if not proposta or len(str(proposta).strip()) == 0:
                logging.error(f"‚ùå DIGIO: Proposta vazia ou inv√°lida - pulando linha")
                continue
                
            # Log final antes de prosseguir
            logging.info(f"‚úÖ DIGIO: Linha v√°lida encontrada - prosseguindo com processamento")

            
            # MELHORADO: Detec√ß√£o inteligente de ORGAO DIGIO baseada no map_relat_atualizados.txt
            def detect_digio_organ(nome_orgao, nome_empregador="", cod_empregador=""):
                """Detecta √≥rg√£o do DIGIO baseado nos campos NOME_ORGAO, NOME_EMPREGADOR"""
                orgao_upper = nome_orgao.upper() if nome_orgao else ""
                empregador_upper = nome_empregador.upper() if nome_empregador else ""
                
                # Log para debug
                logging.info(f"üèõÔ∏è DIGIO detectando √≥rg√£o: NOME_ORGAO='{orgao_upper}' | NOME_EMPREGADOR='{empregador_upper}' | COD_EMP='{cod_empregador}'")
                
                # Baseado nos exemplos do mapeamento:
                # NOME_ORGAO: INSS, PREFEITURA DE B, PREFEITURA DE L, PREFEITURA DE S
                # NOME_EMPREGADOR: INSS, PREF BAURU SP, PREF LINS - SP, PREF SERTAOZINH
                
                # Prioridade 1: INSS (mais comum)
                if 'INSS' in orgao_upper or 'INSS' in empregador_upper:
                    return 'INSS'
                
                # Prioridade 2: Prefeituras espec√≠ficas (usar formato do empregador que √© mais espec√≠fico)
                if 'BAURU' in empregador_upper:
                    return 'PREF BAURU SP'
                elif 'LINS' in empregador_upper:
                    return 'PREF LINS - SP'
                elif 'AGUDOS' in empregador_upper:
                    return 'PREF AGUDOS - S'
                elif 'JABOTICABA' in empregador_upper:
                    return 'PREF JABOTICABA'
                elif 'SERTAOZINHO' in empregador_upper or 'SERTAOZINH' in empregador_upper:
                    return 'PREF SERTAOZINHO - SP'
                elif 'PREFEITURA DE B' in orgao_upper:
                    return 'PREF BAURU SP'  # B = BAURU
                elif 'PREFEITURA DE L' in orgao_upper:
                    return 'PREF LINS - SP'  # L = LINS
                elif 'PREFEITURA DE S' in orgao_upper:
                    return 'PREF SERTAOZINHO - SP'  # S = SERTAOZINHO
                elif 'PREFEITURA DE A' in orgao_upper:
                    return 'PREF AGUDOS - S'  # A = AGUDOS
                elif 'PREF' in empregador_upper or 'PREFEITURA' in orgao_upper:
                    # Prefeitura gen√©rica - usar formato do empregador
                    return empregador_upper if empregador_upper else orgao_upper.replace('PREFEITURA', 'PREF').strip()
                
                # Default: INSS
                return 'INSS'
            
            # Se tem estrutura nomeada, usar o campo ORGAO diretamente, sen√£o detectar
            if not has_unnamed_structure:
                # J√° temos o √≥rg√£o normalizado no CSV
                nome_orgao = nome_orgao_raw if nome_orgao_raw else 'INSS'
            else:
                # Detectar √≥rg√£o a partir dos campos Unnamed
                nome_empregador = str(row.get('Unnamed: 23', '')).strip()
                cod_empregador = str(row.get('Unnamed: 17', '')).strip()
                nome_orgao = detect_digio_organ(nome_orgao_raw, nome_empregador, cod_empregador)
            
            # MELHORADO: Detec√ß√£o inteligente de tipo de opera√ß√£o DIGIO
            def detect_digio_operation(tipo_op, tabela_nome=""):
                """Detecta tipo de opera√ß√£o do DIGIO de forma inteligente"""
                tipo_upper = tipo_op.upper() if tipo_op else ""
                tabela_upper = tabela_nome.upper() if tabela_nome else ""
                
                logging.info(f"üîß DIGIO detectando opera√ß√£o: tipo='{tipo_upper}' | tabela='{tabela_upper[:50]}...'")
                
                # Analisar tanto o tipo quanto o nome da tabela
                combined_text = f"{tipo_upper} {tabela_upper}"
                
                # Prioridade 1: Refinanciamento + Portabilidade (mais espec√≠fico primeiro)
                if any(x in combined_text for x in ['REFIN DA PORT', 'REFINANCIAMENTO DA PORTABILIDADE', 'REFIN PORT', 'REFIN PORTABILIDADE']):
                    return "Refinanciamento da Portabilidade"
                
                # Prioridade 2: Portabilidade + Refin (diferente do anterior)
                elif any(x in combined_text for x in ['PORT+REFIN', 'PORTABILIDADE + REFIN', 'PORT REFIN']):
                    return "Portabilidade + Refin"
                
                # Prioridade 2.5: Portabilidade simples
                elif 'PORTABILIDADE' in combined_text and 'REFIN' not in combined_text:
                    return "Portabilidade"
                
                # Prioridade 3: Refinanciamento simples
                elif 'REFIN' in combined_text and 'PORT' not in combined_text:
                    return "Refinanciamento"
                
                # Prioridade 4: Margem Livre (mais comum)
                else:
                    return "Margem Livre (Novo)"
            
            # ‚úÖ CORRIGIDO: Se estrutura nomeada, usar TIPO DE OPERACAO do arquivo diretamente
            if not has_unnamed_structure:
                # Arquivo j√° processado tem tipo correto
                tipo_operacao_norm = tipo_operacao if tipo_operacao else "Margem Livre (Novo)"
                logging.info(f"‚úÖ DIGIO usando tipo de opera√ß√£o do arquivo: '{tipo_operacao_norm}'")
            else:
                # Estrutura XLS original - detectar tipo
                tipo_operacao_norm = detect_digio_operation(tipo_operacao, nome_convenio)
                logging.info(f"üîç DIGIO tipo detectado: '{tipo_operacao_norm}'")
                
            # üìû DIGIO: Extrair dados de contato e endere√ßo (se dispon√≠vel)
            telefone = ""
            if not has_unnamed_structure:
                # Estrutura CSV nomeada
                tel_cliente = str(row.get('TEL_CLIENTE', '')).strip()
                cel_cliente = str(row.get('CEL_CLIENTE', '')).strip()
                telefone = cel_cliente if cel_cliente else tel_cliente
                
                endereco = str(row.get('END_CLIENTE', '')).strip()
                num_endereco = str(row.get('NUM_END_CLIENTE', '')).strip()
                complemento = str(row.get('COMPLEMENTO', '')).strip()
                endereco_completo = f"{endereco}, {num_endereco}".strip(", ")
                if complemento:
                    endereco_completo += f", {complemento}"
                
                bairro = str(row.get('BAIRRO', '')).strip()
                cep = str(row.get('CEP_CLIENTE', '')).strip()
                uf = str(row.get('UF_CLIENTE', '')).strip()
            else:
                # Estrutura XLS com Unnamed (baseado no MAP)
                tel_cliente = str(row.get('Unnamed: 43', '')).strip()  # TEL_CLIENTE
                cel_cliente = str(row.get('Unnamed: 44', '')).strip()  # CEL_CLIENTE
                telefone = cel_cliente if cel_cliente else tel_cliente
                
                endereco = str(row.get('Unnamed: 37', '')).strip()  # END_CLIENTE
                num_endereco = str(row.get('Unnamed: 38', '')).strip()  # NUM_END_CLIENTE
                complemento = str(row.get('Unnamed: 39', '')).strip()  # COMPLEMENTO
                endereco_completo = f"{endereco}, {num_endereco}".strip(", ")
                if complemento and complemento not in ['nan', '']:
                    endereco_completo += f", {complemento}"
                
                bairro = str(row.get('Unnamed: 40', '')).strip()  # BAIRRO
                cep = str(row.get('Unnamed: 42', '')).strip()  # CEP_CLIENTE  
                uf = str(row.get('Unnamed: 41', '')).strip()  # UF_CLIENTE
            
            normalized_row = {
                "PROPOSTA": proposta,
                "DATA_CADASTRO": data_cadastro,
                "BANCO": "BANCO DIGIO S.A.",  # Nome completo como no relat_orgaos.csv
                "ORGAO": nome_orgao,
                "TIPO_OPERACAO": tipo_operacao_norm,
                "NUMERO_PARCELAS": qtd_parcelas,
                "VALOR_OPERACAO": vlr_financiado,
                "VALOR_LIBERADO": vlr_lib1 if vlr_lib1 else vlr_financiado,
                "USUARIO_BANCO": usuario_digitador,
                "SITUACAO": situacao,
                "DATA_PAGAMENTO": data_lancamento,
                "CPF": cpf_cliente,
                "NOME": nome_cliente,
                "DATA_NASCIMENTO": data_nascimento,
                "TELEFONE": telefone,
                "ENDERECO": endereco_completo.strip(", "),
                "BAIRRO": bairro,
                "CEP": cep,
                "UF": uf,
                "VALOR_PARCELAS": vlr_parcela,
                "CODIGO_TABELA": cod_convenio,  # ‚úÖ DIGIO: Usar COD_CONVENIO direto (5076, 5077, 1720, etc)
                "TAXA": "",  # Taxa deve vir do arquivo ou ser buscada depois
                "OBSERVACOES": str(row.get('Unnamed: 11', row.get('Observa√ß√µes', ''))).strip()  # NOME_ATIVIDADE como observa√ß√£o
            }
            
            # ‚úÖ DIGIO: N√ÉO aplicar mapeamento! 
            # O arquivo DIGIO j√° vem com c√≥digos corretos (5076, 5077, 1720, 2055, etc)
            # Diferente de VCTEX que precisa converter "Tabela EXP" ‚Üí "TabelaEXP"
            logging.info(f"‚úÖ DIGIO FINAL: Proposta={proposta} | C√≥digo='{cod_convenio}' | √ìrg√£o='{normalized_row['ORGAO']}' | Opera√ß√£o='{normalized_row['TIPO_OPERACAO']}'")
            logging.info(f"   ‚îî‚îÄ Usando c√≥digo direto do arquivo (SEM mapeamento)")
            
            
        elif bank_type == "PRATA":
            # Mapeamento baseado na estrutura real do Prata
            # PRATA usa prazo em MESES, precisa dividir por 12
            prazo = str(row.get('Prazo proposta', '')).strip()
            numero_parcelas = ""
            if prazo and prazo.isdigit():
                numero_parcelas = str(int(prazo) // 12) if int(prazo) >= 12 else prazo
            
            # PRATA: Pegar campo Usuario e limpar (remover nome entre par√™nteses)
            usuario_prata = str(row.get('Nome do Vendedor', '')).strip()
            if not usuario_prata:
                usuario_prata = str(row.get('Usu√°rio (acesso login)', '')).strip()
            
            # Limpar: remover tudo ap√≥s o email (ex: "lprodrigues@q-faz.com (LARIANA PITON RODRIGUES)" ‚Üí "lprodrigues@q-faz.com")
            if '(' in usuario_prata:
                usuario_prata = usuario_prata.split('(')[0].strip()
            
            normalized_row = {
                "PROPOSTA": str(row.get('N√∫mero da Proposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data da opera√ß√£o', '')).strip(),
                "BANCO": "BANCO PRATA DIGITAL",
                "ORGAO": "FGTS",
                "TIPO_OPERACAO": "Margem Livre (Novo)",
                "NUMERO_PARCELAS": numero_parcelas,
                "VALOR_OPERACAO": str(row.get('Valor da Emiss√£o', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor Desembolso', '')).strip(),
                "USUARIO_BANCO": usuario_prata,
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data do Desembolso', '')).strip(),
                "CPF": str(row.get('CPF do Cliente', '')).strip(),
                "NOME": str(row.get('Nome do Cliente', '')).strip(),
                "DATA_NASCIMENTO": "",
                "TELEFONE": str(row.get('Telefone', row.get('Tel', row.get('Fone', '')))).strip(),
                "ENDERECO": str(row.get('Endereco', row.get('Endere√ßo', row.get('End', '')))).strip(), 
                "BAIRRO": str(row.get('Bairro', '')).strip(),
                "CEP": str(row.get('CEP', '')).strip(),
                "UF": str(row.get('UF', row.get('Estado', ''))).strip(),
                "VALOR_PARCELAS": "",  # PRATA n√£o fornece valor da parcela
                "CODIGO_TABELA": str(row.get('Tabela', '')).strip(),  # Nome da tabela do banco
                "TAXA": "",  # Vazio para buscar no relat_orgaos.csv
                "OBSERVACOES": str(row.get('Observa√ß√µes', row.get('Observacoes', row.get('Obs', '')))).strip()
            }
            
        elif bank_type == "VCTEX":
            # Mapeamento BANCO VCTEX - Estrutura REAL dos arquivos
            # VCTEX usa prazo em MESES, precisa dividir por 12
            prazo_vctex = str(row.get('Prazo proposta', '')).strip()
            numero_parcelas_vctex = ""
            if prazo_vctex and prazo_vctex.isdigit():
                numero_parcelas_vctex = str(int(prazo_vctex) // 12) if int(prazo_vctex) >= 12 else prazo_vctex
            
            # Fun√ß√£o para busca flex√≠vel de datas VCTEX - CORRIGIDA
            def get_vctex_date_field(row, date_type='cadastro'):
                """Busca flex√≠vel por campo de data no VCTEX - prioridade corrigida"""
                
                logging.debug(f"üîç VCTEX buscando campo de data tipo '{date_type}' nas colunas dispon√≠veis")
                
                if date_type == 'cadastro':
                    # Prioridade ALTA para campos espec√≠ficos de cria√ß√£o/opera√ß√£o
                    cadastro_high_priority = [
                        'Data da opera√ß√£o', 'Data da operacao', 'Data operacao', 'Data Operacao',
                        'Data de criacao', 'Data de cria√ß√£o', 'Data criacao', 'Data cria√ß√£o',
                        'Data contrato', 'Data Contrato', 'Data do contrato', 'Data do Contrato',
                        'Data inclusao', 'Data inclus√£o', 'Data de inclusao', 'Data de inclus√£o'
                    ]
                    
                    # Prioridade M√âDIA para campos de cadastro gen√©ricos  
                    cadastro_medium_priority = [
                        'Data cadastro', 'Data Cadastro', 'Data de cadastro', 'Data de Cadastro',
                        'Data assinatura', 'Data Assinatura', 'DT_OPERACAO', 'DT_CADASTRO', 'DT_CRIACAO'
                    ]
                    
                    # Buscar por prioridade
                    for field_list in [cadastro_high_priority, cadastro_medium_priority]:
                        for field in field_list:
                            if field in row and str(row.get(field, '')).strip() and str(row.get(field, '')).strip() != 'nan':
                                found_date = str(row.get(field, '')).strip()
                                logging.info(f"‚úÖ VCTEX DATA_CADASTRO encontrada em '{field}': {found_date}")
                                return found_date
                            
                elif date_type == 'pagamento':
                    # Prioridade ALTA para campos espec√≠ficos de pagamento/finaliza√ß√£o
                    pagamento_high_priority = [
                        'Data pagamento Opera√ß√£o', 'Data pagamento Operacao', 'Data pagamento',
                        'Data de pagamento', 'Data Pagamento', 'Data liquidacao', 'Data liquida√ß√£o',
                        'Data liberacao', 'Data libera√ß√£o', 'Data credito', 'Data cr√©dito'
                    ]
                    
                    # Prioridade M√âDIA para campos de finaliza√ß√£o/vencimento
                    pagamento_medium_priority = [
                        'Data finalizacao', 'Data finaliza√ß√£o', 'Data vencimento', 'Data Vencimento',
                        'Data de vencimento', 'Data de Vencimento', 'DT_PAGAMENTO', 'DT_FINALIZACAO', 'DT_LIQUIDACAO'
                    ]
                    
                    # Buscar por prioridade
                    for field_list in [pagamento_high_priority, pagamento_medium_priority]:
                        for field in field_list:
                            if field in row and str(row.get(field, '')).strip() and str(row.get(field, '')).strip() != 'nan':
                                found_date = str(row.get(field, '')).strip()
                                logging.info(f"‚úÖ VCTEX DATA_PAGAMENTO encontrada em '{field}': {found_date}")
                                return found_date
                
                # üö´ EVITAR campos gen√©ricos que causam confus√£o entre cadastro e pagamento
                # Vamos ser mais restritivos para evitar pegar campos errados
                logging.warning(f"‚ö†Ô∏è VCTEX: Nenhum campo espec√≠fico de {date_type} encontrado!")
                
                # Log das colunas dispon√≠veis para debug
                available_columns = [col for col in row.index if 'data' in col.lower() or 'date' in col.lower()]
                if available_columns:
                    logging.info(f"üîç VCTEX: Colunas relacionadas a datas dispon√≠veis: {available_columns}")
                
                return ""  # Retornar vazio ao inv√©s de tentar campo gen√©rico
            
            # Fun√ß√£o para validar e normalizar formato de data - MELHORADA
            def validate_and_normalize_date(date_str, field_name=""):
                """Valida, normaliza e detecta problemas em datas do VCTEX"""
                if not date_str or date_str.strip() == "" or str(date_str).strip().lower() in ['nan', 'none', 'null']:
                    logging.debug(f"üîç VCTEX {field_name}: Campo vazio ou inv√°lido")
                    return ""
                
                date_clean = str(date_str).strip()
                
                # Verificar padr√µes de data v√°lidos com regex mais rigoroso
                import re
                from datetime import datetime
                
                # Padr√µes aceitos com valida√ß√£o mais rigorosa
                date_patterns = [
                    (r'^\d{1,2}/\d{1,2}/\d{4}$', '%d/%m/%Y'),     # DD/MM/YYYY
                    (r'^\d{1,2}/\d{1,2}/\d{2}$', '%d/%m/%y'),     # DD/MM/YY  
                    (r'^\d{1,2}-\d{1,2}-\d{4}$', '%d-%m-%Y'),     # DD-MM-YYYY
                    (r'^\d{1,2}-\d{1,2}-\d{2}$', '%d-%m-%y'),     # DD-MM-YY
                    (r'^\d{4}-\d{1,2}-\d{1,2}$', '%Y-%m-%d'),     # YYYY-MM-DD
                    (r'^\d{1,2}\.\d{1,2}\.\d{4}$', '%d.%m.%Y'),   # DD.MM.YYYY
                    (r'^\d{4}/\d{1,2}/\d{1,2}$', '%Y/%m/%d')      # YYYY/MM/DD
                ]
                
                # Tentar validar com cada padr√£o
                for pattern, date_format in date_patterns:
                    if re.match(pattern, date_clean):
                        try:
                            # Tentar fazer o parse da data para validar se √© real
                            parsed_date = datetime.strptime(date_clean, date_format)
                            
                            # Verificar se a data faz sentido (n√£o muito antiga nem futura)
                            current_year = datetime.now().year
                            if parsed_date.year < 1990 or parsed_date.year > current_year + 1:
                                logging.warning(f"‚ö†Ô∏è VCTEX {field_name}: Ano suspeito ({parsed_date.year}) na data '{date_clean}'")
                            
                            logging.info(f"‚úÖ VCTEX {field_name}: Data v√°lida '{date_clean}' (formato: {date_format})")
                            return date_clean
                            
                        except ValueError as ve:
                            logging.warning(f"‚ö†Ô∏è VCTEX {field_name}: Data inv√°lida '{date_clean}' - erro: {ve}")
                            continue
                
                # üîß MODO FLEX√çVEL: Se valida√ß√£o rigorosa falhou, aceitar formatos razo√°veis
                logging.warning(f"‚ö†Ô∏è VCTEX {field_name}: Formato n√£o padr√£o: '{date_clean}' - aplicando modo flex√≠vel")
                
                # üîß CORRE√á√ÉO ESPEC√çFICA: Tratar timestamps (DD/MM/YYYY HH:MM:SS)
                if len(date_clean) > 10:  # Poss√≠vel timestamp/datetime
                    logging.info(f"üîß VCTEX {field_name}: Detectado timestamp: '{date_clean}'")
                    
                    # Padr√£o brasileiro: DD/MM/YYYY HH:MM:SS
                    timestamp_br_match = re.match(r'^(\d{1,2}/\d{1,2}/\d{4})\s+\d{1,2}:\d{2}', date_clean)
                    if timestamp_br_match:
                        date_only = timestamp_br_match.group(1)
                        logging.info(f"‚úÖ VCTEX {field_name}: Extra√≠do data brasileira: '{date_only}'")
                        return date_only
                    
                    # Padr√£o ISO: YYYY-MM-DD HH:MM:SS
                    timestamp_iso_match = re.match(r'^(\d{4}-\d{2}-\d{2})\s+\d{1,2}:\d{2}', date_clean) 
                    if timestamp_iso_match:
                        date_only = timestamp_iso_match.group(1)
                        logging.info(f"‚úÖ VCTEX {field_name}: Extra√≠do data ISO: '{date_only}'")
                        return date_only
                    
                    # Tentar extrair primeiros 10 caracteres se parecer data
                    if re.match(r'^\d{4}-\d{2}-\d{2}', date_clean):
                        date_part = date_clean[:10]
                        logging.info(f"üîß VCTEX {field_name}: Extraindo primeiros 10 chars: '{date_part}'")
                        return date_part
                
                # üÜò MODO EMERG√äNCIA: Se tem qualquer padr√£o de data, ACEITAR!
                # Formatos que podem existir no mundo real
                flexible_patterns = [
                    r'\d{1,4}[/.-]\d{1,2}[/.-]\d{1,4}',  # Qualquer separador com n√∫meros
                    r'\d{8}',  # DDMMYYYY ou YYYYMMDD
                    r'\d{6}',  # DDMMYY ou YYMMDD
                ]
                
                for pattern in flexible_patterns:
                    if re.search(pattern, date_clean):
                        logging.warning(f"‚ö†Ô∏è VCTEX {field_name}: ACEITANDO formato flex√≠vel: '{date_clean}'")
                        return date_clean  # Aceitar como est√°
                
                # Se realmente n√£o parece data de jeito nenhum
                logging.error(f"‚ùå VCTEX {field_name}: Realmente n√£o parece data: '{date_clean}' - retornando vazio")
                return ""
            
            # Pegar campos brutos
            convenio_raw = str(row.get('Conv√™nio', row.get('Nome da entidade consignataria', ''))).strip().upper()
            tabela_raw = str(row.get('Tabela', row.get('Nome tabela juros', ''))).strip()
            taxa_raw = str(row.get('Taxa Juros Aplicada', row.get('Taxa de juros', ''))).strip()
            
            # üéØ VCTEX: Normalizar nome da tabela para fazer matching correto no relat_orgaos.csv
            # O relat_orgaos.csv tem: "TABELA BANCO" (ex: "Tabela Exponencial") ‚Üí "CODIGO TABELA STORM" (ex: "TabelaExponencial")
            # Normalizar tabela_raw para garantir prefixo "Tabela" quando necess√°rio
            def normalize_vctex_table_name(table_name):
                """Normaliza nome da tabela VCTEX para matching no mapeamento
                REGRAS:
                - "Tabela EXP" ‚Üí "TabelaEXP"  
                - "Tabela Exponencial" ‚Üí "TabelaExponencial"
                - "EXP" ‚Üí "TabelaEXP"
                - "Exponencial" ‚Üí "TabelaExponencial"
                """
                if not table_name:
                    return ""
                
                table_clean = str(table_name).strip()
                
                # üéØ CASOS ESPEC√çFICOS COMPLETOS (com "Tabela" no nome)
                # EXP e Exponencial s√£o DIFERENTES!
                if table_clean.upper() == "TABELA EXP":
                    logging.info(f"üîß VCTEX: 'Tabela EXP' ‚Üí 'TabelaEXP' (EXP √© diferente de Exponencial)")
                    return "TabelaEXP"
                elif table_clean.upper() == "TABELA EXPONENCIAL":
                    logging.info(f"üîß VCTEX: 'Tabela Exponencial' ‚Üí 'TabelaExponencial' (Exponencial √© diferente de EXP)")
                    return "TabelaExponencial"
                elif table_clean.upper() == "TABELA LINEAR":
                    return "TabelaLinear"
                elif table_clean.upper() == "TABELA VCT":
                    return "TabelaVCT"
                elif table_clean.upper() == "TABELA RELAX":
                    return "TabelaRelax"
                elif table_clean.upper() == "TABELA VAMO":
                    return "TabelaVamo"
                
                # Se j√° come√ßa com "Tabela" e n√£o foi tratado acima, manter como est√°
                if table_clean.startswith("Tabela"):
                    return table_clean
                
                # üéØ CASOS SEM PREFIXO "Tabela"
                table_upper = table_clean.upper()
                
                # EXP e Exponencial s√£o DIFERENTES - tratar separadamente
                if table_upper == "EXP":
                    logging.info(f"üîß VCTEX: 'EXP' ‚Üí 'TabelaEXP' (mant√©m EXP, n√£o confundir com Exponencial)")
                    return "TabelaEXP"
                elif table_upper == "EXPONENCIAL":
                    logging.info(f"üîß VCTEX: 'Exponencial' ‚Üí 'TabelaExponencial' (mant√©m Exponencial, n√£o confundir com EXP)")
                    return "TabelaExponencial"
                
                # Outros casos espec√≠ficos
                elif table_upper == "VCT":
                    return "TabelaVCT"
                elif table_upper == "RELAX":
                    return "TabelaRelax" 
                elif table_upper == "VAMO":
                    return "TabelaVamo"
                elif table_upper == "LINEAR":
                    return "TabelaLinear"
                elif table_upper in ["DIFERENCIADA", "ESPECIAL", "PADR√ÉO", "PADRAO"]:
                    return f"Tabela{table_clean}"
                
                # Para outros casos, adicionar prefixo Tabela mantendo nome original
                normalized = f"Tabela{table_clean}"
                logging.info(f"üîß VCTEX: Tabela gen√©rica '{table_clean}' ‚Üí '{normalized}'")
                return normalized
            
            tabela_normalized = normalize_vctex_table_name(tabela_raw)
            logging.info(f"üìã VCTEX: Tabela original: '{tabela_raw}' ‚Üí Normalizada: '{tabela_normalized}' (ser√° usada para buscar CODIGO TABELA STORM no CSV)")
            logging.info(f"üîç VCTEX DEBUG: tabela_raw='{tabela_raw}' | tabela_normalized='{tabela_normalized}' | EXP‚â†Exponencial")
            
            # Normalizar ORGAO usando CONVENIO e TABELA como indicadores
            orgao_vctex = ""
            
            # Primeiro, tentar pelo campo Conv√™nio
            if 'FGTS' in convenio_raw or 'FUNDO' in convenio_raw:
                orgao_vctex = 'FGTS'
            elif 'INSS' in convenio_raw or 'PREVID' in convenio_raw:
                orgao_vctex = 'INSS'
            # Se Conv√™nio n√£o ajudou, usar a TABELA como indicador
            elif 'INSS' in tabela_raw.upper():
                orgao_vctex = 'INSS'
            elif 'FGTS' in tabela_raw.upper():
                orgao_vctex = 'FGTS'
            # Detectar por nome de tabela t√≠picas
            elif any(x in tabela_raw.upper() for x in ['VAMO', 'EXPONENCIAL', 'RELAX', 'VCT', 'EXP']):
                # Se tem tabela t√≠pica de FGTS e n√£o mencionou INSS
                orgao_vctex = 'FGTS'
            else:
                # Default para INSS se n√£o conseguiu determinar
                orgao_vctex = 'INSS'
            
            # Garantir que TAXA tenha valor, mesmo que seja 0,00%
            if not taxa_raw or taxa_raw == 'nan':
                taxa_raw = '0,00%'
            elif '%' not in taxa_raw:
                taxa_raw = taxa_raw + '%' if taxa_raw else '0,00%'
            
            # üîç DEBUG: Log das colunas de data dispon√≠veis
            date_columns = [col for col in row.index if any(word in col.lower() for word in ['data', 'date'])]
            logging.info(f"üîç VCTEX Proposta {row.get('N√∫mero do Contrato', 'N/A')}: Colunas de data dispon√≠veis: {date_columns}")
            
            # Buscar datas usando fun√ß√£o flex√≠vel
            data_cadastro_raw = get_vctex_date_field(row, 'cadastro')
            data_pagamento_raw = get_vctex_date_field(row, 'pagamento')
            
            # üîç DEBUG: Log das datas brutas encontradas
            logging.info(f"üîç VCTEX Proposta {row.get('N√∫mero do Contrato', 'N/A')}: Datas brutas - CADASTRO_RAW: '{data_cadastro_raw}' | PAGAMENTO_RAW: '{data_pagamento_raw}'")
            
            # Validar e normalizar datas
            data_cadastro_vctex = validate_and_normalize_date(data_cadastro_raw, "DATA_CADASTRO")
            data_pagamento_vctex = validate_and_normalize_date(data_pagamento_raw, "DATA_PAGAMENTO")
            
            # üîç DEBUG: Log das datas processadas
            logging.info(f"üîç VCTEX Proposta {row.get('N√∫mero do Contrato', 'N/A')}: Datas processadas - CADASTRO_FINAL: '{data_cadastro_vctex}' | PAGAMENTO_FINAL: '{data_pagamento_vctex}'")
            
            # üîß CORRE√á√ÉO ROBUSTA: Verificar e corrigir datas trocadas do VCTEX
            if data_cadastro_vctex and data_pagamento_vctex:
                try:
                    from datetime import datetime
                    cadastro_dt = None
                    pagamento_dt = None
                    
                    # Tentar formatos comuns de data brasileira e internacional
                    date_formats = [
                        '%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d', '%d.%m.%Y',  # Formatos b√°sicos
                        '%d/%m/%y', '%d-%m-%y', '%y-%m-%d', '%d.%m.%y',  # Ano de 2 d√≠gitos
                        '%Y/%m/%d', '%Y.%m.%d'  # Formatos internacionais
                    ]
                    
                    # Tentar converter CADASTRO
                    for fmt in date_formats:
                        try:
                            cadastro_dt = datetime.strptime(data_cadastro_vctex, fmt)
                            break
                        except:
                            continue
                    
                    # Tentar converter PAGAMENTO  
                    for fmt in date_formats:
                        try:
                            pagamento_dt = datetime.strptime(data_pagamento_vctex, fmt)
                            break
                        except:
                            continue
                    
                    # ‚úÖ VALIDA√á√ÉO: Se conseguiu converter ambas, verificar ordem l√≥gica
                    if cadastro_dt and pagamento_dt:
                        # Calcular diferen√ßa em dias
                        diferenca_dias = (pagamento_dt - cadastro_dt).days
                        
                        if diferenca_dias < 0:
                            # Pagamento anterior ao cadastro = ERRO L√ìGICO!
                            logging.error(f"üîÑ VCTEX Proposta {row.get('N√∫mero do Contrato', 'N/A')}: DATAS LOGICAMENTE INCORRETAS!")
                            logging.error(f"   ‚ùå CADASTRO: {data_cadastro_vctex} ({cadastro_dt.strftime('%d/%m/%Y')})")
                            logging.error(f"   ‚ùå PAGAMENTO: {data_pagamento_vctex} ({pagamento_dt.strftime('%d/%m/%Y')})")
                            logging.error(f"   ‚ùå Diferen√ßa: {diferenca_dias} dias (IMPOSS√çVEL!)")
                            
                            # CORRE√á√ÉO AUTOM√ÅTICA: Trocar as datas
                            data_cadastro_vctex, data_pagamento_vctex = data_pagamento_vctex, data_cadastro_vctex
                            logging.warning(f"   üîß CORRIGIDO - CADASTRO: {data_cadastro_vctex} | PAGAMENTO: {data_pagamento_vctex}")
                            
                        elif diferenca_dias > 365:
                            # Muito tempo entre cadastro e pagamento = suspeito
                            logging.warning(f"‚ö†Ô∏è VCTEX Proposta {row.get('N√∫mero do Contrato', 'N/A')}: Diferen√ßa suspeita de {diferenca_dias} dias entre cadastro e pagamento")
                        else:
                            # Datas em ordem l√≥gica
                            logging.info(f"‚úÖ VCTEX Proposta {row.get('N√∫mero do Contrato', 'N/A')}: Datas em ordem correta ({diferenca_dias} dias)")
                    
                    else:
                        logging.warning(f"‚ö†Ô∏è VCTEX: N√£o foi poss√≠vel validar formato das datas - CADASTRO: '{data_cadastro_vctex}' | PAGAMENTO: '{data_pagamento_vctex}'")
                        
                except Exception as e:
                    logging.error(f"‚ùå VCTEX: Erro ao validar datas: {e}")
            
            # üìä LOG COMPLETO das datas VCTEX para debug
            logging.info(f"üìÖ VCTEX Proposta {row.get('N√∫mero do Contrato', 'N/A')}: DATAS ORIGINAIS DO ARQUIVO")
            logging.info(f"   ‚úÖ DATA_CADASTRO_FINAL: '{data_cadastro_vctex}'")
            logging.info(f"   ‚úÖ DATA_PAGAMENTO_FINAL: '{data_pagamento_vctex}'")
            logging.info(f"   üè¶ BANCO: VCTEX | √ìRG√ÉO: {orgao_vctex} | TAXA: {taxa_raw}")
            
            # üí∞ VCTEX: Formata√ß√£o de valores no padr√£o brasileiro (1.234,56)
            def format_vctex_value(value_str):
                """Formata valores do VCTEX no padr√£o brasileiro com ponto e v√≠rgula"""
                if not value_str or str(value_str).strip() in ['', 'nan', 'NaN', 'None', '0']:
                    return "0,00"
                
                try:
                    # Limpar o valor (remover R$, espa√ßos, etc.)
                    clean_value = str(value_str).strip().replace('R$', '').replace(' ', '')
                    
                    # Se j√° est√° no formato brasileiro, verificar se est√° correto
                    if ',' in clean_value:
                        # Formato brasileiro: 1.234,56 ou 87,58
                        parts = clean_value.split(',')
                        if len(parts) == 2 and len(parts[1]) == 2:
                            # J√° est√° formatado corretamente
                            return clean_value
                        else:
                            # Tem v√≠rgula mas n√£o est√° formatado corretamente
                            # Converter para ponto e processar
                            clean_value = clean_value.replace('.', '').replace(',', '.')
                    
                    # Converter para float
                    value_float = float(clean_value)
                    
                    # Formatar no padr√£o brasileiro
                    if value_float >= 1000:
                        # Valores >= 1000: usar ponto como separador de milhar
                        # Ex: 1.234,56
                        formatted = f"{value_float:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
                    else:
                        # Valores < 1000: s√≥ v√≠rgula decimal
                        # Ex: 87,58
                        formatted = f"{value_float:.2f}".replace('.', ',')
                    
                    return formatted
                    
                except (ValueError, TypeError) as e:
                    logging.warning(f"‚ö†Ô∏è VCTEX: Erro ao formatar valor '{value_str}': {e}")
                    return str(value_str)  # Retornar original se houver erro
            
            # Formatar valores antes de adicionar ao normalized_row
            valor_operacao_raw = str(row.get('Valor da operacao', str(row.get('Valor Liberado', '')))).strip()
            valor_liberado_raw = str(row.get('Valor Liberado', '')).strip()
            valor_parcela_raw = str(row.get('Parcela', row.get('Valor parcela', ''))).strip()
            
            valor_operacao_formatado = format_vctex_value(valor_operacao_raw)
            valor_liberado_formatado = format_vctex_value(valor_liberado_raw)
            valor_parcela_formatado = format_vctex_value(valor_parcela_raw)
            
            logging.info(f"üí∞ VCTEX Proposta {row.get('N√∫mero do Contrato', 'N/A')}: Valores formatados - OP: {valor_operacao_formatado}, LIB: {valor_liberado_formatado}, PARC: {valor_parcela_formatado}")
            
            normalized_row = {
                "PROPOSTA": str(row.get('N√∫mero do Contrato', row.get('Identificacao da operacao', ''))).strip(),
                "DATA_CADASTRO": data_cadastro_vctex,
                "BANCO": "BANCO VCTEX",
                "ORGAO": orgao_vctex,
                "TIPO_OPERACAO": "Margem Livre (Novo)",  # VCTEX normalmente s√≥ tem esse tipo
                "NUMERO_PARCELAS": numero_parcelas_vctex,
                "VALOR_OPERACAO": valor_operacao_formatado,  # üí∞ FORMATADO
                "VALOR_LIBERADO": valor_liberado_formatado,  # üí∞ FORMATADO
                "USUARIO_BANCO": str(row.get('Usu√°rio (acesso login)', row.get('CPF Usuario', ''))).strip(),
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": data_pagamento_vctex,
                "CPF": str(row.get('CPF', '')).strip(),
                "NOME": str(row.get('Nome do Cliente', row.get('Nome', ''))).strip(),
                "DATA_NASCIMENTO": str(row.get('Data de nascimento', '')).strip() if 'Data de nascimento' in df.columns else "",
                "TELEFONE": str(row.get('Telefone', row.get('Tel', row.get('Fone', '')))).strip(),
                "ENDERECO": str(row.get('Endereco', row.get('Endere√ßo', row.get('End', '')))).strip(),
                "BAIRRO": str(row.get('Bairro', '')).strip(), 
                "CEP": str(row.get('CEP', '')).strip(),
                "UF": str(row.get('UF', row.get('Estado', ''))).strip(),
                "VALOR_PARCELAS": valor_parcela_formatado,  # üí∞ FORMATADO
                "CODIGO_TABELA": tabela_normalized,  # Nome NORMALIZADO da tabela (usado para buscar no dicion√°rio)
                "TAXA": taxa_raw,  # Taxa do arquivo (mas ser√° substitu√≠da pelo mapeamento se encontrar)
                "OBSERVACOES": str(row.get('Observa√ß√£o', row.get('Observa√ß√µes', row.get('Observacoes', row.get('Obs', ''))))).strip()  # Campo observa√ß√µes do VCTEX
            }
            
        elif bank_type == "DAYCOVAL":
            # üîß DAYCOVAL - Detectar formato (CSV correto vs Unnamed)
            
            # Verificar se j√° est√° no formato CSV correto
            has_correct_columns = any(col in ['PROPOSTA', 'DATA CADASTRO', 'BANCO', 'ORGAO'] for col in row.keys())
            
            if has_correct_columns:
                # ‚úÖ FORMATO CSV CORRETO - Mapear diretamente
                logging.info(f"‚úÖ DAYCOVAL linha {idx}: FORMATO CSV CORRETO DETECTADO")
                
                normalized_row = {
                    "PROPOSTA": str(row.get('PROPOSTA', '')).strip(),
                    "ADE": str(row.get('PROPOSTA', '')).strip(),  # ADE = mesma proposta
                    "DATA_CADASTRO": str(row.get('DATA CADASTRO', '')).strip(),
                    "BANCO": "BANCO DAYCOVAL",
                    "ORGAO": str(row.get('ORGAO', '')).strip(),
                    "TIPO_OPERACAO": str(row.get('TIPO DE OPERACAO', '')).strip(),
                    "NUMERO_PARCELAS": str(row.get('NUMERO PARCELAS', '')).strip(),
                    "VALOR_OPERACAO": str(row.get('VALOR OPERACAO', '')).strip(),
                    "VALOR_LIBERADO": str(row.get('VALOR LIBERADO', '')).strip(),
                    "USUARIO_BANCO": str(row.get('USUARIO BANCO', '')).strip(),
                    "SITUACAO": str(row.get('SITUACAO', '')).strip(),
                    "DATA_PAGAMENTO": str(row.get('DATA DE PAGAMENTO', '')).strip(),
                    "CPF": str(row.get('CPF', '')).strip(),
                    "NOME": str(row.get('NOME', '')).strip().upper(),
                    "DATA_NASCIMENTO": str(row.get('DATA DE NASCIMENTO', '')).strip(),
                    "TELEFONE": str(row.get('TELEFONE', '')).strip(),
                    "ENDERECO": str(row.get('ENDERECO', row.get('ENDERE√áO', ''))).strip(),
                    "BAIRRO": str(row.get('BAIRRO', '')).strip(),
                    "CEP": str(row.get('CEP', '')).strip(),
                    "UF": str(row.get('UF', '')).strip(),
                    "CODIGO_TABELA": str(row.get('CODIGO TABELA', '')).strip(),
                    "VALOR_PARCELAS": str(row.get('VALOR PARCELAS', '')).strip(),
                    "TAXA": str(row.get('TAXA', '')).strip(),
                    "OBSERVACOES": f"Processado via CSV correto | {str(row.get('ENDERECO', row.get('ENDERE√áO', '')))}"
                }
                
                logging.info(f"‚úÖ DAYCOVAL CSV: {normalized_row['PROPOSTA']} | {normalized_row['NOME']} | {normalized_row['SITUACAO']}")
                
            else:
                # ‚úÖ FORMATO ANTIGO - N√£o processar por enquanto
                logging.info(f"‚ö†Ô∏è DAYCOVAL linha {idx}: FORMATO ANTIGO - n√£o processado")  
                normalized_row = None
                # üîß FORMATO ANTIGO COM "Unnamed:" - Manter processamento existente
                # Estrutura DAYCOVAL:
                # Unnamed: 0 = NR.PROP., Unnamed: 1 = Tp. Opera√ß√£o, Unnamed: 2 = CLIENTE, Unnamed: 3 = CPF/CNPJ
                # Unnamed: 4 = MATR√çCULA, Unnamed: 5 = DT.CAD., Unnamed: 6 = DT.BASE
                # Unnamed: 11 = Prz. em Meses, Unnamed: 12 = TX.AM, Unnamed: 13 = VLR.LIQ
                # Unnamed: 16 = VLR.OPER, Unnamed: 18 = VLR.PARC, Unnamed: 23 = DESCRI√á√ÉO EMPREGADOR
                # Unnamed: 27 = Situa√ß√£o_Atual_da_Proposta, Unnamed: 36 = Data da libera√ß√£o
                
                logging.info(f"=" * 80)
                logging.info(f"üè¶ DAYCOVAL linha {idx}: FORMATO ANTIGO DETECTADO")
                logging.info(f"   Colunas dispon√≠veis: {list(row.keys())[:10]}")
                logging.info(f"=" * 80)
                
                # üîç Debug: Verificar estrutura completa dos valores importantes
                logging.error(f"üîç DAYCOVAL DEBUG - Valores importantes:")
                campos_importantes = [10, 11, 12, 13, 16, 17, 18, 26, 27, 36, 38]
                for i in campos_importantes:
                    col_name = f'Unnamed: {i}'
                    col_value = str(row.get(col_name, 'N/A')).strip()[:30]
                    logging.error(f"   {col_name}: '{col_value}'")
                
                # Extrair campos principais do DAYCOVAL - ajustado para estrutura real
                # Se a primeira coluna n√£o √© Unnamed: 0, usar o nome real da coluna
                primeira_coluna = 'BANCO DAYCOVAL S/A - Consignado'  # Nome real da primeira coluna
                proposta_raw = str(row.get(primeira_coluna, row.get('Unnamed: 0', ''))).strip()  # NR.PROP.
                tipo_operacao_raw = str(row.get('Unnamed: 1', '')).strip()  # Tp. Opera√ß√£o
                cliente_raw = str(row.get('Unnamed: 2', '')).strip()  # CLIENTE
                cpf_raw = str(row.get('Unnamed: 3', '')).strip()  # CPF/CNPJ
                matricula_raw = str(row.get('Unnamed: 4', '')).strip()  # MATR√çCULA
                data_cadastro_raw = str(row.get('Unnamed: 5', '')).strip()  # DT.CAD.
                data_base_raw = str(row.get('Unnamed: 6', '')).strip()  # DT.BASE
                prazo_meses_raw = str(row.get('Unnamed: 11', '')).strip()  # Prz. em Meses
                taxa_raw = str(row.get('Unnamed: 12', '')).strip()  # TX.AM
                valor_liquido_raw = str(row.get('Unnamed: 13', '')).strip()  # VLR.LIQ
                valor_operacao_raw = str(row.get('Unnamed: 16', '')).strip()  # VLR.OPER
                valor_parcela_raw = str(row.get('Unnamed: 18', '')).strip()  # VLR.PARC
                descricao_empregador_raw = str(row.get('Unnamed: 23', '')).strip()  # DESCRI√á√ÉO EMPREGADOR
                situacao_raw = str(row.get('Unnamed: 27', '')).strip()  # Situa√ß√£o_Atual_da_Proposta
                data_liberacao_raw = str(row.get('Unnamed: 36', '')).strip()  # Data da libera√ß√£o
            
            # Normalizar campos para detec√ß√£o
            tipo_op = tipo_operacao_raw.upper()
            orgao_descricao = descricao_empregador_raw.upper()
            
            # Logs detalhados para debug
            logging.info(f"ÔøΩ DAYCOVAL extra√≠do:")
            logging.info(f"   Proposta: {proposta_raw}")
            logging.info(f"   Tipo Opera√ß√£o: {tipo_operacao_raw}")
            logging.info(f"   Cliente: {cliente_raw[:30] if cliente_raw else 'N/A'}...")
            logging.info(f"   CPF: {cpf_raw}")
            logging.info(f"   Situa√ß√£o: {situacao_raw}")
            logging.info(f"   √ìrg√£o: {descricao_empregador_raw}")
            logging.info(f"   Valor Opera√ß√£o: {valor_operacao_raw}")
            
            # Fun√ß√£o para detectar √≥rg√£o do DAYCOVAL
            def detect_daycoval_orgao(descricao_empregador):
                """Detecta √≥rg√£o baseado na descri√ß√£o do empregador"""
                desc_upper = descricao_empregador.upper()
                
                if 'INSS' in desc_upper:
                    return "INSS"
                elif 'SPPREV' in desc_upper:
                    return "SPPREV"
                elif 'EDUC' in desc_upper or 'SEC EDU' in desc_upper:
                    return "EDUCACAO"
                elif 'SEFAZ' in desc_upper:
                    return "SEFAZ"
                else:
                    return "INSS"  # Default
            
            # Fun√ß√£o para detectar opera√ß√£o do DAYCOVAL  
            def detect_daycoval_operacao(tipo_operacao):
                """Detecta tipo de opera√ß√£o baseado no campo Tp. Opera√ß√£o"""
                tipo_upper = tipo_operacao.upper()
                
                if 'PORTABILIDADE' in tipo_upper and 'REFINANCIAMENTO' in tipo_upper:
                    return "Refinanciamento da Portabilidade"
                elif 'PORTABILIDADE' in tipo_upper:
                    return "Portabilidade + Refin"
                elif 'REFINANCIAMENTO' in tipo_upper:
                    return "Refinanciamento"
                elif 'NOVA' in tipo_upper:
                    return "Margem Livre (Novo)"
                else:
                    return "Margem Livre (Novo)"  # Default
            
            # Detectar √≥rg√£o e opera√ß√£o usando as fun√ß√µes
            orgao_detectado = detect_daycoval_orgao(descricao_empregador_raw)
            operacao_detectada = detect_daycoval_operacao(tipo_operacao_raw)
            
            # Verificar se n√£o s√£o cabe√ßalhos √≥bvios (menos restritivo)
            if (proposta_raw.upper() in ['PROPOSTAS CADASTRADAS', 'DETALHADO', 'NR.PROP.'] or
                'PROPOSTAS CADASTRADAS' in proposta_raw.upper()):
                logging.info(f"‚è≠Ô∏è DAYCOVAL linha {idx}: Detectado cabe√ßalho - pulando")
                normalized_row = None
            else:
                # Aplicar formata√ß√£o brasileira
                cpf_formatted = format_cpf_global(cpf_raw)
                valor_operacao_formatted = format_value_brazilian(valor_operacao_raw)
                valor_liberado_formatted = format_value_brazilian(valor_liquido_raw)
                valor_parcela_formatted = format_value_brazilian(valor_parcela_raw)
                taxa_formatted = format_percentage_brazilian(taxa_raw)
            
                logging.info(f"‚úÖ DAYCOVAL formatado:")
                logging.info(f"   CPF: {cpf_formatted}")
                logging.info(f"   Valor Opera√ß√£o: {valor_operacao_formatted}")
                logging.info(f"   Valor Liberado: {valor_liberado_formatted}")
                logging.info(f"   √ìrg√£o: {orgao_detectado}")
                logging.info(f"   Opera√ß√£o: {operacao_detectada}")
                
                # ‚úÖ SEMPRE CRIAR normalized_row - Deixar valida√ß√£o final decidir
                logging.info(f"‚úÖ DAYCOVAL linha {idx}: Processando proposta {proposta_raw}")
                
                # Normalizar campos obrigat√≥rios - Valores seguros
                proposta_final = str(proposta_raw).strip() if proposta_raw and str(proposta_raw).strip() not in ['nan', 'None', ''] else f"DAYC_{idx}"
                nome_final = str(cliente_raw).strip().upper() if cliente_raw and str(cliente_raw).strip() not in ['nan', 'None', ''] else "NOME NAO INFORMADO"
                cpf_final = cpf_formatted if cpf_formatted and cpf_formatted != "000.000.000-00" else "000.000.000-00"
                
                normalized_row = {
                    "PROPOSTA": proposta_final,  # Unnamed: 0
                    "ADE": proposta_final,  # Campo ADE = mesma proposta
                    "DATA_CADASTRO": str(data_cadastro_raw) if data_cadastro_raw else "",  # Unnamed: 5 - DT.CAD.
                    "BANCO": "BANCO DAYCOVAL",
                    "ORGAO": orgao_detectado,  # ‚úÖ Detectado do arquivo
                    "TIPO_OPERACAO": operacao_detectada,  # ‚úÖ Detectado do arquivo  
                    "NUMERO_PARCELAS": str(prazo_meses_raw) if prazo_meses_raw else "0",  # Unnamed: 11 - Prz. em Meses
                    "VALOR_OPERACAO": valor_operacao_formatted,  # ‚úÖ Formatado brasileiro
                    "VALOR_LIBERADO": valor_liberado_formatted,  # ‚úÖ Formatado brasileiro
                    "USUARIO_BANCO": str(row.get('Unnamed: 40', '')).strip(),  # Usu√°rio_Digitador
                    "SITUACAO": str(situacao_raw) if situacao_raw else "",  # Unnamed: 27 - Situa√ß√£o_Atual_da_Proposta
                    "DATA_PAGAMENTO": str(data_liberacao_raw) if data_liberacao_raw else "",  # Unnamed: 36 - Data da libera√ß√£o
                    "CPF": cpf_final,  # ‚úÖ Formatado brasileiro (XXX.XXX.XXX-XX)
                    "NOME": nome_final,  # ‚úÖ Mai√∫sculas
                    "DATA_NASCIMENTO": "",  # N√£o dispon√≠vel no DAYCOVAL
                    "TELEFONE": str(row.get('Unnamed: 20', '')).strip(),  # Tentar extrair telefone (poss√≠vel coluna)
                    "ENDERECO": str(row.get('Unnamed: 21', '')).strip(),  # Tentar extrair endere√ßo (poss√≠vel coluna)
                    "BAIRRO": str(row.get('Unnamed: 22', '')).strip(),    # Tentar extrair bairro (poss√≠vel coluna)
                    "CEP": "",          # DAYCOVAL n√£o tem dados de CEP identificados
                    "UF": "",           # DAYCOVAL n√£o tem dados de UF identificados
                    "CODIGO_TABELA": str(row.get('Unnamed: 38', '')).strip() if row.get('Unnamed: 38') else "",  # C√≥digo da tabela
                    "VALOR_PARCELAS": valor_parcela_formatted,  # ‚úÖ Formatado brasileiro
                    "TAXA": taxa_formatted,  # ‚úÖ Formatado brasileiro (X,XX%)
                    "OBSERVACOES": f"Matr√≠cula: {matricula_raw} | Forma Libera√ß√£o: {str(row.get('Unnamed: 32', '')).strip()} | {str(row.get('Unnamed: 29', '')).strip()}"
                }
                
                logging.info(f"‚úÖ‚úÖ‚úÖ DAYCOVAL normalized_row criado com sucesso para proposta: {proposta_final}")
                logging.info(f"‚úÖ‚úÖ‚úÖ DAYCOVAL normalized_row: PROPOSTA={proposta_final}, NOME={nome_final}, CPF={cpf_final}")
            
        elif bank_type == "SANTANDER":
            # üè¶ BANCO SANTANDER - Processamento simplificado
            # Campos reais: COD, COD. BANCO, CPF, CLIENTE, CONVENIO, PRODUTO, QTDE PARCELAS, 
            #               VALOR BRUTO, VALOR LIQUIDO, STATUS, DATA, DATA AVERBACAO, COD DIGITADOR
            
            logging.info(f"=" * 80)
            logging.info(f"üè¶ SANTANDER linha {idx}: INICIANDO PROCESSAMENTO")
            logging.info(f"   Colunas dispon√≠veis: {list(row.keys())[:15]}")
            logging.info(f"=" * 80)
            
            import re
            
            # üîß Fun√ß√µes auxiliares
            def extract_santander_codigo_tabela(produto_str):
                """Extrai c√≥digo tabela do produto (ex: '810021387' de '21387 - 810021387 - OFERTA')"""
                if not produto_str:
                    return ""
                
                parts = str(produto_str).split(' - ')
                if len(parts) >= 2 and parts[1].strip().isdigit():
                    return parts[1].strip()
                
                # Buscar n√∫mero longo
                numbers = re.findall(r'\d{6,}', str(produto_str))
                return numbers[0] if numbers else ""
            
            def format_santander_value(value_str):
                """Formata valores no padr√£o brasileiro - VERS√ÉO ROBUSTA"""
                if not value_str or str(value_str).strip() in ['', 'nan', 'NaN', 'None', '0']:
                    return "0,00"
                
                try:
                    # Limpeza mais cuidadosa
                    clean_value = str(value_str).strip()
                    clean_value = clean_value.replace('R$', '').replace(' ', '').replace('\xa0', '').replace('\u00a0', '')
                    
                    # Se est√° vazio ap√≥s limpeza
                    if not clean_value or clean_value == '0':
                        return "0,00"
                        
                    # Tratar formatos comuns
                    if ',' in clean_value and '.' in clean_value:
                        # Formato: 1.234,56 (brasileiro) ou 1,234.56 (americano)
                        if clean_value.rfind(',') > clean_value.rfind('.'):
                            # Formato brasileiro: 1.234,56
                            clean_value = clean_value.replace('.', '').replace(',', '.')
                        else:
                            # Formato americano: 1,234.56
                            clean_value = clean_value.replace(',', '')
                    elif ',' in clean_value:
                        # Apenas v√≠rgula - formato brasileiro
                        clean_value = clean_value.replace(',', '.')
                        
                    value_float = float(clean_value)
                    
                    # Formatar no padr√£o brasileiro
                    if value_float >= 1000:
                        formatted = f"{value_float:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
                    else:
                        formatted = f"{value_float:.2f}".replace('.', ',')
                        
                    return formatted
                    
                except (ValueError, TypeError) as e:
                    logging.warning(f"‚ö†Ô∏è SANTANDER: Erro ao formatar valor '{value_str}': {e}")
                    # Retornar valor original limpo ao inv√©s de erro
                    clean_fallback = str(value_str).replace('R$', '').replace('  ', ' ').strip()
                    return clean_fallback if clean_fallback else "0,00"
            
            def normalize_santander_status(status_str):
                """Normaliza status para padr√£o Storm - VERS√ÉO ROBUSTA"""
                if not status_str:
                    return "AGUARDANDO"
                
                try:
                    # Convers√£o segura para string
                    status_clean = str(status_str).strip().upper()
                    
                    # Remover apenas caracteres problem√°ticos espec√≠ficos, preservar texto
                    status_clean = status_clean.replace('ÔøΩ', '').replace('\x00', '').replace('\ufffd', '')
                    
                    # Remover acentos de forma mais segura
                    import unicodedata
                    status_clean = unicodedata.normalize('NFKD', status_clean)
                    status_clean = ''.join(c for c in status_clean if not unicodedata.combining(c))
                    
                    logging.info(f"üîç SANTANDER status original: '{status_str}' ‚Üí normalizado: '{status_clean}'")
                    
                    # Verificar palavras-chave SANTANDER - ORDEM ESPEC√çFICA
                    # 1. PAGO: Opera√ß√µes finalizadas/averbadas
                    if any(palavra in status_clean for palavra in ['PAGO', 'LIBERADO', 'DESEMBOLSADO', 'FINALIZADO', 'LIQUIDADO']):
                        return "PAGO"
                    elif 'AVERBADA' in status_clean and 'ANALISE' not in status_clean:
                        # "AVERBADA" sozinha = PAGO, mas "AVERBADA EM ANALISE" = AGUARDANDO  
                        return "PAGO"
                    elif ('AVERBADA' in status_clean and 'ANALISE' in status_clean) or ('AVERBACAO' in status_clean and 'ANALISE' in status_clean):
                        # "AVERBADA EM ANALISE" ou "AVERBACAO EM ANALISE" = ainda em processamento
                        return "AGUARDANDO"
                    # 2. CANCELADO: Opera√ß√µes rejeitadas/negadas
                    elif any(palavra in status_clean for palavra in ['CANCELADO', 'CANCELADA', 'REPROVADO', 'REPROVADA', 'REJEITADO', 'REJEITADA', 'NEGADO', 'NEGADA', 'RECUSADO', 'RECUSADA']):
                        return "CANCELADO"
                    # 3. AGUARDANDO: Opera√ß√µes em processamento/an√°lise
                    elif any(palavra in status_clean for palavra in ['AGUARDANDO', 'ANALISE', 'PENDENTE', 'ABERTO', 'ABERTA', 'DIGITACAO', 'PROCESSAMENTO', 'EM PROCESSAMENTO']):
                        return "AGUARDANDO"
                    else:
                        # Se n√£o reconhecer nenhuma palavra-chave, retornar AGUARDANDO
                        logging.info(f"üîç SANTANDER: Status n√£o reconhecido '{status_clean}' ‚Üí AGUARDANDO")
                        return "AGUARDANDO"
                        
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è SANTANDER: Erro ao normalizar status '{status_str}': {e}")
                    return "AGUARDANDO"
            
            # Fun√ß√£o para limpar texto "quebrado" do SANTANDER
            def clean_santander_text(text_str):
                """Limpa texto quebrado/corrompido do relat√≥rio SANTANDER"""
                if not text_str:
                    return ""
                
                try:
                    # Converter para string e limpar
                    clean_text = str(text_str).strip()
                    
                    # Remover caracteres problem√°ticos comuns
                    clean_text = clean_text.replace('ÔøΩ', '').replace('\ufffd', '').replace('\x00', '')
                    clean_text = clean_text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
                    
                    # Remover espa√ßos m√∫ltiplos
                    while '  ' in clean_text:
                        clean_text = clean_text.replace('  ', ' ')
                    
                    return clean_text.strip()
                except:
                    return str(text_str).strip() if text_str else ""
            
            # Extrair campos do arquivo com limpeza
            proposta = clean_santander_text(row.get('COD. BANCO', row.get('COD', row.get('PROPOSTA', ''))))
            cliente = clean_santander_text(row.get('CLIENTE', row.get('NOME', '')))
            cpf = clean_santander_text(row.get('CPF', ''))
            convenio = clean_santander_text(row.get('CONVENIO', '')).upper()
            produto = clean_santander_text(row.get('PRODUTO', ''))
            parcelas = clean_santander_text(row.get('QTDE PARCELAS', row.get('NUMERO PARCELAS', '96')))
            valor_bruto = clean_santander_text(row.get('VALOR BRUTO', row.get('VALOR OPERACAO', '0')))
            valor_liquido = clean_santander_text(row.get('VALOR LIQUIDO', row.get('VALOR LIBERADO', '0')))
            valor_parcela = clean_santander_text(row.get('VALOR PARCELA', row.get('VALOR PARCELAS', '0')))
            data_cadastro = clean_santander_text(row.get('DATA', row.get('DATA CADASTRO', '')))
            status = clean_santander_text(row.get('STATUS', row.get('SITUACAO', 'AGUARDANDO')))
            data_averbacao = clean_santander_text(row.get('DATA AVERBACAO', row.get('DATA DE PAGAMENTO', '')))
            cod_digitador = str(row.get('COD DIGITADOR NO BANCO', row.get('USUARIO BANCO', ''))).strip()
            
            # üéØ SANTANDER: Extrair CPF do campo "COD DIGITADOR NO BANCO" 
            def format_santander_usuario_from_cod(cod_digitador_raw):
                """Extrair CPF dos primeiros 11 d√≠gitos do COD DIGITADOR NO BANCO
                Ex: '37375205850030700' ‚Üí '373.752.058-50'
                """
                if not cod_digitador_raw or cod_digitador_raw in ['', 'nan', 'NaN']:
                    return "000.000.000-00"
                
                # Limpar e usar apenas d√≠gitos
                digits_only = ''.join(filter(str.isdigit, str(cod_digitador_raw)))
                
                if len(digits_only) >= 11:
                    # Pegar apenas os primeiros 11 d√≠gitos (CPF)
                    cpf_digits = digits_only[:11]
                    return f"{cpf_digits[:3]}.{cpf_digits[3:6]}.{cpf_digits[6:9]}-{cpf_digits[9:11]}"
                else:
                    return cod_digitador_raw  # Manter original se n√£o conseguir formatar
            
            usuario_digitador_formatado = format_santander_usuario_from_cod(cod_digitador)
            
            logging.info(f"üìã SANTANDER extra√≠do: Proposta={proposta}, Cliente={cliente[:20] if cliente else 'N/A'}, CPF={cpf[:6] if cpf else 'N/A'}...")
            logging.info(f"üë§ SANTANDER COD DIGITADOR: '{cod_digitador}' ‚Üí CPF: '{usuario_digitador_formatado}'")
            logging.info(f"üîç SANTANDER valida√ß√µes: proposta='{proposta}', convenio='{convenio[:30] if convenio else 'N/A'}', produto='{produto[:50] if produto else 'N/A'}'")
            
            # ‚úÖ VALIDA√á√ÉO: Verificar se linha deve ser processada
            should_process = True
            
            if not proposta or proposta.upper() in ['NAN', 'NONE', '', 'COD. BANCO']:
                logging.info(f"‚è≠Ô∏è SANTANDER linha {idx}: Pulando - proposta vazia ou cabe√ßalho")
                should_process = False
                normalized_row = None
            
            # üö´ FILTRO ESPECIAL: Propostas SEGURO t√™m 'S' no final do COD. BANCO
            elif proposta.upper().endswith('S') and len(proposta) > 5:
                # Verificar se √© realmente SEGURO (n√£o um c√≥digo normal que termina com S)
                if 'SEGURO' in convenio or 'SEGURO' in produto.upper():
                    logging.info(f"üö´ SANTANDER linha {idx}: Filtrando - proposta SEGURO com 'S' no final ({proposta})")
                    should_process = False
                    normalized_row = None
                else:
                    logging.info(f"‚úì SANTANDER linha {idx}: Proposta termina com 'S' mas N√ÉO √© SEGURO - vai processar")
            
            logging.info(f"üìä SANTANDER linha {idx}: should_process = {should_process}")
            
            # Processar apenas se n√£o foi filtrado
            if should_process:
                # Extrair c√≥digo tabela
                codigo_tabela = extract_santander_codigo_tabela(produto)
                
                logging.info(f"üîç SANTANDER linha {idx}: c√≥digo extra√≠do = '{codigo_tabela}' de produto: {produto[:50] if produto else 'N/A'}...")
                
                # Detectar √≥rg√£o
                if 'PREF' in convenio or 'AGUDOS' in convenio or 'RANCHARIA' in convenio:
                    orgao = 'PREF. DE AGUDOS - SP' if 'AGUDOS' in convenio else 'PREF. DE RANCHARIA - SP'
                elif 'LINS' in convenio:
                    orgao = 'PREF. DE LINS - SP'
                else:
                    orgao = 'INSS'
                
                logging.info(f"üèõÔ∏è SANTANDER linha {idx}: √≥rg√£o = {orgao} (conv√™nio: {convenio[:30] if convenio else 'N/A'}...)")
                
                # Detectar tipo de opera√ß√£o
                produto_upper = produto.upper()
                if 'REFIN' in produto_upper:
                    tipo_operacao = 'REFINANCIAMENTO'
                elif 'PORT' in produto_upper:
                    tipo_operacao = 'PORTABILIDADE'
                else:
                    tipo_operacao = 'MARGEM LIVRE (NOVO)'
                
                logging.info(f"üîß SANTANDER linha {idx}: tipo_operacao = {tipo_operacao}")
                
                # üö´ FILTRO: Remover propostas SEGURO (11111111)
                has_seguro_codigo = codigo_tabela and '11111111' in codigo_tabela
                has_seguro_produto = False  # Desativado - estava filtrando tudo
                is_pure_seguro = 'SEGURO' in produto_upper and not any(p in produto_upper for p in ['OFERTA', 'REFIN', 'PORT'])
                has_todos_convenios = 'TODOS OS CONVENIOS' in produto_upper
                
                logging.info(f"ÔøΩ SANTANDER linha {idx}: Verificando SEGURO - has_seguro_codigo={has_seguro_codigo}, has_seguro_produto={has_seguro_produto}, has_todos_convenios={has_todos_convenios}")
                
                if codigo_tabela and (has_seguro_codigo or is_pure_seguro or has_todos_convenios):
                    logging.info(f"FILTRADO por SEGURO - proposta {proposta}")
                    normalized_row = None
                else:
                    logging.info(f"PASSOU nos filtros - vai criar normalized_row")
                    
                    # Criar registro normalizado apenas se passou nos filtros
                    normalized_row = {
                        "PROPOSTA": proposta,
                        "DATA_CADASTRO": data_cadastro,
                        "BANCO": "BANCO SANTANDER",
                        "ORGAO": orgao,
                        "TIPO_OPERACAO": tipo_operacao,
                        "NUMERO_PARCELAS": parcelas,
                        "VALOR_OPERACAO": format_santander_value(valor_bruto),
                        "VALOR_LIBERADO": format_santander_value(valor_liquido),
                        "USUARIO_BANCO": usuario_digitador_formatado,
                        "SITUACAO": normalize_santander_status(status),
                        "DATA_PAGAMENTO": data_averbacao if normalize_santander_status(status) == "PAGO" else "",
                        "CPF": cpf,
                        "NOME": cliente.upper(),
                        "DATA_NASCIMENTO": "",
                        "TELEFONE": clean_santander_text(row.get('TELEFONE', row.get('TEL', row.get('FONE', '')))),
                        "ENDERECO": clean_santander_text(row.get('ENDERECO', row.get('END', row.get('ENDERE√áO', '')))),
                        "BAIRRO": clean_santander_text(row.get('BAIRRO', '')),
                        "CEP": clean_santander_text(row.get('CEP', '')),
                        "UF": clean_santander_text(row.get('UF', row.get('ESTADO', ''))),
                        "CODIGO_TABELA": codigo_tabela,
                        "VALOR_PARCELAS": format_santander_value(valor_parcela),
                        "TAXA": "0,00%",
                        "OBSERVACOES": ""
                    }
                    
                    logging.info(f"‚úÖ‚úÖ‚úÖ SANTANDER linha {idx}: normalized_row CRIADO! Proposta={proposta} | C√≥digo={codigo_tabela} | √ìrg√£o={orgao} | Status={normalize_santander_status(status)}")
                    logging.info(f"üì¶ SANTANDER linha {idx}: normalized_row pronto para valida√ß√£o comum")
            else:
                # Se should_process=False, garantir que normalized_row=None j√° foi definido
                logging.info(f"‚è≠Ô∏è SANTANDER linha {idx}: should_process=False, normalized_row=None")
                if 'normalized_row' not in locals():
                    normalized_row = None
        
        elif bank_type == "CREFAZ":
            # üîç CREFAZ: Log das colunas dispon√≠veis para debug
            logging.info(f"üè¶ CREFAZ - Colunas dispon√≠veis: {list(row.keys())}")
            
            # Mapeamento BANCO CREFAZ - Campos reais baseados no mapeamento
            # Colunas reais: Data Cadastro, N√∫mero da Proposta, CPF, Cliente, Cidade, Status, Agente, etc.
            
            # üí∞ FUN√á√ÉO para formatar valores no padr√£o brasileiro
            def format_crefaz_value(value_str):
                """Converte valores para formato brasileiro: 1.255,00"""
                if not value_str or str(value_str).strip() in ['', 'nan', 'None', '0']:
                    return "0,00"
                
                try:
                    # Limpar o valor (remover espa√ßos, moeda, etc.)
                    clean_value = str(value_str).strip().replace('R$', '').replace(' ', '')
                    
                    # Se j√° est√° no formato brasileiro, manter
                    if ',' in clean_value and clean_value.count(',') == 1:
                        parts = clean_value.split(',')
                        if len(parts[1]) == 2:  # Duas casas decimais ap√≥s v√≠rgula
                            return clean_value
                    
                    # Converter do formato americano (ponto decimal)
                    if '.' in clean_value:
                        # Separar parte inteira e decimal
                        parts = clean_value.split('.')
                        integer_part = parts[0]
                        decimal_part = parts[1][:2] if len(parts) > 1 else "00"
                    else:
                        # Sem decimal, assumir valor inteiro
                        integer_part = clean_value
                        decimal_part = "00"
                    
                    # Garantir que decimal tenha 2 d√≠gitos
                    if len(decimal_part) == 1:
                        decimal_part += "0"
                    elif len(decimal_part) == 0:
                        decimal_part = "00"
                    
                    # Converter para float para formatar
                    float_value = float(f"{integer_part}.{decimal_part}")
                    
                    # Formatar no padr√£o brasileiro: 1.255,00
                    if float_value >= 1000:
                        # Valores >= 1000: usar ponto para milhar
                        formatted = f"{float_value:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
                    else:
                        # Valores < 1000: apenas v√≠rgula decimal
                        formatted = f"{float_value:.2f}".replace('.', ',')
                    
                    return formatted
                    
                except (ValueError, TypeError) as e:
                    logging.warning(f"‚ö†Ô∏è CREFAZ: Erro ao formatar valor '{value_str}': {e}")
                    return str(value_str)  # Retornar original se houver erro
            
            # üîß CREFAZ: CORRE√á√ÉO - ADE deve vir da coluna "Cod Opera√ß√£o" (ex: 3915740)
            # O ADE correto est√° em "Cod Opera√ß√£o", n√£o em "N√∫mero da Proposta"
            proposta = str(row.get('Cod Opera√ß√£o', '')).strip()
            
            # Fallback: tentar outras varia√ß√µes da coluna se n√£o encontrar
            if not proposta or proposta in ['nan', 'None', '', 'NaN', '0']:
                proposta = str(row.get('Cod Operacao', row.get('COD_OPERACAO', ''))).strip()
            
            # ‚úÖ FILTRAR LINHAS VAZIAS - Valida√ß√µes mais robustas
            if not proposta or proposta in ['nan', 'None', '', 'NaN', '0']:
                logging.info(f"‚è≠Ô∏è CREFAZ: Pulando linha - ADE vazio na coluna 'Cod Opera√ß√£o': '{proposta}'")
                continue
            
            # Validar campos essenciais antes de prosseguir
            cpf_cliente = str(row.get('CPF', '')).strip()
            nome_cliente = str(row.get('Cliente', row.get('Nome', ''))).strip()
            
            # Pular linhas com campos cr√≠ticos vazios
            if not cpf_cliente or cpf_cliente in ['nan', 'None', '', 'NaN']:
                logging.info(f"‚è≠Ô∏è CREFAZ: Pulando linha - CPF vazio (ADE: {proposta})")
                continue
                
            if not nome_cliente or nome_cliente in ['nan', 'None', '', 'NaN']:
                logging.info(f"‚è≠Ô∏è CREFAZ: Pulando linha - Nome vazio (ADE: {proposta})")
                continue
                
            logging.info(f"üéØ CREFAZ: ADE correto encontrado em 'Cod Opera√ß√£o': {proposta}")
            
            # üîß CREFAZ: C√ìDIGO DE TABELA - gerar baseado no produto
            produto_raw = str(row.get('Produto', '')).strip().upper()
            
            # Validar se produto n√£o est√° vazio
            if not produto_raw or produto_raw in ['NAN', 'NONE', '']:
                logging.info(f"‚è≠Ô∏è CREFAZ: Pulando linha - Produto vazio (ADE: {proposta})")
                continue
            
            if 'ENERGIA' in produto_raw or 'LUZ' in produto_raw:
                cod_operacao = "ENER"
            elif 'BOLETO' in produto_raw:
                cod_operacao = "BOL" 
            elif 'VEICULO' in produto_raw or 'AUTO' in produto_raw:
                cod_operacao = "CPAUTO"
            elif 'TRABALHADOR' in produto_raw or 'CLT' in produto_raw:
                cod_operacao = "CSD"
            else:
                cod_operacao = "ENER"  # Default para energia
                
            logging.info(f"üéØ CREFAZ: C√≥digo gerado do produto '{produto_raw}': {cod_operacao}")
            produto = str(row.get('Produto', '')).strip()
            
            # ‚úÖ VALIDA√á√ÉO: Pular linhas com c√≥digo de opera√ß√£o vazio
            if not cod_operacao or cod_operacao.upper() in ['NAN', 'NONE', '']:
                logging.info(f"‚è≠Ô∏è CREFAZ: Pulando proposta {proposta} - c√≥digo de opera√ß√£o vazio")
                continue
            
            # üîß CREFAZ: Extrair usu√°rio digitador - tentar m√∫ltiplas colunas
            usuario_candidates = [
                'Login Agente', 'login agente', 'LOGIN_AGENTE',
                'Agente', 'agente', 'AGENTE',
                'C√≥digo Digitador', 'Codigo Digitador', 'COD_DIGITADOR',
                'Usuario', 'USUARIO', 'usuario',
                'Digitador', 'DIGITADOR', 'digitador'
            ]
            
            usuario_banco = ""
            col_usuario_usada = ""
            for col_name in usuario_candidates:
                temp_usuario = str(row.get(col_name, '')).strip()
                if temp_usuario and temp_usuario not in ['nan', 'None', '', 'NaN']:
                    usuario_banco = temp_usuario
                    col_usuario_usada = col_name
                    break
            
            # Se n√£o encontrou usu√°rio v√°lido, usar valor padr√£o
            if not usuario_banco:
                usuario_banco = "SISTEMA"
                col_usuario_usada = "DEFAULT"
            
            logging.info(f"üéØ CREFAZ: Usu√°rio digitador: '{usuario_banco}' (coluna: '{col_usuario_usada}')")
            
            # üîç CREFAZ: Detectar √ìRG√ÉO baseado no C√ìDIGO (n√£o no produto)
            # Os c√≥digos j√° v√™m corretos do arquivo: ENER, CPAUTO, LUZ, BOL, CSD
            cod_upper = cod_operacao.upper()
            
            # Mapear √≥rg√£o baseado no c√≥digo
            if cod_upper in ['ENER', 'LUZ']:
                orgao = 'ENERGIA'
            elif cod_upper in ['BOL', 'BOLETO']:
                orgao = 'BOLETO'
            elif cod_upper in ['CPAUTO', 'AUTO', 'VEICULO']:
                orgao = 'VEICULOS'
            elif cod_upper in ['CSD', 'CLT', 'TRABALHADOR']:
                orgao = 'CR√âDITO DO TRABALHADOR'
            else:
                # Se c√≥digo desconhecido, tentar detectar pelo produto
                produto_upper = produto.upper()
                if 'ENERGIA' in produto_upper or 'LUZ' in produto_upper or 'FATURA' in produto_upper:
                    orgao = 'ENERGIA'
                elif 'BOLETO' in produto_upper:
                    orgao = 'BOLETO'
                elif 'VEICULO' in produto_upper or 'AUTO' in produto_upper or 'CARRO' in produto_upper:
                    orgao = 'VEICULOS'
                elif 'TRABALHADOR' in produto_upper or 'CLT' in produto_upper or 'PRIVADO' in produto_upper:
                    orgao = 'CR√âDITO DO TRABALHADOR'
                else:
                    orgao = 'ENERGIA'  # Default
            
            tipo_operacao = 'Margem Livre (Novo)'  # CREFAZ sempre √© margem livre
            
            # üí∞ Formatar valores no padr√£o brasileiro
            valor_operacao_br = format_crefaz_value(row.get('Valor Liberado', row.get('Valor Solicitado', '')))
            valor_liberado_br = format_crefaz_value(row.get('Valor Liberado', ''))
            valor_parcela_br = format_crefaz_value(row.get('Valor da Parcela', row.get('Parcela', '')))
            
            # Criar registro normalizado
            normalized_row = {
                "PROPOSTA": proposta,
                "DATA_CADASTRO": str(row.get('Data Cadastro', '')).strip(),
                "BANCO": "BANCO CREFAZ",
                "ORGAO": orgao,
                "TIPO_OPERACAO": tipo_operacao,
                "NUMERO_PARCELAS": str(row.get('Prazo', '')).strip(),
                "VALOR_OPERACAO": valor_operacao_br,  # üí∞ FORMATO BRASILEIRO
                "VALOR_LIBERADO": valor_liberado_br,  # üí∞ FORMATO BRASILEIRO
                "USUARIO_BANCO": usuario_banco,
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Altera√ß√£o', row.get('Data Pagamento', ''))).strip(),
                "CPF": str(row.get('CPF', '')).strip(),
                "NOME": str(row.get('Cliente', row.get('Nome', ''))).strip(),
                "DATA_NASCIMENTO": "",
                "TELEFONE": str(row.get('Telefone', row.get('Fone', row.get('Tel', '')))).strip(),
                "ENDERECO": str(row.get('Endereco', row.get('Endere√ßo', row.get('End', '')))).strip(),
                "BAIRRO": str(row.get('Bairro', '')).strip(),
                "CEP": str(row.get('CEP', row.get('Cep', ''))).strip(),
                "UF": str(row.get('UF', row.get('Estado', row.get('Uf', '')))).strip(),
                "CODIGO_TABELA": cod_operacao,  # ‚úÖ Usar c√≥digo diretamente do arquivo (ENER, CPAUTO, LUZ, BOL, CSD)
                "VALOR_PARCELAS": valor_parcela_br,  # üí∞ FORMATO BRASILEIRO
                "TAXA": "0,00%",  # CREFAZ n√£o tem taxa no relat_orgaos (sempre 0,00%)
                "OBSERVACOES": str(row.get('Motivos', row.get('Observacoes', ''))).strip()
            }
            
            logging.info(f"‚úÖ CREFAZ processado: Proposta={proposta} | C√≥digo='{cod_operacao}' | √ìrg√£o='{orgao}'")

        
        elif bank_type == "QUERO_MAIS":
            # Mapeamento BANCO QUERO MAIS CREDITO - ESTRUTURA REAL IDENTIFICADA
            
            # Log para debug das colunas dispon√≠veis
            logging.info(f"üè¶ QUERO MAIS - Colunas dispon√≠veis: {list(row.keys())[:15]}...")
            
            # ESTRUTURA REAL baseada no map_relat_atualizados.txt:
            # Unnamed: 11 = CPF Cliente (213.651.558-62, 141.255.778-03)
            # Unnamed: 19 = Data Cadastro (03/09/2025, 05/09/2025) 
            # Unnamed: 20 = Data Nasc. (10/12/1969, 24/10/1958)
            # Unnamed: 22 = Descr. Tabela (INSS CART√ÉO BENEF√çCIO_LOAS_CCC, INSS_RMC_ QUERO MAIS_CCC)
            # Unnamed: 24 = Descr. Empregador (INSS BENEFICIO, INSS RMC, GOV S√ÉO PAULO)
            # Unnamed: 25 = Descr. Orgao (INSS BENEFICIO, INSS RMC, GOV S√ÉO PAULO)
            # Unnamed: 33 = Proposta (601967573, 601972997)
            # Unnamed: 37 = Nome do Agente (GRUPO QFZ)
            # Unnamed: 38 = Cliente (ADRIANA NATALINA RANCURA)
            # Unnamed: 40 = Nome usu. cad. Proposta (02579846158_901064) - USU√ÅRIO COM _
            # Unnamed: 42 = Qtd Parcelas (96)
            # Unnamed: 46 = Tabela utilizada (004713, 006640) - C√ìDIGO DA TABELA!
            # Unnamed: 48 = Vlr.da parcela (53.13, 194.36)
            # Unnamed: 49 = Valor liberacao 1 (1829.79, 1717.23)
            
            # Detec√ß√£o de √≥rg√£o pela descri√ß√£o correta
            descr_orgao = str(row.get('Unnamed: 25', '')).strip().upper()  # Descr. Orgao
            descr_empregador = str(row.get('Unnamed: 24', '')).strip().upper()  # Descr. Empregador
            
            orgao_text = f"{descr_orgao} {descr_empregador}"
            if 'INSS' in orgao_text:
                orgao = 'INSS'
            elif 'GOV' in orgao_text or 'S√ÉO PAULO' in orgao_text or 'SP' in orgao_text:
                orgao = 'SIAPE'
            elif 'FGTS' in orgao_text:
                orgao = 'FGTS'
            else:
                orgao = 'INSS'  # Default
            
            # Campos mapeados corretamente
            proposta = str(row.get('Unnamed: 33', '')).strip()  # Proposta
            cpf_cliente = str(row.get('Unnamed: 11', '')).strip()  # CPF Cliente
            nome_cliente = str(row.get('Unnamed: 38', '')).strip()  # Cliente
            data_cadastro = str(row.get('Unnamed: 19', '')).strip()  # Data Cadastro
            data_nascimento = str(row.get('Unnamed: 20', '')).strip()  # Data Nasc.
            qtd_parcelas = str(row.get('Unnamed: 42', '')).strip()  # Qtd Parcelas
            codigo_tabela_raw = str(row.get('Unnamed: 46', '')).strip()  # Tabela utilizada (C√ìDIGO!)
            # Formatar c√≥digo de tabela com zeros √† esquerda (6 d√≠gitos)
            codigo_tabela = codigo_tabela_raw.zfill(6) if codigo_tabela_raw.isdigit() else codigo_tabela_raw
            valor_parcela = str(row.get('Unnamed: 48', '')).strip()  # Vlr.da parcela
            valor_liberado = str(row.get('Unnamed: 49', '')).strip()  # Valor liberacao 1
            descr_tabela = str(row.get('Unnamed: 22', '')).strip()  # Descr. Tabela
            usuario_cadastro = str(row.get('Unnamed: 40', '')).strip()  # Nome usu. cad. Proposta (com _)
            
            # Determina√ß√£o do tipo de opera√ß√£o baseado na descri√ß√£o da tabela
            tipo_operacao = "Margem Livre (Novo)"  # Default
            if descr_tabela:
                descr_upper = descr_tabela.upper()
                if "CARTAO" in descr_upper or "CART√ÉO" in descr_upper:
                    if "SAQUE" in descr_upper:
                        tipo_operacao = "Cartao c/ saque"  # Sem acentos para evitar corrup√ß√£o
                    else:
                        tipo_operacao = "Cartao s/ saque"  # Sem acentos para evitar corrup√ß√£o
                elif "RMC" in descr_upper:
                    tipo_operacao = "RMC"
                elif "LOAS" in descr_upper:
                    tipo_operacao = "Margem Livre (LOAS)"
            
            # Remover zeros √† esquerda do c√≥digo de tabela (004717 ‚Üí 4717)
            codigo_tabela_original = str(row.get('Unnamed: 46', '')).strip()
            codigo_tabela_final = codigo_tabela_original.lstrip('0') if codigo_tabela_original else ''
            # Se ficou vazio ap√≥s remover zeros, manter o original
            if not codigo_tabela_final:
                codigo_tabela_final = codigo_tabela_original
            
            # üîß QUERO_MAIS: Manter usu√°rio digitador no formato original com underscore
            # Exemplo: "39891947807_901064" (manter como est√°)
            usuario_final = usuario_cadastro  # Manter formato original: 39891947807_901064
            
            normalized_row = {
                "PROPOSTA": proposta,
                "DATA_CADASTRO": data_cadastro,
                "BANCO": "BANCO QUERO MAIS CREDITO",
                "ORGAO": orgao,
                "TIPO_OPERACAO": tipo_operacao,  # Determinado pela descri√ß√£o da tabela
                "NUMERO_PARCELAS": qtd_parcelas,
                "VALOR_OPERACAO": valor_liberado,  # Usar valor liberado como opera√ß√£o
                "VALOR_LIBERADO": valor_liberado,
                "USUARIO_BANCO": usuario_final,  # Usu√°rio no formato original completo (com underscore)
                "SITUACAO": "DIGITADA",  # ‚úÖ MANUAL conforme solicitado 
                "DATA_PAGAMENTO": "",   # ‚úÖ MANUAL conforme solicitado (sempre vazio)
                "CPF": cpf_cliente,
                "NOME": nome_cliente,
                "DATA_NASCIMENTO": data_nascimento,
                "TELEFONE": "",    # QUERO MAIS n√£o tem dados de telefone
                "ENDERECO": "",    # QUERO MAIS n√£o tem dados de endere√ßo
                "BAIRRO": "",      # QUERO MAIS n√£o tem dados de bairro
                "CEP": "",         # QUERO MAIS n√£o tem dados de CEP
                "UF": "",          # QUERO MAIS n√£o tem dados de UF
                "CODIGO_TABELA": codigo_tabela_final,  # C√≥digo sem zeros √† esquerda (4717)
                "VALOR_PARCELAS": valor_parcela,
                "TAXA": "0,00%",  # Taxa fixa para QUERO MAIS
                "OBSERVACOES": descr_tabela  # Descri√ß√£o da tabela como observa√ß√£o
            }
            
            # Log para debug dos valores mapeados
            logging.info(f"‚úÖ QUERO MAIS mapeado: PROPOSTA={proposta}, ORGAO={orgao}, CPF={cpf_cliente}, TIPO_OP={tipo_operacao}")
        
        elif bank_type == "PAN":
            # Mapeamento BANCO PAN - Estrutura de cart√£o e saque
            normalized_row = {
                "PROPOSTA": str(row.get('N¬∫ Proposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data do Cadastro', '')).strip(),
                "BANCO": "BANCO PAN",
                "ORGAO": str(row.get('Nome do √ìrg√£o', row.get('Nome do Empregador', 'INSS'))).strip(),
                "TIPO_OPERACAO": str(row.get('Tipo de Opera√ß√£o', 'Cart√£o')).strip(),
                "NUMERO_PARCELAS": str(row.get('Quantidade de Parcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor Financiado', '')).strip(),
                "VALOR_LIBERADO": str(row.get('VLR_LIB1', row.get('Valor Financiado', ''))).strip(),
                "USUARIO_BANCO": str(row.get('Nome do Usu√°rio Digitador', '')).strip(),
                "SITUACAO": str(row.get('Nome da Atividade', row.get('Situa√ß√£o da Proposta', ''))).strip(),
                "DATA_PAGAMENTO": str(row.get('Data do Lan√ßamento', '')).strip(),
                "CPF": str(row.get('CPF do Cliente', '')).strip(),
                "NOME": str(row.get('Nome do Cliente', '')).strip(),
                "DATA_NASCIMENTO": str(row.get('Data de Nascimento', '')).strip(),
                "TELEFONE": "",    # PAN n√£o tem dados de telefone
                "ENDERECO": "",    # PAN n√£o tem dados de endere√ßo
                "BAIRRO": "",      # PAN n√£o tem dados de bairro
                "CEP": "",         # PAN n√£o tem dados de CEP
                "UF": "",          # PAN n√£o tem dados de UF
                "CODIGO_TABELA": str(row.get('Nome do Conv√™nio', row.get('C√≥digo do Conv√™nio', ''))).strip(),
                "VALOR_PARCELAS": str(row.get('Valor da Parcela', '')).strip(),
                "TAXA": "",
                "OBSERVACOES": str(row.get('Observa√ß√µes', row.get('Observacoes', row.get('Obs', '')))).strip()
            }
        
        elif bank_type == "C6":
            # Mapeamento BANCO C6 BANK - Campos reais do mapeamento
            # Colunas reais: Nome Entidade, Numero do Contrato, Proposta, Data da operacao, etc.
            
            # Detectar √≥rg√£o pelo Nome Entidade
            nome_entidade = str(row.get('Nome Entidade', '')).strip().upper()
            if 'INSS' in nome_entidade:
                orgao = 'INSS'
            elif 'FGTS' in nome_entidade:
                orgao = 'FGTS'
            else:
                orgao = 'INSS'  # Default
            
            normalized_row = {
                "PROPOSTA": str(row.get('N√∫mero do Contrato', row.get('Proposta', row.get('Numero do Contrato', '')))).strip(),
                "DATA_CADASTRO": str(row.get('Data da opera√ß√£o', row.get('Data da operacao', row.get('Data Cadastro', '')))).strip(),
                "BANCO": "BANCO C6 BANK",
                "ORGAO": orgao,
                "TIPO_OPERACAO": str(row.get('Produto', 'Margem Livre (Novo)')).strip(),
                "NUMERO_PARCELAS": str(row.get('Prazo proposta', row.get('Parcelas', ''))).strip(),
                "VALOR_OPERACAO": str(row.get('Valor Liberado', row.get('Valor', ''))).strip(),
                "VALOR_LIBERADO": str(row.get('Valor Liberado', '')).strip(),
                "USUARIO_BANCO": str(row.get('Usu√°rio (acesso login)', row.get('Usuario', ''))).strip(),
                "SITUACAO": str(row.get('Status', row.get('Situacao', ''))).strip(),
                "DATA_PAGAMENTO": str(row.get('Data pagamento Opera√ß√£o', row.get('Data Pagamento', ''))).strip(),
                "CPF": str(row.get('CPF', '')).strip(),
                "NOME": str(row.get('Nome do Cliente', row.get('Nome', ''))).strip(),
                "DATA_NASCIMENTO": str(row.get('Data Nascimento', '')).strip(),
                "CODIGO_TABELA": str(row.get('Tabela', '')).strip(),
                "VALOR_PARCELAS": str(row.get('Parcela', row.get('Valor Parcela', ''))).strip(),
                "TAXA": str(row.get('Taxa Juros Aplicada', '')).strip(),
                "OBSERVACOES": str(row.get('Observa√ß√£o', row.get('Observacoes', ''))).strip()
            }
        
        elif bank_type == "FACTA92":
            # Mapeamento FACTA92 - Estrutura REAL baseada em map_relat_atualizados.txt
            # Colunas principais: CODIGO, NM_CLIENT, CPF, VL_LIQUIDO, VL_BRUTO, NUMEROPRESTACAO, DS_TABCOM, CORRETOR
            
            # Proposta pode estar em CODIGO, PROPOSTA ou NUMERO_CONTRATO
            proposta = str(row.get('CODIGO', row.get('PROPOSTA', row.get('NUMERO_CONTRATO', '')))).strip()
            
            # Nome do cliente em NM_CLIENT
            nome = str(row.get('NM_CLIENT', row.get('NOME', row.get('CLIENTE', '')))).strip()
            
            # CPF
            cpf = str(row.get('CPF', '')).strip()
            
            # Valores (mapeamento melhorado baseado no arquivo real)
            vl_liquido = str(row.get('VL_LIQUIDO', row.get('VALOR_LIBERADO', row.get('VLR_LIBERADO', '')))).strip()
            vl_bruto = str(row.get('VL_BRUTO', row.get('VALOR_OPERACAO', row.get('VLR_FINANCIADO', '')))).strip()
            vl_parcela = str(row.get('VL_PARCELA', row.get('VALOR_PARCELA', row.get('VLR_PRESTACAO', '')))).strip()
            
            # Parcelas
            num_parcelas = str(row.get('NUMEROPRESTACAO', row.get('NUMERO_PARCELAS', row.get('PARCELAS', row.get('QTD_PARCELAS', ''))))).strip()
            
            # Data de nascimento (se dispon√≠vel)
            data_nascimento = str(row.get('DATA_NASCIMENTO', row.get('DT_NASCIMENTO', ''))).strip()
            
            # Status/Situa√ß√£o
            situacao = str(row.get('STATUS', row.get('SITUACAO', row.get('SITUACAO_PROPOSTA', '')))).strip()
            if not situacao:
                situacao = "PAGO"  # FACTA92 default para pagos
            
            # Tabela - DS_TABCOM tem formato: "60186 - INSS NOVO GOLD MAX PN-S"
            # NR_TABCOM tem o c√≥digo num√©rico: 60186
            tabela_completa = str(row.get('DS_TABCOM', row.get('TABELA', row.get('TIPO_TABELA', '')))).strip()
            nr_tabcom = str(row.get('NR_TABCOM', '')).strip()
            
            # ‚úÖ CORRE√á√ÉO: Extrair c√≥digo da tabela (priorizar NR_TABCOM)
            codigo_tabela = ""
            if nr_tabcom:
                # Usar NR_TABCOM diretamente (mais confi√°vel)
                codigo_tabela = nr_tabcom
                logging.info(f"‚úÖ FACTA92 c√≥digo de NR_TABCOM: {codigo_tabela}")
            elif tabela_completa:
                # Fallback: extrair do DS_TABCOM
                import re
                match = re.match(r'^(\d+)', tabela_completa)
                if match:
                    codigo_tabela = match.group(1)
                    logging.info(f"‚úÖ FACTA92 c√≥digo extra√≠do de DS_TABCOM: '{tabela_completa}' ‚Üí '{codigo_tabela}'")
                else:
                    codigo_tabela = tabela_completa
                    logging.warning(f"‚ö†Ô∏è FACTA92 n√£o conseguiu extrair c√≥digo de: '{tabela_completa}'")
            
            # Usu√°rio/Corretor
            usuario = str(row.get('LOGIN_CORRETOR', row.get('CORRETOR', row.get('USUARIO', '')))).strip()
            
            # Data
            data_cadastro = str(row.get('DATA_CADASTRO', row.get('DATA_REGISTRO', row.get('DATA', '')))).strip()
            data_pagamento = str(row.get('DATAEFETIVACAO', row.get('DATA_PAGAMENTO_CLIENTE', row.get('DATA_PAGAMENTO', '')))).strip()
            
            # Conv√™nio e detec√ß√£o de √≥rg√£o melhorada
            convenio = str(row.get('CONVENIO', '')).strip()
            
            # ‚úÖ CORRE√á√ÉO: Detectar √≥rg√£o baseado no DS_TABCOM
            orgao = 'INSS'  # Default
            if tabela_completa:
                tabela_upper = tabela_completa.upper()
                if 'FGTS' in tabela_upper:
                    orgao = 'FGTS'  # ‚úÖ FGTS √© √≥rg√£o pr√≥prio
                elif 'CLT' in tabela_upper:
                    orgao = 'CR√âDITO DO TRABALHADOR'  # ‚úÖ CLT √© cr√©dito do trabalhador
                elif 'INSS' in tabela_upper:
                    orgao = 'INSS'
                elif 'SIAPE' in tabela_upper:
                    orgao = 'SIAPE'
                elif 'PREFEITURA' in tabela_upper or 'PREF' in tabela_upper:
                    orgao = 'PREFEITURA'
                else:
                    orgao = 'INSS'
            
            # ‚ùå N√ÉO buscar TAXA do arquivo - deixar vazio para o mapeamento preencher com 0,00%
            # O relat_orgaos.csv tem a taxa correta (0,00% para FACTA)
            taxa_formatada = ""  # Sempre vazio - ser√° preenchido pelo apply_mapping()
            
            # Log para debug
            logging.info(f"‚úÖ FACTA92 processado: PROPOSTA={proposta}, CODIGO_TABELA={codigo_tabela}, ORGAO={orgao}, TAXA={taxa_formatada}")
            
            normalized_row = {
                "PROPOSTA": proposta,
                "DATA_CADASTRO": data_cadastro,
                "BANCO": "FACTA FINANCEIRA",  # ‚úÖ Nome correto
                "ORGAO": orgao,
                "TIPO_OPERACAO": "",  # ‚úÖ Ser√° buscado em relat_orgaos.csv pelo CODIGO_TABELA
                "NUMERO_PARCELAS": num_parcelas,
                "VALOR_OPERACAO": vl_bruto if vl_bruto else vl_liquido,
                "VALOR_LIBERADO": vl_liquido,
                "USUARIO_BANCO": usuario,
                "SITUACAO": situacao if situacao else "PAGO",
                "DATA_PAGAMENTO": data_pagamento,
                "CPF": cpf,
                "NOME": nome,
                "DATA_NASCIMENTO": data_nascimento,
                "CODIGO_TABELA": codigo_tabela,  # ‚úÖ C√≥digo da tabela (ex: 61700)
                "VALOR_PARCELAS": vl_parcela,
                "TAXA": taxa_formatada,  # ‚úÖ Vazia - ser√° preenchida pelo mapeamento
                "OBSERVACOES": ""
            }
        
        elif bank_type == "PAULISTA":
            logging.error(f"üéØ CHEGOU NO BLOCO PAULISTA! Linha {idx}: '{row.get('Unnamed: 0', '')}'")
            logging.error(f"üí• INICIO DO PROCESSAMENTO PAULISTA - ANTES DE QUALQUER L√ìGICA")
            logging.info(f"=" * 80)
            logging.info(f"üè¶ PAULISTA linha {idx}: INICIANDO PROCESSAMENTO")
            logging.info(f"   Colunas dispon√≠veis: {list(row.keys())[:10]}")
            
            # üîß CORRE√á√ÉO PAULISTA: Verificar se primeira linha cont√©m cabe√ßalhos
            primeira_celula = row.get('Unnamed: 0', '')
            logging.error(f"üîß PAULISTA: Verificando primeira_celula = '{primeira_celula}'")
            if str(primeira_celula) == 'N¬∫ Proposta':
                logging.info(f"ÔøΩ PAULISTA: Primeira linha √© cabe√ßalho! Pulando...")
                continue  # Pular linha de cabe√ßalho
            
            logging.error(f"üéâ PAULISTA: Passou da verifica√ß√£o de cabe√ßalho! Continuando processamento...")
            
            # Debug da linha atual - log detalhado para ver o que est√° vindo
            logging.error(f"üîç PAULISTA: Iniciando debug da linha atual...")
            logging.info(f"   üîç Proposta bruta: '{primeira_celula}' (tipo: {type(primeira_celula)})")
            logging.info(f"   üîç Segunda coluna (Contrato): '{row.get('Unnamed: 1', '')}'")
            logging.info(f"   üîç CPF: '{row.get('Unnamed: 4', '')}'")
            logging.info(f"   üîç Nome: '{row.get('Unnamed: 5', '')}'")
            
            logging.error(f"üîç PAULISTA: Ap√≥s logs b√°sicos - continuando...")
            logging.info(f"=" * 80)
            
            def detect_paulista_organ(especies_beneficio, produto, proposta=""):
                """Detecta √≥rg√£o do Paulista de forma inteligente"""
                especies_upper = especies_beneficio.upper() if especies_beneficio else ""
                produto_upper = produto.upper() if produto else ""
                proposta_upper = proposta.upper() if proposta else ""
                
                logging.info(f"üèõÔ∏è PAULISTA detectando √≥rg√£o: esp√©cie='{especies_upper[:30]}...' | produto='{produto_upper[:30]}...'")
                
                # An√°lise combinada de esp√©cie + produto
                combined_text = f"{especies_upper} {produto_upper} {proposta_upper}"
                
                # INSS - mais comum no Paulista
                if any(x in combined_text for x in ['INSS', 'APOSENT', 'PENS√ÉO', 'PENSAO', 'BENEFICI', 'PREVIDENC']):
                    return "INSS"
                
                # FGTS
                elif any(x in combined_text for x in ['FGTS', 'TRABALHADOR', 'SAQUE']):
                    return "FGTS"
                
                # Prefeituras ou outros √≥rg√£os
                elif any(x in combined_text for x in ['PREFEIT', 'MUNICIPAL', 'SERVIDOR']):
                    return "PREFEITURA"
                
                # Padr√£o: INSS (Paulista √© principalmente INSS)
                else:
                    return "INSS"
            
            def detect_paulista_operation(produto, especies_beneficio="", status=""):
                """Detecta tipo de opera√ß√£o do Paulista"""
                produto_upper = produto.upper() if produto else ""
                especies_upper = especies_beneficio.upper() if especies_beneficio else ""
                status_upper = status.upper() if status else ""
                
                logging.info(f"üîß PAULISTA detectando opera√ß√£o: produto='{produto_upper[:30]}...' | esp√©cie='{especies_upper[:20]}...'")
                
                combined_text = f"{produto_upper} {especies_upper} {status_upper}"
                
                # Refinanciamento
                if any(x in combined_text for x in ['REFIN', 'REFINANCIAMENTO']):
                    return "Refinanciamento"
                
                # Portabilidade
                elif any(x in combined_text for x in ['PORT', 'PORTABILIDADE']):
                    return "Portabilidade"
                
                # Margem Livre (mais comum)
                elif any(x in combined_text for x in ['MARGEM', 'LIVRE', 'CONSIGNADO', 'NOVO']):
                    return "Margem Livre (Novo)"
                
                # Padr√£o
                else:
                    return "Margem Livre (Novo)"
            
            # Mapeamento BANCO PAULISTA - Colunas NOMEADAS (n√£o Unnamed!)
            # Baseado no arquivo real: N¬∫ Proposta | Contrato | Data Captura | etc.
            
            # Mapeamento PAULISTA corrigido baseado em map_relat_atualizados.txt
            # Unnamed: 0='N¬∫ Proposta', 1='Contrato', 2='Data Captura', 3='Dt Atividade'  
            # Unnamed: 4='CPF/CNPJ Proponente', 5='Nome do Proponente', 6='Matr√≠cula', 7='Esp√©cie Benef√≠cio'
            # Unnamed: 8='Banco', 9='Ag√™ncia', 10='Conta', 11='Valor Solicitado'
            # Unnamed: 12='Vl. Liberado', 13='Vl. Troco', 14='Quant. Parcelas', 15='Valor Parcela'
            # Unnamed: 16='Plano', 17='1¬∫ Vencimento', 18='Produto', 19='Fase', 20='Status'
            # Unnamed: 21='Dta. Integra√ß√£o', 22='Loja/Sub', 23='Lojista/Master', 24='Usu√°rio Digitador'
            
            proposta = str(row.get('Unnamed: 0', '')).strip()  # N¬∫ Proposta
            especies_beneficio = str(row.get('Unnamed: 7', '')).strip()  # Esp√©cie Benef√≠cio
            produto = str(row.get('Unnamed: 18', '')).strip()  # Produto
            status = str(row.get('Unnamed: 20', '')).strip()  # Status
            
            # Logs detalhados para debug
            logging.info(f"üìã PAULISTA extra√≠do: Proposta={proposta}, Produto={produto[:30] if produto else 'N/A'}")
            logging.info(f"üìã PAULISTA status='{status}', esp√©cie='{especies_beneficio[:20] if especies_beneficio else 'N/A'}'")
            
            logging.error(f"üîç PAULISTA: Chegou na valida√ß√£o! Proposta extra√≠da: '{proposta}'")
            
            # ‚úÖ VALIDA√á√ÉO: Verificar se linha deve ser processada
            if not proposta or proposta.upper() in ['NAN', 'NONE', '', 'UNNAMED: 0', 'N¬∫ PROPOSTA', 'PROPOSTA']:
                logging.info(f"‚è≠Ô∏è PAULISTA linha {idx}: Pulando - proposta vazia ou cabe√ßalho ({proposta})")
                normalized_row = None
            else:
                logging.info(f"‚úÖ PAULISTA linha {idx}: Proposta v√°lida - vai processar")
                
                # Detectar √≥rg√£o e opera√ß√£o
                orgao_detectado = detect_paulista_organ(especies_beneficio, produto, proposta)
                operacao_detectada = detect_paulista_operation(produto, especies_beneficio, status)
                
                # Aplicar formata√ß√£o brasileira usando posi√ß√µes Unnamed corretas do mapeamento
                valor_operacao_raw = str(row.get('Unnamed: 11', '')).strip()  # Valor Solicitado
                valor_parcela_raw = str(row.get('Unnamed: 15', '')).strip()   # Valor Parcela  
                valor_liberado_raw = str(row.get('Unnamed: 12', '')).strip()  # Vl. Liberado
                cpf_raw = str(row.get('Unnamed: 4', '')).strip()             # CPF/CNPJ Proponente
                
                # Usar as fun√ß√µes globais de formata√ß√£o
                valor_operacao_formatted = format_value_brazilian(valor_operacao_raw)
                valor_liberado_formatted = format_value_brazilian(valor_liberado_raw)
                valor_parcela_formatted = format_value_brazilian(valor_parcela_raw)
                cpf_formatted = format_cpf_global(cpf_raw)
                
                logging.info(f"‚úÖ PAULISTA formatado: CPF={cpf_formatted}, Valor={valor_operacao_formatted}, √ìrg√£o={orgao_detectado}")
                
                normalized_row = {
                    "PROPOSTA": proposta,  # Unnamed: 0 = N¬∫ Proposta
                    "ADE": proposta,  # Campo ADE expl√≠cito = mesma proposta
                    "DATA_CADASTRO": str(row.get('Unnamed: 2', '')).strip(),  # Data Captura
                    "BANCO": "BANCO PAULISTA",
                    "ORGAO": orgao_detectado,
                    "TIPO_OPERACAO": operacao_detectada,
                    "NUMERO_PARCELAS": str(row.get('Unnamed: 14', '')).strip(),  # Quant. Parcelas
                    "VALOR_OPERACAO": valor_operacao_formatted,  # ‚úÖ Formatado brasileiro
                    "VALOR_LIBERADO": valor_liberado_formatted,  # ‚úÖ Formatado brasileiro
                    "USUARIO_BANCO": str(row.get('Unnamed: 24', '')).strip(),   # Usu√°rio Digitador
                    "SITUACAO": status,  # STATUS direto (ser√° normalizado depois)
                    "DATA_PAGAMENTO": str(row.get('Unnamed: 21', '')).strip(),  # Dta. Integra√ß√£o
                    "CPF": cpf_formatted,  # ‚úÖ Formatado brasileiro (XXX.XXX.XXX-XX)
                    "NOME": str(row.get('Unnamed: 5', '')).strip().upper(),     # Nome do Proponente ‚úÖ Mai√∫sculas
                    "DATA_NASCIMENTO": "",  # N√£o dispon√≠vel no PAULISTA
                    "CODIGO_TABELA": str(row.get('Unnamed: 16', '')).strip(),   # Plano - ser√° mapeado pelo Storm depois
                    "VALOR_PARCELAS": valor_parcela_formatted,  # ‚úÖ Formatado brasileiro
                    "TAXA": "0,00%",  # Padr√£o brasileiro
                    "OBSERVACOES": f"Contrato: {str(row.get('Unnamed: 1', '')).strip()} | Banco: {str(row.get('Unnamed: 8', '')).strip()} | Ag√™ncia: {str(row.get('Unnamed: 9', '')).strip()}"
                }
                
                logging.info(f"‚úÖ‚úÖ‚úÖ PAULISTA normalized_row criado com sucesso!")
                logging.info(f"‚úÖ‚úÖ‚úÖ PAULISTA normalized_row final: {normalized_row}")
        
        elif bank_type == "BRB":
            # Mapeamento BRB (Banco de Bras√≠lia) - Estrutura REAL do arquivo
            # Colunas REAIS do arquivo BRB (Propostas-202593.csv):
            # - ID Card: ID interno Q-FAZ (2579370)
            # - N¬∫ Contrato: N√∫mero da proposta BRB (1901615764) ‚úÖ USAR ESTE!
            # - Nome do cliente
            # - CPF do Benefici√°rio (sem formata√ß√£o)
            # - Data da Proposta
            # - Qtd. Parcelas
            # - Valor da Parcela (formato: 294,30)
            # - Valor da Proposta (formato: 13082,34)
            # - Tabela: TAXA em decimal (1.85, 1.79, 1) ‚úÖ N√ÉO √© c√≥digo!
            # - Produto: Tipo opera√ß√£o (Refinanciamento, Novo, Portabilidade e Refinanciamento)
            # - Status da Proposta (Nova proposta, Pago, Perdido, etc)
            # - Sub-Status (opcional)
            # - Observa√ß√µes (opcional)
            # - E-mail Agente Respons√°vel
            # - Parceiro: "70-BRB - Banco de Bras√≠lia S.A."
            
            normalized_row = {
                "PROPOSTA": str(row.get('N¬∫ Contrato', '')).strip(),  # ‚úÖ CORRIGIDO: N¬∫ Contrato, n√£o ID Card!
                "DATA_CADASTRO": str(row.get('Data da Proposta', '')).strip(),
                "BANCO": "BRB - CR√âDITO, FINANCIAMENTO E INVESTIMENTO",  # ‚úÖ Nome completo
                "ORGAO": "INSS",  # ‚úÖ Todos BRB s√£o INSS (arquivo n√£o tem coluna ORG√ÉO)
                "TIPO_OPERACAO": str(row.get('Produto', 'Margem Livre (Novo)')).strip(),  # ‚úÖ CORRIGIDO: Produto, n√£o OPERA√á√ÉO!
                "NUMERO_PARCELAS": str(row.get('Qtd. Parcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor da Proposta', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor da Proposta', '')).strip(),
                "USUARIO_BANCO": str(row.get('E-mail Agente Respons√°vel', '')).strip(),
                "SITUACAO": str(row.get('Status da Proposta', '')).strip(),
                "DATA_PAGAMENTO": "",  # ‚úÖ Arquivo n√£o tem Data da PAGAMENTO, ser√° vazio
                "CPF": str(row.get('CPF do Benefici√°rio', '')).strip(),
                "NOME": str(row.get('Nome do cliente', '')).strip().upper(),
                "DATA_NASCIMENTO": "",  # N√£o dispon√≠vel
                "CODIGO_TABELA": str(row.get('Tabela', '')).strip(),  # ‚úÖ Tabela = TAXA que vira c√≥digo (1.85 ‚Üí 185)
                "VALOR_PARCELAS": str(row.get('Valor da Parcela', '')).strip(),
                "TAXA": str(row.get('Tabela', '')).strip(),  # ‚úÖ CORRIGIDO: Tabela cont√©m TAXA!
                "OBSERVACOES": str(row.get('Observa√ß√µes', '')).strip()
            }
            
            # ‚úÖ FORMATA√á√ÉO BRASILEIRA para BRB
            # Converter valores para formato brasileiro COM R$
            valor_operacao = normalized_row.get("VALOR_OPERACAO", "")
            if valor_operacao:
                valor_formatado = format_value_brazilian(valor_operacao)
                normalized_row["VALOR_OPERACAO"] = f"R$ {valor_formatado}"
            
            valor_liberado = normalized_row.get("VALOR_LIBERADO", "")
            if valor_liberado:
                valor_formatado = format_value_brazilian(valor_liberado)
                normalized_row["VALOR_LIBERADO"] = f"R$ {valor_formatado}"
            
            valor_parcelas = normalized_row.get("VALOR_PARCELAS", "")
            if valor_parcelas:
                valor_formatado = format_value_brazilian(valor_parcelas)
                normalized_row["VALOR_PARCELAS"] = f"R$ {valor_formatado}"
            
            # Formatar CPF para padr√£o brasileiro (vem sem formata√ß√£o: 13097582800)
            normalized_row["CPF"] = format_cpf_global(normalized_row.get("CPF", ""))
            
            # Converter CODIGO_TABELA de taxa decimal para c√≥digo inteiro
            # BRB: Tabela = 1.85 ‚Üí CODIGO_TABELA = 185
            #      Tabela = 1.79 ‚Üí CODIGO_TABELA = 179
            #      Tabela = 1 ‚Üí CODIGO_TABELA = 100
            codigo_tabela_raw = normalized_row.get("CODIGO_TABELA", "")
            if codigo_tabela_raw:
                try:
                    taxa_str = str(codigo_tabela_raw).replace(',', '.')
                    taxa_float = float(taxa_str)
                    # Multiplicar por 100 para obter c√≥digo
                    codigo_int = int(taxa_float * 100)
                    normalized_row["CODIGO_TABELA"] = str(codigo_int)
                    logging.info(f"  üî¢ CODIGO_TABELA: {codigo_tabela_raw} ‚Üí {codigo_int}")
                except (ValueError, TypeError):
                    normalized_row["CODIGO_TABELA"] = str(codigo_tabela_raw)
            
            # Formatar TAXA para percentual brasileiro
            # BRB vem como decimal COM PONTO: 1.85 ‚Üí deve virar 1,85%
            taxa_raw = normalized_row.get("TAXA", "")
            if taxa_raw:
                try:
                    taxa_str = str(taxa_raw).replace(',', '.')
                    taxa_float = float(taxa_str)
                    
                    if taxa_float < 10:
                        normalized_row["TAXA"] = f"{taxa_float:.2f}%".replace('.', ',')
                    else:
                        taxa_percentual = taxa_float / 100
                        normalized_row["TAXA"] = f"{taxa_percentual:.2f}%".replace('.', ',')
                except (ValueError, TypeError):
                    normalized_row["TAXA"] = format_percentage_brazilian(taxa_raw)
            
            logging.info(f"‚úÖ BRB formatado: PROPOSTA={normalized_row.get('PROPOSTA')}, TAXA={normalized_row.get('TAXA')}, CODIGO={normalized_row.get('CODIGO_TABELA')}")
            
            # üîß NORMALIZA√á√ÉO DE STATUS BRB
            # Nova proposta ‚Üí AGUARDANDO
            # Ag. aprova√ß√£o do conv√™nio ‚Üí AGUARDANDO
            # Formaliza√ß√£o cliente ‚Üí AGUARDANDO
            # Pendente de documenta√ß√£o ‚Üí AGUARDANDO
            # (vazio) ‚Üí AGUARDANDO
            # Perdido ‚Üí CANCELADO
            # PAGO ‚Üí PAGO
            situacao_original = normalized_row.get('SITUACAO', '').strip()
            situacao_upper = situacao_original.upper()
            
            if not situacao_original or situacao_original == '':
                normalized_row['SITUACAO'] = 'AGUARDANDO'
                logging.info(f"  üìä STATUS: (vazio) ‚Üí AGUARDANDO")
            elif 'NOVA PROPOSTA' in situacao_upper:
                normalized_row['SITUACAO'] = 'AGUARDANDO'
                logging.info(f"  üìä STATUS: {situacao_original} ‚Üí AGUARDANDO")
            elif 'APROVA√á√ÉO' in situacao_upper or 'CONV√äNIO' in situacao_upper or 'CONVENIO' in situacao_upper:
                normalized_row['SITUACAO'] = 'AGUARDANDO'
                logging.info(f"  üìä STATUS: {situacao_original} ‚Üí AGUARDANDO")
            elif 'FORMALIZA√á√ÉO' in situacao_upper or 'FORMALIZACAO' in situacao_upper:
                normalized_row['SITUACAO'] = 'AGUARDANDO'
                logging.info(f"  üìä STATUS: {situacao_original} ‚Üí AGUARDANDO")
            elif 'PENDENTE' in situacao_upper or 'DOCUMENTA√á√ÉO' in situacao_upper or 'DOCUMENTACAO' in situacao_upper:
                normalized_row['SITUACAO'] = 'AGUARDANDO'
                logging.info(f"  üìä STATUS: {situacao_original} ‚Üí AGUARDANDO")
            elif 'PERDIDO' in situacao_upper:
                normalized_row['SITUACAO'] = 'CANCELADO'
                logging.info(f"  üìä STATUS: {situacao_original} ‚Üí CANCELADO")
            elif 'PAGO' in situacao_upper:
                normalized_row['SITUACAO'] = 'PAGO'
                logging.info(f"  üìä STATUS: {situacao_original} ‚Üí PAGO")
            elif 'CANCELAD' in situacao_upper:
                normalized_row['SITUACAO'] = 'CANCELADO'
                logging.info(f"  üìä STATUS: {situacao_original} ‚Üí CANCELADO")
            else:
                # Manter original se n√£o reconhecer
                normalized_row['SITUACAO'] = 'AGUARDANDO'
                logging.info(f"  üìä STATUS: {situacao_original} ‚Üí AGUARDANDO (padr√£o)")
            
            # üîß REGRAS ESPEC√çFICAS BRB - Portabilidade e Refinanciamento
            try:
                tipo_operacao = normalized_row.get('TIPO_OPERACAO', '').upper()
                observacoes = normalized_row.get('OBSERVACOES', '').upper()
                
                # Produtos BRB que s√£o Portabilidade/Refin:
                # - "Portabilidade e Refinanciamento" (campo Produto)
                # - "Refinanciamento" (campo Produto)
                # Estes N√ÉO devem esvaziar CODIGO_TABELA (c√≥digo deve vir da taxa)
                
                if 'PORTABILIDADE' in tipo_operacao:
                    # Portabilidade: manter CODIGO_TABELA e DATA_PAGAMENTO vazios
                    normalized_row['DATA_PAGAMENTO'] = ''
                    
                    # Adicionar marcador nas observa√ß√µes
                    obs_atual = normalized_row.get('OBSERVACOES', '')
                    if obs_atual:
                        normalized_row['OBSERVACOES'] = f"{obs_atual} | MANUAL: Portabilidade/Refin"
                    else:
                        normalized_row['OBSERVACOES'] = "MANUAL: Portabilidade/Refin"
                    
                    logging.info(f"üîß BRB PROPOSTA {normalized_row.get('PROPOSTA')}: {tipo_operacao} - DATA_PAGAMENTO vazio, CODIGO_TABELA mantido")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Erro aplicando regras espec√≠ficas BRB: {e}")
        
        elif bank_type == "QUALIBANKING":
            # Mapeamento QUALIBANKING - Baseado em map_relat_atualizados.txt
            normalized_row = {
                "PROPOSTA": str(row.get('N√∫mero do Contrato', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data da Proposta', '')).strip(),
                "BANCO": "QUALIBANKING",
                "ORGAO": str(row.get('Tipo de Produto', 'INSS')).strip(),
                "TIPO_OPERACAO": str(row.get('Tipo de Opera√ß√£o', 'Margem Livre (Novo)')).strip(),
                "NUMERO_PARCELAS": str(row.get('Prazo', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor do Empr√©stimo', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor L√≠quido ao Cliente', '')).strip(),
                "USUARIO_BANCO": str(row.get('Login', '')).strip(),
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data do Cr√©dito ao Cliente', '')).strip(),
                "CPF": str(row.get('CPF', '')).strip(),
                "NOME": str(row.get('Nome', '')).strip(),
                "DATA_NASCIMENTO": "",  # N√£o dispon√≠vel
                "CODIGO_TABELA": str(row.get('Nome da Tabela', '')).strip(),
                "VALOR_PARCELAS": str(row.get('Valor da Parcela', '')).strip(),
                "TAXA": str(row.get('Taxa', '')).strip(),
                "OBSERVACOES": str(row.get('Motivo do Status', '')).strip()
            }
        
        elif bank_type == "MERCANTIL":
            # Mapeamento BANCO MERCANTIL - Baseado em map_relat_atualizados.txt
            # Detectar √≥rg√£o pelo nome do conv√™nio
            nome_convenio = str(row.get('NomeConvenio', '')).upper()
            if 'FGTS' in nome_convenio or 'F.G.T.S' in nome_convenio:
                orgao = 'FGTS'
            elif 'INSS' in nome_convenio:
                orgao = 'INSS'
            else:
                orgao = 'INSS'  # Default
            
            # Detectar tipo de opera√ß√£o
            modalidade = str(row.get('ModalidadeCredito', '')).upper()
            if 'FGTS' in modalidade or 'SAQUEANIVERSARIOFGTS' in modalidade:
                tipo_operacao = 'Margem Livre (Novo)'
            elif 'CREDITOPESSOAL' in modalidade:
                tipo_operacao = 'Margem Livre (Novo)'
            else:
                tipo_operacao = 'Margem Livre (Novo)'
            
            normalized_row = {
                "PROPOSTA": str(row.get('NumeroProposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('DataCadastro', '')).strip(),
                "BANCO": "BANCO MERCANTIL",
                "ORGAO": orgao,
                "TIPO_OPERACAO": tipo_operacao,
                "NUMERO_PARCELAS": str(row.get('QuantidadeParcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('ValorFinanciado', '')).strip(),
                "VALOR_LIBERADO": str(row.get('ValorEmprestimo', '')).strip(),
                "USUARIO_BANCO": str(row.get('LoginUsuarioDigitador', '')).strip(),
                "SITUACAO": str(row.get('SituacaoProposta', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('DataPagamentoCliente', '')).strip(),
                "CPF": str(row.get('Cpf', '')).strip(),
                "NOME": str(row.get('Nome', '')).strip(),
                "DATA_NASCIMENTO": str(row.get('DataNascimento', '')).strip(),
                "CODIGO_TABELA": str(row.get('CodigoProduto', '')).strip(),
                "VALOR_PARCELAS": str(row.get('ValorParcela', '')).strip(),
                "TAXA": str(row.get('TaxaJurosMes', '')).strip(),
                "OBSERVACOES": ""
            }
        
        elif bank_type == "AMIGOZ":
            # Mapeamento BANCO AMIGOZ - Baseado em map_relat_atualizados.txt
            normalized_row = {
                "PROPOSTA": str(row.get('Nr Proposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data Cadastro', '')).strip(),
                "BANCO": "BANCO AMIGOZ",
                "ORGAO": str(row.get('Convenio', 'INSS')).strip(),
                "TIPO_OPERACAO": str(row.get('Produto', 'Cart√£o Consignado')).strip(),
                "NUMERO_PARCELAS": str(row.get('Qtd Parcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor Proposta', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor Liberado Cliente', '')).strip(),
                "USUARIO_BANCO": str(row.get('Usu√°rio Digitador', '')).strip(),
                "SITUACAO": str(row.get('Status Proposta', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data Integra√ß√£o', '')).strip(),
                "CPF": str(row.get('CPF Cliente', '')).strip(),
                "NOME": str(row.get('Nome Cliente', '')).strip(),
                "DATA_NASCIMENTO": "",  # N√£o dispon√≠vel diretamente
                "CODIGO_TABELA": str(row.get('Tipo de Cart√£o', '')).strip(),
                "VALOR_PARCELAS": "",  # N√£o dispon√≠vel
                "TAXA": str(row.get('Taxa da Opera√ß√£o', '')).strip(),
                "OBSERVACOES": str(row.get('Restricoes', '')).strip()
            }
        
        elif bank_type == "TOTALCASH":
            # Mapeamento BANCO TOTALCASH - Colunas corretas
            convenio = str(row.get('Convenio', '')).strip()
            if 'INSS' in convenio:
                orgao = 'INSS'
            elif 'FGTS' in convenio:
                orgao = 'FGTS'
            else:
                orgao = 'INSS'  # Default
            
            normalized_row = {
                "PROPOSTA": str(row.get('Nr Proposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data Cadastro', '')).strip(),
                "BANCO": "BANCO TOTALCASH",
                "ORGAO": orgao,
                "TIPO_OPERACAO": str(row.get('Produto', 'Margem Livre (Novo)')).strip(),
                "NUMERO_PARCELAS": str(row.get('Qtd Parcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor Proposta', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor Liberado Cliente', '')).strip(),
                "USUARIO_BANCO": str(row.get('Usu√°rio Digitador', '')).strip(),
                "SITUACAO": str(row.get('Status Proposta', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data Integra√ß√£o', '')).strip(),
                "CPF": str(row.get('CPF Cliente', '')).strip(),
                "NOME": str(row.get('Nome Cliente', '')).strip(),
                "DATA_NASCIMENTO": "",  # N√£o dispon√≠vel
                "CODIGO_TABELA": "",  # N√£o dispon√≠vel
                "VALOR_PARCELAS": str(row.get('Valor Parcela', '')).strip(),
                "TAXA": str(row.get('Taxa da Opera√ß√£o', '')).strip(),
                "OBSERVACOES": str(row.get('Observa√ß√µes', row.get('Observacoes', row.get('Obs', '')))).strip()
            }
        
        else:  # GENERICO
            # Tentar mapear colunas automaticamente
            normalized_row = {
                "PROPOSTA": "",
                "DATA_CADASTRO": "",
                "BANCO": bank_type,
                "ORGAO": "",
                "TIPO_OPERACAO": "MARGEM LIVRE (NOVO)",
                "NUMERO_PARCELAS": "",
                "VALOR_OPERACAO": "",
                "VALOR_LIBERADO": "",
                "USUARIO_BANCO": "",
                "SITUACAO": "",
                "DATA_PAGAMENTO": "",
                "CPF": "",
                "NOME": "",
                "DATA_NASCIMENTO": "",
                "TAXA": ""
            }
            
            # Tentar mapear automaticamente baseado em nomes de colunas
            for col in df.columns:
                col_lower = str(col).lower()
                if 'proposta' in col_lower or 'id' in col_lower:
                    normalized_row["PROPOSTA"] = str(row.get(col, '')).strip()
                elif 'data' in col_lower and 'cadastro' in col_lower:
                    normalized_row["DATA_CADASTRO"] = str(row.get(col, '')).strip()
                elif 'nome' in col_lower and 'cliente' in col_lower:
                    normalized_row["NOME"] = str(row.get(col, '')).strip()
                elif 'cpf' in col_lower:
                    normalized_row["CPF"] = str(row.get(col, '')).strip()
                elif 'status' in col_lower or 'situacao' in col_lower:
                    normalized_row["SITUACAO"] = str(row.get(col, '')).strip()
                elif 'valor' in col_lower and ('operacao' in col_lower or 'liberado' in col_lower):
                    normalized_row["VALOR_LIBERADO"] = str(row.get(col, '')).strip()
                    if not normalized_row["VALOR_OPERACAO"]:
                        normalized_row["VALOR_OPERACAO"] = str(row.get(col, '')).strip()
        
        # ‚úÖ VERIFICA√á√ÉO CR√çTICA: Pular linhas filtradas (normalized_row = None)
        # DEVE VIR ANTES de qualquer acesso a normalized_row.get() ou normalized_row[]
        if normalized_row is None:
            logging.info(f"‚è≠Ô∏è [{bank_type}] Linha filtrada (normalized_row=None), pulando mapeamento e valida√ß√£o")
            continue
        
        # Aplicar mapeamento de status (normaliza√ß√£o completa)
        if normalized_row.get("SITUACAO"):
            situacao_original = str(normalized_row["SITUACAO"]).strip()
            situacao_lower = situacao_original.lower()
            
            # Tentar encontrar no mapeamento
            situacao_normalizada = STATUS_MAPPING.get(situacao_lower, None)
            
            # Se n√£o encontrou exato, tentar normalizar caracteres especiais e espa√ßos
            if not situacao_normalizada:
                # Remover acentos e caracteres especiais para busca mais flex√≠vel
                import unicodedata
                situacao_clean = ''.join(
                    c for c in unicodedata.normalize('NFD', situacao_lower)
                    if unicodedata.category(c) != 'Mn'
                )
                situacao_clean = situacao_clean.replace('/', ' ').replace('-', ' ').strip()
                situacao_clean = ' '.join(situacao_clean.split())  # Remove espa√ßos m√∫ltiplos
                
                # Tentar encontrar novamente
                situacao_normalizada = STATUS_MAPPING.get(situacao_clean, None)
            
            # Se ainda n√£o encontrou, fazer busca por palavras-chave
            if not situacao_normalizada:
                situacao_palavras = situacao_lower.lower()
                if any(word in situacao_palavras for word in ['pag', 'integra', 'finaliz', 'quitad', 'liberad', 'desembolsa', 'aprovad']):
                    situacao_normalizada = "PAGO"
                elif any(word in situacao_palavras for word in ['cancel', 'reprov', 'rejeit', 'negad', 'expirad', 'invalid', 'recus', 'desist']):
                    situacao_normalizada = "CANCELADO"
                elif any(word in situacao_palavras for word in ['aguard', 'pendent', 'aberto', 'digital', 'andament', 'analise', 'process', 'formal', 'cadastr', 'enviad']):
                    situacao_normalizada = "AGUARDANDO"
            
            # Aplicar a normaliza√ß√£o (ou manter original se n√£o encontrou)
            normalized_row["SITUACAO"] = situacao_normalizada if situacao_normalizada else situacao_original
            
            # Log para debug (apenas se n√£o encontrou mapeamento)
            if not situacao_normalizada:
                logging.warning(f"‚ö† Status n√£o mapeado: '{situacao_original}' - mantido como est√°")
        
        # üîß REGRA GERAL: Se SITUACAO vier vazia, definir como AGUARDANDO
        if not normalized_row.get("SITUACAO") or str(normalized_row.get("SITUACAO", "")).strip() == "":
            normalized_row["SITUACAO"] = "AGUARDANDO"
            logging.info(f"üìã Status vazio detectado - definido como AGUARDANDO para PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}")
        
        # Aplicar mapeamento de c√≥digo de tabela (sem depend√™ncia de usu√°rio para maior estabilidade)
        # EXCETO para DIGIO, AVERBAI, DAYCOVAL, QUERO_MAIS e SANTANDER que j√° t√™m c√≥digos corretos
        # VCTEX PRECISA de mapeamento: "Tabela EXP" (banco) ‚Üí "TabelaEXP" (storm)
        if bank_type == "DIGIO":
            # DIGIO j√° aplicou mapeamento espec√≠fico, pular mapeamento geral
            logging.info(f"üìä PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: DIGIO j√° mapeado, pulando mapeamento geral")
            mapping_result = None
        elif bank_type == "SANTANDER":
            # üè¶ SANTANDER: C√≥digos j√° extra√≠dos corretamente (810021387, 82721387, etc.)
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            logging.info(f"‚úÖ PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: SANTANDER c√≥digo direto {codigo_direto}, pulando mapeamento autom√°tico")
            mapping_result = None
        elif bank_type == "AVERBAI" and normalized_row.get("CODIGO_TABELA", "").isdigit():
            # üéØ AVERBAI com c√≥digo direto do arquivo - n√£o precisa mapeamento complexo!
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            logging.info(f"‚úÖ PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: AVERBAI c√≥digo direto {codigo_direto}, pulando mapeamento complexo")
            mapping_result = None
        elif bank_type == "DAYCOVAL" and normalized_row.get("CODIGO_TABELA", "").isdigit():
            # üéØ DAYCOVAL com c√≥digo direto do arquivo - mesma l√≥gica do AVERBAI!
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            logging.info(f"‚úÖ PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: DAYCOVAL c√≥digo direto {codigo_direto}, pulando mapeamento complexo")
            mapping_result = None
        elif bank_type == "QUERO_MAIS":
            # üéØ QUERO MAIS - preservar c√≥digos originais, n√£o aplicar mapeamento autom√°tico
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            logging.info(f"‚úÖ PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: QUERO MAIS c√≥digo direto {codigo_direto}, pulando mapeamento autom√°tico")
            mapping_result = None
        elif bank_type == "VCTEX":
            # üéØ VCTEX - Processamento com mapeamento direto (BYPASS DO SISTEMA CSV)
            tabela_original = normalized_row.get("CODIGO_TABELA", "").strip()
            
            # Mapeamento direto VCTEX
            vctex_map = {
                "Tabela Vamo Com Tudo": "TabelaVamoComTudo",
                "Tabela Vamo com tudo com Seguro": "TabelaVamoComTudoComSeg", 
                "Tabela Exponencial": "TabelaExponencial",
                "Tabela Relax": "TabelaRelax",
                "Tabela VCT": "TabelaVCT",
                "Tabela EXP": "TabelaEXP",
                "Tabela INSS Exponencial TX 1,85 - com Seguro Hot": "TabelaExponencialHot",
                "TabelaVamoComTudoComSeg": "TabelaVamoComTudoComSeg"
            }
            
            # Aplicar mapeamento direto se encontrar
            if tabela_original in vctex_map:
                codigo_novo = vctex_map[tabela_original]
                normalized_row["CODIGO_TABELA"] = codigo_novo
                normalized_row["TAXA"] = "1,83%"
                normalized_row["TIPO_OPERACAO"] = "Margem Livre (Novo)"
                print(f"‚úÖüéØ VCTEX MAPEAMENTO DIRETO: '{tabela_original}' ‚Üí '{codigo_novo}'")
                logging.warning(f"‚úÖüéØ VCTEX MAPEAMENTO DIRETO: '{tabela_original}' ‚Üí '{codigo_novo}'")
            else:
                print(f"‚ö†Ô∏èüéØ VCTEX SEM MAPEAMENTO: mantendo '{tabela_original}'")
                logging.warning(f"‚ö†Ô∏èüéØ VCTEX SEM MAPEAMENTO: mantendo '{tabela_original}'")
            
            # SKIP o sistema de mapeamento CSV para VCTEX - usar None para evitar sobrescrita
            mapping_result = None
        elif bank_type == "FACTA92":
            # üéØ FACTA92 - c√≥digo vem correto do arquivo (NR_TABCOM), buscar por BANCO + CODIGO apenas
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            banco_para_mapeamento = normalized_row.get("BANCO", "")
            
            print(f"üö®üö®üö® FACTA92 INICIOU - Proposta {normalized_row.get('PROPOSTA', 'N/A')}, Codigo {codigo_direto}")
            logging.warning(f"ÔøΩ FACTA92 PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: C√≥digo {codigo_direto}, buscando mapeamento por BANCO + CODIGO")
            
            # üîç Buscar no TABELA_MAPPING usando apenas BANCO e CODIGO (ignorar ORGAO e OPERACAO)
            mapping_result = None
            codigo_norm = ' '.join(str(codigo_direto).strip().upper().split()) if codigo_direto else ""
            banco_norm = ' '.join(str(banco_para_mapeamento).strip().upper().split())
            
            logging.info(f"üîç FACTA92 Debug INICIAL: banco_norm='{banco_norm}', codigo_norm='{codigo_norm}', codigo_isdigit={codigo_norm.isdigit()}")
            logging.info(f"üîç FACTA92 TABELA_MAPPING type: {type(TABELA_MAPPING)}, len: {len(TABELA_MAPPING) if TABELA_MAPPING else 0}")
            
            # DEBUG: Mostrar TODAS as chaves FACTA que cont√©m 61700 ou CLT
            if TABELA_MAPPING:
                facta_keys = [k for k in TABELA_MAPPING.keys() if 'FACTA' in k]
                logging.warning(f"üîç FACTA92 Total de chaves FACTA no mapping: {len(facta_keys)}")
                
                # Procurar especificamente 61700
                keys_61700 = [k for k in facta_keys if '61700' in k]
                logging.warning(f"üîç FACTA92 Chaves com 61700: {len(keys_61700)}")
                for k in keys_61700:
                    logging.warning(f"   -> '{k}'")
                    
                # Mostrar todas as chaves CLT
                keys_clt = [k for k in facta_keys if 'CLT' in k or 'CR√âDITO DO TRABALHADOR' in k or 'CREDITO DO TRABALHADOR' in k]
                logging.warning(f"üîç FACTA92 Total chaves CLT/CREDITO: {len(keys_clt)}")
                for i, k in enumerate(keys_clt[:5]):
                    logging.warning(f"   CLT #{i+1}: '{k}'")
            
            if codigo_norm and TABELA_MAPPING:
                matches_found = 0
                banco_matches = 0
                for key, details in TABELA_MAPPING.items():
                    parts = key.split('|')
                    if len(parts) == 4:
                        key_banco, key_orgao, key_operacao, key_tabela = parts
                        key_banco_norm = ' '.join(key_banco.upper().split())
                        key_tabela_norm = ' '.join(key_tabela.upper().split())
                        
                        # Contar quantos matches de banco temos
                        if banco_norm == key_banco_norm:
                            banco_matches += 1
                            
                            # Debug: Log primeiras 3 chaves do CSV que batem com BANCO
                            if matches_found < 3:
                                logging.info(f"üîç FACTA92 Exemplo chave CSV #{matches_found+1}: '{key}' ‚Üí banco='{key_banco_norm}', tabela='{key_tabela_norm}'")
                                matches_found += 1
                        
                        # Match EXATO por BANCO e c√≥digo num√©rico no in√≠cio da tabela
                        if banco_norm == key_banco_norm:
                            # üîß CORRE√á√ÉO: Tabela vem como "61700 - CLT NOVO GOLD PN-S", ent√£o verificar se come√ßa com "61700 " OU "61700-"
                            codigo_match = False
                            if codigo_norm.isdigit():
                                # Aceitar tanto "61700 -" quanto "61700 " quanto "61700-"
                                if (key_tabela_norm.startswith(codigo_norm + ' ') or 
                                    key_tabela_norm.startswith(codigo_norm + '-') or
                                    key_tabela_norm == codigo_norm):
                                    codigo_match = True
                                    
                            # Log TODOS os testes de c√≥digo para 61700
                            if codigo_norm == '61700':
                                logging.info(f"üéØ FACTA92 Testando 61700: key_tabela_norm='{key_tabela_norm}', codigo_match={codigo_match}")
                            
                            if codigo_match:
                                # Encontrou! Usar esse mapeamento mas SEM sobrescrever o c√≥digo
                                mapping_result = {
                                    'orgao_storm': details.get('orgao_storm', ''),
                                    'operacao_storm': details.get('operacao_storm', ''),
                                    'taxa_storm': details.get('taxa_storm', '0,00%')
                                }
                                logging.info(f"‚úÖ FACTA92: Encontrou mapeamento para c√≥digo {codigo_direto}: ORGAO={mapping_result['orgao_storm']}, OPERACAO={mapping_result['operacao_storm']}, TAXA={mapping_result['taxa_storm']}")
                                break
                
                # Log resumo da busca
                logging.info(f"üîç FACTA92 Resumo busca c√≥digo {codigo_norm}: {banco_matches} chaves com BANCO match, mapping_result={'ENCONTRADO' if mapping_result else 'N√ÉO ENCONTRADO'}")
            
            if not mapping_result:
                logging.warning(f"‚ö†Ô∏è FACTA92 PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: Mapeamento N√ÉO encontrado para c√≥digo {codigo_direto}")
                logging.warning(f"‚ö†Ô∏è FACTA92 Debug: banco_norm='{banco_norm}', codigo_norm='{codigo_norm}', TABELA_MAPPING tem {len(TABELA_MAPPING)} entradas")
        else:
            banco_para_mapeamento = normalized_row.get("BANCO", "")
            orgao_para_mapeamento = normalized_row.get("ORGAO", "")
            operacao_para_mapeamento = normalized_row.get("TIPO_OPERACAO", "")
            tabela_para_mapeamento = normalized_row.get("CODIGO_TABELA", "")
            
            logging.info(f"üìä PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: Aplicando mapeamento com BANCO={banco_para_mapeamento}, ORGAO={orgao_para_mapeamento}, OPERACAO={operacao_para_mapeamento}, TABELA={tabela_para_mapeamento}")
            
            mapping_result = apply_mapping(
                banco_para_mapeamento,
                orgao_para_mapeamento,
                operacao_para_mapeamento,
                "",  # usuario vazio - n√£o mais usado para evitar problemas futuros
                tabela_para_mapeamento
            )
        
        # SEMPRE usar dados do relat_orgaos.csv (formato Storm) quando dispon√≠vel
        # Os dados do banco s√£o apenas para valores financeiros
        
        # Log ANTES do mapeamento
        logging.info(f"üìã ANTES do mapeamento - PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: ORGAO={normalized_row.get('ORGAO', '')}, CODIGO_TABELA={normalized_row.get('CODIGO_TABELA', '')}, TAXA={normalized_row.get('TAXA', '')}, OPERACAO={normalized_row.get('TIPO_OPERACAO', '')}")
        
        # Se encontrou mapeamento, aplicar TODOS os campos do Storm
        if mapping_result:
            # 1. ORG√ÉO padronizado (Storm)
            if mapping_result.get('orgao_storm'):
                normalized_row["ORGAO"] = mapping_result.get('orgao_storm', '')
            
            # 2. CODIGO TABELA (Storm) - SEMPRE substituir se encontrou mapeamento
            if mapping_result.get('codigo_tabela'):
                codigo_anterior = normalized_row.get("CODIGO_TABELA", "")
                codigo_novo = mapping_result.get('codigo_tabela', '')
                normalized_row["CODIGO_TABELA"] = codigo_novo
                
                # Log espec√≠fico para VCTEX
                if bank_type == "VCTEX":
                    print(f"üîÑüî• VCTEX APLICANDO MAPEAMENTO: '{codigo_anterior}' ‚Üí '{codigo_novo}'")
                    logging.warning(f"üîÑüî• VCTEX APLICANDO MAPEAMENTO: '{codigo_anterior}' ‚Üí '{codigo_novo}'")
                    print(f"üî• VCTEX CODIGO_TABELA FINAL: '{normalized_row.get('CODIGO_TABELA', '')}'")
                    logging.warning(f"üî• VCTEX CODIGO_TABELA FINAL: '{normalized_row.get('CODIGO_TABELA', '')}'")
            elif bank_type == "VCTEX":
                # Se √© VCTEX mas n√£o tem mapping_result, manter original e avisar
                print(f"‚ö†Ô∏èüî• VCTEX SEM MAPEAMENTO: mantendo '{normalized_row.get('CODIGO_TABELA', '')}'")
                logging.warning(f"‚ö†Ô∏èüî• VCTEX SEM MAPEAMENTO: mantendo '{normalized_row.get('CODIGO_TABELA', '')}')")
            
            # 3. TAXA (Storm) - SEMPRE substituir se encontrou mapeamento
            if mapping_result.get('taxa_storm'):
                taxa_mapeada = mapping_result.get('taxa_storm', "0,00%")
                # Garantir que tem formato percentual
                if taxa_mapeada and '%' not in taxa_mapeada:
                    taxa_mapeada = taxa_mapeada + '%'
                normalized_row["TAXA"] = taxa_mapeada if taxa_mapeada else "0,00%"
            
            # 4. TIPO DE OPERACAO (Storm) - SEMPRE substituir se encontrou mapeamento
            if mapping_result.get('operacao_storm'):
                normalized_row["TIPO_OPERACAO"] = mapping_result.get('operacao_storm', "")
        else:
            # Se N√ÉO encontrou mapeamento, manter valores do banco mas garantir que TAXA existe
            logging.warning(f"‚ö†Ô∏è PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: Mapeamento N√ÉO encontrado! Usando valores do banco")
            # üîß CORRE√á√ÉO FACTA92: SEMPRE definir TAXA como 0,00% se n√£o encontrou mapeamento
            if bank_type == "FACTA92":
                normalized_row["TAXA"] = "0,00%"
                logging.info(f"‚úÖ FACTA92: Sem mapeamento, TAXA definida como 0,00%")
            elif not normalized_row.get("TAXA") or normalized_row.get("TAXA") == "":
                normalized_row["TAXA"] = "0,00%"
            elif '%' not in normalized_row.get("TAXA", ""):
                normalized_row["TAXA"] = normalized_row.get("TAXA") + '%'
        
        # VALIDA√á√ÉO FINAL: Garantir que TAXA e CODIGO_TABELA nunca fiquem vazios
        if not normalized_row.get("TAXA") or normalized_row.get("TAXA").strip() == "":
            normalized_row["TAXA"] = "0,00%"
            logging.warning(f"‚ö†Ô∏è PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: TAXA vazia, definida como 0,00%")
        
        if not normalized_row.get("CODIGO_TABELA") or normalized_row.get("CODIGO_TABELA").strip() == "":
            normalized_row["CODIGO_TABELA"] = "SEM_CODIGO"
            logging.warning(f"‚ö†Ô∏è PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: CODIGO_TABELA vazio, definido como SEM_CODIGO")
        
        # üîç PRESERVAR DATAS ORIGINAIS - n√£o deixar o mapeamento alterar
        data_cadastro_original = normalized_row.get('DATA_CADASTRO', '')
        data_pagamento_original = normalized_row.get('DATA_PAGAMENTO', '')
        
        # Log DEPOIS do mapeamento
        logging.info(f"üìó DEPOIS do mapeamento - PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: ORGAO={normalized_row.get('ORGAO', '')}, CODIGO_TABELA={normalized_row.get('CODIGO_TABELA', '')}, TAXA={normalized_row.get('TAXA', '')}, OPERACAO={normalized_row.get('TIPO_OPERACAO', '')}")
        
        # ‚úÖ GARANTIR que as datas originais sejam mantidas
        if data_cadastro_original:
            normalized_row['DATA_CADASTRO'] = data_cadastro_original
        if data_pagamento_original: 
            normalized_row['DATA_PAGAMENTO'] = data_pagamento_original
            
        logging.info(f"üìÖ DATAS FINAIS PRESERVADAS - PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: CADASTRO='{normalized_row.get('DATA_CADASTRO')}' | PAGAMENTO='{normalized_row.get('DATA_PAGAMENTO')}'")

        
        # VALIDA√á√ÉO MELHORADA: S√≥ adicionar se tiver dados essenciais v√°lidos
        proposta = str(normalized_row.get("PROPOSTA", "")).strip()
        nome = str(normalized_row.get("NOME", "")).strip()
        cpf = str(normalized_row.get("CPF", "")).strip()
        
        logging.info(f"üîç VALIDA√á√ÉO FINAL - Proposta: '{proposta}', Nome: '{nome[:30] if nome else 'N/A'}', CPF: '{cpf}'")
        
        # Palavras-chave que indicam linha inv√°lida (EXATAS, n√£o substring)
        invalid_exact_keywords = [
            "nan", "none", "null", "unnamed", "relat√≥rio", "relatorio",
            "total", "p√°gina", "pagina"
        ]
        
        # Palavras que s√≥ s√£o inv√°lidas se forem a palavra COMPLETA
        invalid_whole_words = [
            "proposta", "nome", "cliente", "cpf", "banco", "c√≥digo", "codigo", "data"
        ]
        
        # Verificar se proposta √© v√°lida
        proposta_lower = proposta.lower()
        
        # Verifica√ß√£o melhorada: n√£o rejeitar se √© uma proposta real que cont√©m essas palavras
        has_invalid_exact = any(keyword == proposta_lower for keyword in invalid_exact_keywords)
        has_invalid_whole = any(proposta_lower == keyword for keyword in invalid_whole_words)
        
        # Valida√ß√£o mais leniente para diagn√≥stico
        is_valid_proposta = (
            proposta and 
            proposta.strip() not in ["", "nan", "None", "NULL", "NaN"] and
            not has_invalid_exact and
            not has_invalid_whole and
            len(proposta.strip()) >= 1  # Qualquer proposta com pelo menos 1 caractere
        )
        
        logging.info(f"   is_valid_proposta={is_valid_proposta} (has_invalid_exact={has_invalid_exact}, has_invalid_whole={has_invalid_whole})")
        
        # Verificar se tem pelo menos nome OU cpf v√°lido
        nome_lower = nome.lower() if nome else ""
        nome_is_invalid = any(nome_lower == keyword for keyword in invalid_whole_words + invalid_exact_keywords)
        
        # Relaxar valida√ß√£o de CPF para aceitar formatados (XXX.XXX.XXX-XX)
        cpf_clean = ''.join(filter(str.isdigit, cpf)) if cpf else ""
        cpf_valid = len(cpf_clean) >= 11 or (cpf and len(cpf) >= 11)
        
        # VALIDA√á√ÉO ESPEC√çFICA PARA DAYCOVAL - Mais flex√≠vel s√≥ para este banco
        if bank_type == "DAYCOVAL":
            # DAYCOVAL: Se tem proposta num√©rica, √© praticamente sempre v√°lido
            has_numeric_proposta = proposta and any(c.isdigit() for c in proposta)
            has_valid_data = (
                (nome and len(nome) > 2 and not nome_is_invalid) or  # Nome com 3+ chars
                (cpf_clean and len(cpf_clean) >= 8) or  # CPF mais flex√≠vel para DAYCOVAL
                has_numeric_proposta or  # Proposta com n√∫meros
                (proposta and len(proposta) >= 5)  # Qualquer proposta longa o suficiente
            )
        else:
            # VALIDA√á√ÉO NORMAL PARA OUTROS BANCOS (Storm, Santander, etc.)
            has_valid_data = (
                (nome and len(nome) > 3 and not nome_is_invalid) or
                cpf_valid or
                proposta  # Se tem proposta, j√° √© v√°lido
            )
        
        logging.info(f"   has_valid_data={has_valid_data} (nome_valid={nome and len(nome) > 3 and not nome_is_invalid}, cpf_valid={cpf_valid})")
        
        if is_valid_proposta and has_valid_data:
            normalized_data.append(normalized_row)
            logging.info(f"‚úÖ‚úÖ‚úÖ Linha ADICIONADA com sucesso: Proposta={proposta}, Nome={nome[:20] if nome else 'N/A'}, CPF={cpf[:6] if cpf else 'N/A'}...")
        else:
            logging.warning(f"‚ùå‚ùå‚ùå Linha IGNORADA [{bank_type}] - Proposta='{proposta}' (len={len(proposta)}), Nome='{nome[:20] if nome else 'N/A'}' (len={len(nome)}), CPF='{cpf}' (len={len(cpf)}), is_valid_proposta={is_valid_proposta}, has_valid_data={has_valid_data}")
            # Log detalhado para debug
            if not is_valid_proposta:
                logging.warning(f"  üîç Proposta inv√°lida: has_invalid_exact={has_invalid_exact}, has_invalid_whole={has_invalid_whole}")
            if not has_valid_data:
                logging.warning(f"  üîç Dados inv√°lidos: nome_valid={nome and len(nome) > 3 and not nome_is_invalid}, cpf_valid={cpf_valid}")
    
    logging.info(f"üìä [{bank_type}] RESUMO: {len(normalized_data)} registros v√°lidos de {len(df)} linhas processadas")
    
    # Log detalhado se n√£o temos dados
    if len(normalized_data) == 0:
        logging.error(f"‚ùå [{bank_type}] NENHUM dado v√°lido foi extra√≠do!")
        logging.error(f"   üìã Colunas do DataFrame: {list(df.columns)[:10]}...")
        if not df.empty:
            logging.error(f"   üìÑ Primeira linha: {dict(df.iloc[0])}") 
        return pd.DataFrame()
    
    # üßπ FILTRO DE SEGURAN√áA: Remover qualquer None que possa ter escapado
    normalized_data_clean = [row for row in normalized_data if row is not None and isinstance(row, dict)]
    
    if len(normalized_data_clean) != len(normalized_data):
        logging.warning(f"‚ö†Ô∏è [{bank_type}] Removidos {len(normalized_data) - len(normalized_data_clean)} registros None da lista")
    
    if len(normalized_data_clean) == 0:
        logging.error(f"‚ùå [{bank_type}] Ap√≥s filtrar None, nenhum dado restou!")
        return pd.DataFrame()
    
    return pd.DataFrame(normalized_data_clean)

def _get_daycoval_operation_type(table_description: str) -> str:
    """Determina o tipo de opera√ß√£o baseado na descri√ß√£o da tabela do Daycoval"""
    table_lower = table_description.lower()
    if 'rfn' in table_lower or 'refin' in table_lower:
        if 'portab' in table_lower:
            return "REFINANCIAMENTO DA PORTABILIDADE"
        else:
            return "REFINANCIAMENTO"
    elif 'portab' in table_lower:
        return "PORTABILIDADE + REFIN"
    else:
        return "MARGEM LIVRE (NOVO)"

def map_to_final_format(df: pd.DataFrame, bank_type: str) -> tuple[pd.DataFrame, int]:
    """Mapear dados para o formato final de 24 colunas com estat√≠sticas de mapeamento"""
    try:
        # Debug espec√≠fico para PAULISTA
        if bank_type == "PAULISTA":
            logging.info(f"üè¶ map_to_final_format: PAULISTA com {len(df)} linhas")
            logging.info(f"üè¶ Primeiras 3 linhas do DF:")
            for i, (idx, row) in enumerate(df.head(3).iterrows()):
                logging.info(f"   Linha {idx}: Unnamed:0='{row.get('Unnamed: 0', 'N/A')}'")
        
        # Primeiro normalizar os dados
        normalized_df = normalize_bank_data(df, bank_type)
        
        if normalized_df.empty:
            logging.warning(f"Dados normalizados vazios para banco {bank_type}")
            return pd.DataFrame(), 0
        
        # Remover duplicatas espec√≠ficas para QUERO MAIS (propostas duplicadas)
        if bank_type == "QUERO_MAIS":
            original_count = len(normalized_df)
            # Remover duplicatas baseado na PROPOSTA (campo √∫nico) mantendo o primeiro registro
            normalized_df = normalized_df.drop_duplicates(subset=['PROPOSTA'], keep='first')
            removed_count = original_count - len(normalized_df)
            if removed_count > 0:
                logging.info(f"üßπ QUERO MAIS: Removidas {removed_count} propostas duplicadas ({original_count} ‚Üí {len(normalized_df)})")
        
        final_data = []
        mapped_count = 0
        
        for _, row in normalized_df.iterrows():
            situacao = row.get("SITUACAO", "")
            data_pagamento = row.get("DATA_PAGAMENTO", "")
            
            # DATA DE PAGAMENTO s√≥ deve ter valor se STATUS for exatamente PAGO (ap√≥s normaliza√ß√£o)
            if situacao.upper() != "PAGO":
                data_pagamento = ""
            
            # üåé APLICAR FORMATA√á√ÉO GLOBAL BRASILEIRA (CPF + Valores Monet√°rios)
            cpf_raw = row.get("CPF", "")
            cpf_formatted = format_cpf_global(cpf_raw)
            
            valor_parcelas_raw = row.get("VALOR_PARCELAS", "")
            valor_parcelas_formatted = format_value_brazilian(valor_parcelas_raw)
            
            valor_operacao_raw = row.get("VALOR_OPERACAO", "")
            valor_operacao_formatted = format_value_brazilian(valor_operacao_raw)
            
            valor_liberado_raw = row.get("VALOR_LIBERADO", "")
            valor_liberado_formatted = format_value_brazilian(valor_liberado_raw)
            
            final_row = {
                "PROPOSTA": row.get("PROPOSTA", ""),
                "DATA CADASTRO": row.get("DATA_CADASTRO", ""),
                "BANCO": row.get("BANCO", ""),
                "ORGAO": row.get("ORGAO", ""),
                "CODIGO TABELA": row.get("CODIGO_TABELA", ""),
                "TIPO DE OPERACAO": row.get("TIPO_OPERACAO", ""),
                "NUMERO PARCELAS": row.get("NUMERO_PARCELAS", ""),
                "VALOR PARCELAS": valor_parcelas_formatted,  # ‚úÖ Formatado em padr√£o brasileiro
                "VALOR OPERACAO": valor_operacao_formatted,  # ‚úÖ Formatado em padr√£o brasileiro
                "VALOR LIBERADO": valor_liberado_formatted,  # ‚úÖ Formatado em padr√£o brasileiro
                "VALOR QUITAR": "",
                "USUARIO BANCO": row.get("USUARIO_BANCO", ""),
                "CODIGO LOJA": "",
                "SITUACAO": situacao,
                "DATA DE PAGAMENTO": data_pagamento,
                "CPF": cpf_formatted,  # ‚úÖ Formatado em padr√£o brasileiro (XXX.XXX.XXX-XX)
                "NOME": row.get("NOME", ""),
                "DATA DE NASCIMENTO": row.get("DATA_NASCIMENTO", ""),
                "TIPO DE CONTA": "",
                "TIPO DE PAGAMENTO": "",
                "AGENCIA CLIENTE": "",
                "CONTA CLIENTE": "",
                "FORMALIZACAO DIGITAL": "SIM",
                "TAXA": row.get("TAXA", ""),  # TAXA j√° vem do relat_orgaos.csv (aplicada em normalize_bank_data)
                "OBSERVACOES": row.get("OBSERVACOES", "")  # Campo de observa√ß√µes (principalmente VCTEX)
            }
            
            # Contar se foi mapeado o c√≥digo de tabela
            if final_row["CODIGO TABELA"]:
                mapped_count += 1
            
            # Limpar valores 'nan'
            for key, value in final_row.items():
                if str(value).lower() in ['nan', 'none', 'null', '']:
                    final_row[key] = ""
            
            final_data.append(final_row)
        
        result_df = pd.DataFrame(final_data)
        logging.info(f"Mapeamento conclu√≠do para {bank_type}: {len(result_df)} registros, {mapped_count} mapeados")
        return result_df, mapped_count
        
    except Exception as e:
        logging.error(f"Erro no mapeamento para {bank_type}: {str(e)}")
        return pd.DataFrame(), 0

def remove_duplicates_enhanced(df: pd.DataFrame, storm_data: Dict[str, str]) -> pd.DataFrame:
    """Remo√ß√£o aprimorada de duplicatas baseada na Storm"""
    if df.empty:
        return df
    
    filtered_data = []
    removed_count = 0
    
    for _, row in df.iterrows():
        proposta = str(row.get('PROPOSTA', '')).strip()
        
        if not proposta or proposta.lower() in ['nan', 'null', '']:
            continue
        
        # Verificar se proposta existe na Storm
        skip_record = False
        for storm_proposta, storm_status in storm_data.items():
            if proposta == storm_proposta or proposta in storm_proposta or storm_proposta in proposta:
                if storm_status in ["PAGO", "CANCELADO"]:
                    skip_record = True
                    removed_count += 1
                    break
        
        if not skip_record:
            filtered_data.append(row.to_dict())
    
    logging.info(f"Duplicatas removidas: {removed_count}")
    return pd.DataFrame(filtered_data)

def format_csv_for_storm(df: pd.DataFrame) -> str:
    """Formatar CSV otimizado para importa√ß√£o na Storm com separador ';'"""
    if df.empty:
        return ""
    
    # Garantir que todas as colunas est√£o presentes na ordem correta
    required_columns = [
        "PROPOSTA", "DATA CADASTRO", "BANCO", "ORGAO", "CODIGO TABELA",
        "TIPO DE OPERACAO", "NUMERO PARCELAS", "VALOR PARCELAS", "VALOR OPERACAO",
        "VALOR LIBERADO", "VALOR QUITAR", "USUARIO BANCO", "CODIGO LOJA",
        "SITUACAO", "DATA DE PAGAMENTO", "CPF", "NOME", "DATA DE NASCIMENTO",
        "TELEFONE", "ENDERECO", "BAIRRO", "CEP", "UF",
        "TIPO DE CONTA", "TIPO DE PAGAMENTO", "AGENCIA CLIENTE", "CONTA CLIENTE",
        "FORMALIZACAO DIGITAL", "TAXA", "OBSERVACOES"
    ]
    
    # Reordenar colunas
    df_ordered = df.reindex(columns=required_columns, fill_value="")
    
    # üßπ LIMPEZA ROBUSTA: Aplicar limpeza de caracteres especiais no relat√≥rio final
    logging.info(f"üßπ Aplicando limpeza de caracteres especiais no relat√≥rio final ({len(df_ordered)} linhas)")
    
    for col in df_ordered.columns:
        df_ordered[col] = df_ordered[col].astype(str).apply(clean_special_characters)
        df_ordered[col] = df_ordered[col].replace(['nan', 'None', 'null', 'NaN'], '')
    
    logging.info(f"‚úÖ Limpeza de caracteres especiais conclu√≠da no relat√≥rio final")
    
    # üîß FIX: Corrigir formata√ß√£o do CPF digitador (USUARIO BANCO) no relat√≥rio final
    if "USUARIO BANCO" in df_ordered.columns:
        def format_cpf_usuario_banco(cpf_str):
            """Manter formato original dos bancos que j√° v√™m corretos
            QUERO MAIS, C6, PAULISTA, DIGIO, etc: C√≥digos como '12345678901_202902' MANT√âM com underscore
            Apenas formatar como CPF se for CPF puro sem c√≥digo
            """
            if not cpf_str or cpf_str in ['', '0', '000.000.000-00']:
                return '000.000.000-00'
            
            cpf_clean = str(cpf_str).strip()
            
            # üéØ Se cont√©m underscore, √© formato banco correto - MANTER COMO EST√Å
            if '_' in cpf_clean:
                # Ex: "12345678901_202902" ‚Üí "12345678901_202902" (manter original)
                return cpf_clean
            
            # üéØ Se tem mais de 14 d√≠gitos, provavelmente √© c√≥digo longo - manter como est√°  
            cpf_digits = ''.join(filter(str.isdigit, cpf_clean))
            if len(cpf_digits) > 14:
                return cpf_clean  # Manter c√≥digos longos originais
            
            # üéØ CPF normal (sem c√≥digo): formatar no padr√£o brasileiro
            if len(cpf_digits) == 11:
                return f"{cpf_digits[:3]}.{cpf_digits[3:6]}.{cpf_digits[6:9]}-{cpf_digits[9:11]}"
            else:
                # Menos de 11 d√≠gitos ou formato especial, manter original
                return cpf_clean
        
        df_ordered["USUARIO BANCO"] = df_ordered["USUARIO BANCO"].apply(format_cpf_usuario_banco)
    
    # Formatar datas para DD/MM/YYYY (padr√£o brasileiro)
    date_columns = ["DATA CADASTRO", "DATA DE PAGAMENTO", "DATA DE NASCIMENTO"]
    for date_col in date_columns:
        if date_col in df_ordered.columns:
            df_ordered[date_col] = df_ordered[date_col].apply(lambda x: format_date_to_brazilian(x))
    
    # Usar separador ';' como solicitado
    return df_ordered.to_csv(index=False, sep=';', encoding='utf-8', lineterminator='\n')

def format_date_to_brazilian(date_str: str) -> str:
    """Converte data para formato DD/MM/YYYY"""
    if not date_str or date_str in ['', 'nan', 'None', 'null']:
        return ""
    
    date_str = str(date_str).strip()
    
    # J√° est√° no formato DD/MM/YYYY
    if '/' in date_str and len(date_str.split('/')) == 3:
        parts = date_str.split('/')
        if len(parts[0]) == 2 and len(parts[2]) == 4:  # DD/MM/YYYY
            return date_str
    
    # Tentar converter de outros formatos comuns
    try:
        # Formato YYYY-MM-DD
        if '-' in date_str:
            date_obj = pd.to_datetime(date_str, format='%Y-%m-%d', errors='coerce')
            if pd.notna(date_obj):
                return date_obj.strftime('%d/%m/%Y')
        
        # Formato DD/MM/YY (ano com 2 d√≠gitos)
        if '/' in date_str:
            parts = date_str.split('/')
            if len(parts) == 3 and len(parts[2]) == 2:
                day, month, year = parts
                year = '20' + year if int(year) < 50 else '19' + year
                return f"{day.zfill(2)}/{month.zfill(2)}/{year}"
        
        # Tentar parsing autom√°tico do pandas
        date_obj = pd.to_datetime(date_str, errors='coerce', dayfirst=True)
        if pd.notna(date_obj):
            return date_obj.strftime('%d/%m/%Y')
    except:
        pass
    
    return date_str  # Retorna original se n√£o conseguir converter

@api_router.post("/upload-storm")
async def upload_storm_report(file: UploadFile = File(...)):
    """Upload e processamento do relat√≥rio da Storm"""
    try:
        if not file.filename:
            raise HTTPException(status_code=400, detail="Nome do arquivo √© obrigat√≥rio")
        
        content = await file.read()
        if len(content) == 0:
            raise HTTPException(status_code=400, detail="Arquivo est√° vazio")
        
        df = read_file_optimized(content, file.filename)
        
        # Detectar tipo de banco
        bank_type = detect_bank_type_enhanced(df, file.filename)
        if bank_type != "STORM":
            raise HTTPException(status_code=400, detail="Este n√£o √© um arquivo da Storm v√°lido")
        
        # Processar dados da Storm
        storm_proposals = process_storm_data_enhanced(df)
        
        # Armazenar globalmente
        global storm_data_global
        storm_data_global = storm_proposals
        
        return {
            "message": "Arquivo da Storm processado com sucesso",
            "total_proposals": len(storm_proposals),
            "paid_cancelled": sum(1 for status in storm_proposals.values() if status in ["PAGO", "CANCELADO"]),
            "filename": file.filename
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Erro ao processar Storm: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@api_router.post("/process-banks")
async def process_bank_reports(files: List[UploadFile] = File(...)):
    """Processamento aprimorado de m√∫ltiplos relat√≥rios de bancos"""
    try:
        if not storm_data_global:
            raise HTTPException(status_code=400, detail="Primeiro fa√ßa upload do relat√≥rio da Storm")
        
        if not files or len(files) == 0:
            raise HTTPException(status_code=400, detail="Nenhum arquivo de banco foi enviado")
        
        job_id = str(uuid.uuid4())
        job = ProcessingJob(id=job_id, total_records=0, processed_records=0)
        processing_jobs[job_id] = job
        
        all_final_data = []
        bank_summaries = []
        
        for file in files:
            try:
                if not file.filename:
                    continue
                
                content = await file.read()
                if len(content) == 0:
                    continue
                
                logging.info(f"Processando arquivo: {file.filename}")
                
                # Ler arquivo com tratamento de erros melhorado
                try:
                    df = read_file_optimized(content, file.filename)
                except Exception as read_error:
                    logging.error(f"‚ùå Erro ao ler arquivo {file.filename}: {str(read_error)}")
                    continue
                
                # Validar DataFrame
                if df is None or df.empty:
                    logging.warning(f"‚ö†Ô∏è Arquivo {file.filename} resultou em DataFrame vazio")
                    continue
                
                # Limpar DataFrame - remover linhas completamente vazias
                df = df.dropna(how='all')
                
                if df.empty:
                    logging.warning(f"‚ö†Ô∏è Arquivo {file.filename} n√£o cont√©m dados v√°lidos ap√≥s limpeza")
                    continue
                
                # Detectar tipo de banco
                try:
                    bank_type = detect_bank_type_enhanced(df, file.filename)
                except Exception as detect_error:
                    logging.error(f"‚ùå Erro ao detectar banco em {file.filename}: {str(detect_error)}")
                    continue
                
                if bank_type == "STORM":
                    continue  # Pular arquivos da Storm
                
                logging.info(f"‚úÖ Banco detectado: {bank_type}, Registros originais: {len(df)}, Colunas: {len(df.columns)}")
                
                # DEBUG: Log adicional para PAULISTA
                if 'AF5EEBB7' in file.filename or 'paulista' in file.filename.lower():
                    logging.error(f"üîç DEBUG PAULISTA: Arquivo={file.filename}, Banco detectado={bank_type}")
                    logging.error(f"üîç DEBUG PAULISTA: Primeiras colunas: {list(df.columns)[:10]}")
                    if not df.empty:
                        first_row = df.iloc[0].to_dict()
                        logging.error(f"üîç DEBUG PAULISTA: Primeira linha: {first_row}")
                
                # Mapear para formato final
                if bank_type == "PAULISTA":
                    logging.error(f"üè¶ PAULISTA: Chamando map_to_final_format com {len(df)} linhas")
                
                mapped_df, mapped_count = map_to_final_format(df, bank_type)
                
                if mapped_df.empty:
                    logging.warning(f"Nenhum dado mapeado para {file.filename}")
                    continue
                
                # Remover duplicatas baseado na Storm
                original_count = len(mapped_df)
                filtered_df = remove_duplicates_enhanced(mapped_df, storm_data_global)
                duplicates_removed = original_count - len(filtered_df)
                
                logging.info(f"Ap√≥s remo√ß√£o de duplicatas: {len(filtered_df)} registros")
                
                if not filtered_df.empty:
                    all_final_data.append(filtered_df)
                
                # Criar resumo
                status_dist = {}
                if not filtered_df.empty and "SITUACAO" in filtered_df.columns:
                    status_dist = filtered_df["SITUACAO"].value_counts().to_dict()
                
                bank_summaries.append(ReportSummary(
                    bank_name=bank_type,
                    total_records=len(filtered_df),
                    duplicates_removed=duplicates_removed,
                    status_distribution=status_dist,
                    mapped_records=mapped_count,
                    unmapped_records=original_count - mapped_count
                ))
                
            except Exception as e:
                logging.error(f"Erro ao processar arquivo {file.filename}: {str(e)}")
                continue
        
        # Combinar todos os dados
        if not all_final_data:
            logging.error("üö´ ERRO CR√çTICO: Nenhum DataFrame v√°lido foi processado!")
            for i, file in enumerate(files):
                logging.error(f"   üìÇ Arquivo {i+1}: {file.filename}")
            raise HTTPException(status_code=400, detail="Nenhum dado v√°lido foi processado. Verifique se os arquivos t√™m o formato correto e cont√™m dados v√°lidos.")
        
        final_df = pd.concat(all_final_data, ignore_index=True)
        
        # üßπ LIMPEZA FINAL: Garantir que n√£o h√° caracteres especiais no relat√≥rio final
        logging.info(f"üßπ Aplicando limpeza final de caracteres especiais no relat√≥rio combinado ({len(final_df)} registros)")
        for col in final_df.columns:
            if final_df[col].dtype == 'object':
                final_df[col] = final_df[col].astype(str).apply(clean_special_characters)
        logging.info(f"‚úÖ Limpeza final conclu√≠da - relat√≥rio pronto para Storm")
        
        # **FORMATA√á√ÉO OTIMIZADA PARA STORM COM SEPARADOR ';'**
        csv_content = format_csv_for_storm(final_df)
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8')
        temp_file.write(csv_content)
        temp_file.close()
        
        # Atualizar job
        job.status = "completed"
        job.completed_at = datetime.utcnow()
        job.message = f"Processamento conclu√≠do: {len(final_df)} registros"
        job.total_records = len(final_df)
        job.result_file = temp_file.name
        
        return {
            "job_id": job_id,
            "message": "Processamento conclu√≠do com sucesso",
            "total_records": len(final_df),
            "bank_summaries": [summary.dict() for summary in bank_summaries],
            "download_url": f"/api/download-result/{job_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Erro no processamento: {str(e)}")
        if 'job' in locals():
            job.status = "failed"
            job.message = str(e)
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@api_router.get("/download-result/{job_id}")
async def download_result(job_id: str):
    """Download do resultado processado"""
    if job_id not in processing_jobs:
        raise HTTPException(status_code=404, detail="Job n√£o encontrado")
    
    job = processing_jobs[job_id]
    if job.status != "completed":
        raise HTTPException(status_code=400, detail="Processamento ainda n√£o conclu√≠do")
    
    if not hasattr(job, 'result_file') or not os.path.exists(job.result_file):
        raise HTTPException(status_code=404, detail="Arquivo de resultado n√£o encontrado")
    
    return FileResponse(
        path=job.result_file,
        filename=f"relatorio_final_storm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        media_type='text/csv'
    )

@api_router.get("/processing-status/{job_id}")
async def get_processing_status(job_id: str):
    """Status do processamento"""
    if job_id not in processing_jobs:
        raise HTTPException(status_code=404, detail="Job n√£o encontrado")
    
    job = processing_jobs[job_id]
    return job.dict()

@api_router.get("/")
async def root():
    return {"message": "Sistema de Processamento de Relat√≥rios Financeiros - V6.6.0 Melhorias Completas DIGIO, VCTEX e AVERBAI"}

@api_router.post("/reload-mapping")
async def reload_mapping():
    """Endpoint para recarregar o mapeamento de √≥rg√£os quando novos c√≥digos s√£o adicionados"""
    try:
        success = reload_organ_mapping()
        if success:
            return {
                "message": "‚úÖ Mapeamento recarregado com sucesso!",
                "total_bancos": len(ORGAN_MAPPING),
                "total_combinacoes": len(DETAILED_MAPPING),
                "total_tabelas": len(TABELA_MAPPING),
                "total_fallback": len(BANK_ORGAN_MAPPING)
            }
        else:
            raise HTTPException(status_code=500, detail="Erro ao recarregar mapeamento")
    except Exception as e:
        logging.error(f"Erro no reload: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@api_router.get("/averbai-codes")
async def get_averbai_codes():
    """Endpoint para listar todos os c√≥digos AVERBAI reconhecidos pelo sistema"""
    try:
        averbai_codes = {
            "FGTS": [],
            "INSS": [],
            "CREDITO_DO_TRABALHADOR": []
        }
        
        # Buscar todos os c√≥digos AVERBAI no mapeamento
        for key, details in TABELA_MAPPING.items():
            parts = key.split('|')
            if len(parts) >= 2 and parts[0] == "AVERBAI":
                orgao = parts[1]
                codigo = details.get('codigo_tabela', '')
                tabela = details.get('tabela_banco', '')
                taxa = details.get('taxa_storm', '')
                
                code_info = {
                    "codigo": codigo,
                    "tabela": tabela,
                    "taxa": taxa,
                    "operacao": details.get('operacao_storm', '')
                }
                
                if orgao == "FGTS":
                    averbai_codes["FGTS"].append(code_info)
                elif orgao == "INSS":
                    averbai_codes["INSS"].append(code_info)
                elif orgao == "CR√âDITO DO TRABALHADOR":
                    averbai_codes["CREDITO_DO_TRABALHADOR"].append(code_info)
        
        # Ordenar por c√≥digo
        for orgao in averbai_codes:
            averbai_codes[orgao] = sorted(averbai_codes[orgao], key=lambda x: int(x["codigo"]) if x["codigo"].isdigit() else 9999)
        
        return {
            "message": "C√≥digos AVERBAI reconhecidos pelo sistema",
            "total_fgts": len(averbai_codes["FGTS"]),
            "total_inss": len(averbai_codes["INSS"]), 
            "total_clt": len(averbai_codes["CREDITO_DO_TRABALHADOR"]),
            "codes": averbai_codes
        }
        
    except Exception as e:
        logging.error(f"Erro ao listar c√≥digos AVERBAI: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@api_router.post("/debug-file")
async def debug_file(file: UploadFile = File(...)):
    """Endpoint de debug para testar leitura e detec√ß√£o de arquivos"""
    try:
        content = await file.read()
        
        # Tentar ler arquivo
        try:
            df = read_file_optimized(content, file.filename)
            
            debug_info = {
                "filename": file.filename,
                "file_size": len(content),
                "success": True,
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": list(df.columns)[:20],  # Primeiras 20 colunas
                "first_row_sample": {}
            }
            
            # Pegar primeira linha como exemplo
            if not df.empty:
                first_row = df.iloc[0]
                for col in list(df.columns)[:10]:  # Primeiras 10 colunas
                    debug_info["first_row_sample"][col] = str(first_row[col])[:100]
            
            # Tentar detectar banco
            try:
                bank_type = detect_bank_type_enhanced(df, file.filename)
                debug_info["detected_bank"] = bank_type
            except Exception as detect_error:
                debug_info["detected_bank"] = f"ERRO: {str(detect_error)}"
            
            return debug_info
            
        except Exception as read_error:
            return {
                "filename": file.filename,
                "file_size": len(content),
                "success": False,
                "error": str(read_error),
                "error_type": type(read_error).__name__
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erro no debug: {str(e)}")

# Include the router in the main app
app.include_router(api_router)

# Mount static files for frontend (comentado para Railway - frontend est√° no Azure)
# app.mount("/", StaticFiles(directory="static", html=True), name="static")

# Configure CORS with a safe default.
# If CORS_ORIGINS is set to '*' or empty, we allow all origins but disable credentials
# because Starlette does not allow allow_credentials=True with '*' origin.
cors_origins_env = os.environ.get('CORS_ORIGINS', '').strip()
if not cors_origins_env or cors_origins_env == '*':
    # Allow all origins (no credentials)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=False,
        allow_methods=["*"],
        allow_headers=["*"],
    )
else:
    # Parse a comma-separated list of allowed origins and enable credentials
    origins_list = [o.strip() for o in cors_origins_env.split(',') if o.strip()]
    app.add_middleware(
        CORSMiddleware,
        allow_credentials=True,
        allow_origins=origins_list,
        allow_methods=["*"],
        allow_headers=["*"],
    )

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@app.on_event("shutdown")
async def shutdown_db_client():
    client.close()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
