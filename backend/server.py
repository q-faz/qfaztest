from fastapi import FastAPI, APIRouter, File, UploadFile, HTTPException
from fastapi.responses import FileResponse
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime
import pandas as pd
import numpy as np
import tempfile
import json
import io
import re

ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# MongoDB connection
mongo_url = os.environ['MONGO_URL']
client = AsyncIOMotorClient(mongo_url)
db = client[os.environ['DB_NAME']]

# Create the main app without a prefix
app = FastAPI()

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")

# Models
class ProcessingJob(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    status: str = "processing"  # processing, completed, failed
    message: str = ""
    processed_records: int = 0
    total_records: int = 0
    created_at: datetime = Field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None
    result_file: Optional[str] = None

class ReportSummary(BaseModel):
    bank_name: str
    total_records: int
    duplicates_removed: int
    status_distribution: Dict[str, int]
    mapped_records: int = 0
    unmapped_records: int = 0

# ===== MAPEAMENTOS COMPLETOS MELHORADOS =====

# Status normalization - Mapeamento COMPLETO
STATUS_MAPPING = {
    # ===== PAGO variants ===== (proposta finalizada e paga)
    "pago": "PAGO", 
    "paga": "PAGO", 
    "integrada": "PAGO", 
    "integrado": "PAGO",
    "finalizado": "PAGO", 
    "finalizada": "PAGO", 
    "finalizada / paga": "PAGO",
    "emitido": "PAGO", 
    "quitado": "PAGO", 
    "quitada": "PAGO", 
    "concluido": "PAGO", 
    "concluÃ­da": "PAGO", 
    "liberado": "PAGO", 
    "liberada": "PAGO",
    "desembolsado": "PAGO",
    "desembolsada": "PAGO",
    "valor desembolsado para a conta do cliente operaÃ§Ã£o concluÃ­da.": "PAGO",
    "proposta integrada": "PAGO", 
    "int": "PAGO",
    "prova de vida aprovada / aguardando pagamento": "PAGO",
    "aprovado": "PAGO",
    "aprovada": "PAGO",
    "pago ao cliente": "PAGO",
    "credito liberado": "PAGO",
    
    # ===== CANCELADO variants ===== (proposta cancelada/reprovada)
    "cancelado": "CANCELADO", 
    "cancelada": "CANCELADO", 
    "reprovado": "CANCELADO", 
    "reprovada": "CANCELADO", 
    "rejeitado": "CANCELADO", 
    "rejeitada": "CANCELADO",
    "negado": "CANCELADO", 
    "negada": "CANCELADO", 
    "recusado": "CANCELADO",
    "recusada": "CANCELADO",
    "expirado": "CANCELADO", 
    "expirada": "CANCELADO", 
    "invÃ¡lido": "CANCELADO", 
    "invÃ¡lida": "CANCELADO",
    "invalido": "CANCELADO",
    "invalida": "CANCELADO",
    "cancelado devido a proposta expirada.": "CANCELADO",
    "cancelado permanentemente": "CANCELADO",
    "operaÃ§Ã£o cancelada para desaverbaÃ§Ã£o.": "CANCELADO",
    "reprovada - mesa de formalizaÃ§Ã£o": "CANCELADO",
    "reprovada - mesa de formalizacao": "CANCELADO",
    "proposta expirada": "CANCELADO",
    "rep": "CANCELADO", 
    "can": "CANCELADO",
    "reprovada por averbadora": "CANCELADO",
    "reprovada pelo banco": "CANCELADO",
    "nao aprovado": "CANCELADO",
    "nÃ£o aprovado": "CANCELADO",
    "desistencia": "CANCELADO",
    "desistÃªncia": "CANCELADO",
    "cliente desistiu": "CANCELADO",
    
    # ===== AGUARDANDO variants ===== (proposta em processamento/pendente)
    "digitada / aguardando formalizaÃ§Ã£o": "AGUARDANDO",
    "digitada / aguardando formalizaÃ‡Ãƒo": "AGUARDANDO",
    "digitada / aguardando formalizacao": "AGUARDANDO",
    "emitido/aguardando averbaÃ§Ã£o": "AGUARDANDO",
    "emitido/aguardando averbacao": "AGUARDANDO",
    "emitido/aguardando averbaÃ£Â§Ã£Â£o": "AGUARDANDO",
    "criada / aguardando link de formalizaÃ§Ã£o": "AGUARDANDO",
    "criada / aguardando link de formalizacao": "AGUARDANDO",
    "aguardando saldo cip": "AGUARDANDO",
    "aguardando portabilidade": "AGUARDANDO",
    "aguardando prova de vida / assinatura": "AGUARDANDO",
    "aguardando prova de vida": "AGUARDANDO",
    "aguardando assinatura": "AGUARDANDO",
    "ajustar troco": "AGUARDANDO",
    "andamento": "AGUARDANDO", 
    "and": "AGUARDANDO", 
    "em andamento": "AGUARDANDO",
    "pendente": "AGUARDANDO", 
    "pendencia": "AGUARDANDO",
    "pendÃªncia": "AGUARDANDO",
    "aguardando pagamento": "AGUARDANDO",
    "aguardando": "AGUARDANDO",
    "pendÃªncia autorizaÃ§Ã£o": "AGUARDANDO",
    "pendencia autorizacao": "AGUARDANDO",
    "pendÃªncia / documentaÃ§Ã£o": "AGUARDANDO",
    "pendencia / documentacao": "AGUARDANDO",
    "pendente documentacao": "AGUARDANDO",
    "pendente documentaÃ§Ã£o": "AGUARDANDO",
    "link de prova de vida e coleta de assinatura enviado para o cliente.": "AGUARDANDO",
    "pendÃªncia de autorizaÃ§Ã£o da qitech para pagamento.": "AGUARDANDO",
    "pendente formalizacao": "AGUARDANDO",
    "pendente formalizaÃ§Ã£o": "AGUARDANDO",
    "aguardar aumento margem": "AGUARDANDO",
    "digitada": "AGUARDANDO",
    "em aberto": "AGUARDANDO",
    "aberto": "AGUARDANDO",
    "aberta": "AGUARDANDO",
    "formalizaÃ§Ã£o": "AGUARDANDO",
    "formalizacao": "AGUARDANDO",
    "checagem - mesa formalizaÃ§Ã£o": "AGUARDANDO",
    "checagem - mesa formalizacao": "AGUARDANDO",
    "aguarda form dig web": "AGUARDANDO",
    "analise corban": "AGUARDANDO",
    "anÃ¡lise corban": "AGUARDANDO",
    "em analise": "AGUARDANDO",
    "em anÃ¡lise": "AGUARDANDO",
    "analise": "AGUARDANDO",
    "anÃ¡lise": "AGUARDANDO",
    "processando": "AGUARDANDO",
    "em processamento": "AGUARDANDO",
    "cadastrada": "AGUARDANDO",
    "cadastrado": "AGUARDANDO",
    "nova": "AGUARDANDO",
    "novo": "AGUARDANDO",
    "enviado": "AGUARDANDO",
    "enviada": "AGUARDANDO",
    "aguardando averbacao": "AGUARDANDO",
    "aguardando averbaÃ§Ã£o": "AGUARDANDO"
}

# Tipos de operaÃ§Ã£o padronizados
OPERATION_TYPES = {
    "MARGEM LIVRE (NOVO)": "MARGEM LIVRE (NOVO)",
    "MARGEM LIVRE": "MARGEM LIVRE (NOVO)", 
    "margem livre (novo)": "MARGEM LIVRE (NOVO)",
    "PORTABILIDADE": "PORTABILIDADE",
    "PORTABILIDADE + REFIN": "PORTABILIDADE + REFIN",
    "REFINANCIAMENTO": "REFINANCIAMENTO",
    "REFINANCIAMENTO DA PORTABILIDADE": "REFINANCIAMENTO DA PORTABILIDADE",
    "EMPRÃ‰STIMO COMPLEMENTAR": "EMPRÃ‰STIMO COMPLEMENTAR",
    "Saque FGTS": "MARGEM LIVRE (NOVO)",
    "Consignado FGTS": "MARGEM LIVRE (NOVO)",
    "Consignado INSS": "MARGEM LIVRE (NOVO)",
    "Portabilidade + Refin": "PORTABILIDADE + REFIN",
    "Refinanciamento": "REFINANCIAMENTO",
    "CartÃ£o c/ saque": "CARTÃƒO C/ SAQUE",
    "CartÃ£o c/ saque complementar Ã  vista": "CARTÃƒO C/ SAQUE COMPLEMENTAR Ã€ VISTA"
}

# Global storage for processing state
processing_jobs = {}
storm_data_global = {}

# ðŸŒ FUNÃ‡Ã•ES GLOBAIS DE FORMATAÃ‡ÃƒO (aplicadas a TODOS os bancos)

def format_cpf_global(cpf_str):
    """Formata CPF para o padrÃ£o brasileiro: 000.000.000-00"""
    if not cpf_str:
        return ""
    
    # Remover tudo que nÃ£o Ã© nÃºmero
    cpf_numbers = ''.join(filter(str.isdigit, str(cpf_str)))
    
    # Verificar se tem 11 dÃ­gitos
    if len(cpf_numbers) != 11:
        # Se nÃ£o tem 11 dÃ­gitos, retornar original
        return str(cpf_str).strip()
    
    # Formatar: 000.000.000-00
    cpf_formatted = f"{cpf_numbers[0:3]}.{cpf_numbers[3:6]}.{cpf_numbers[6:9]}-{cpf_numbers[9:11]}"
    return cpf_formatted

def format_value_brazilian(value_str):
    """Formata valores monetÃ¡rios para o padrÃ£o brasileiro: 1.255,00"""
    if not value_str or str(value_str).strip() in ['', 'nan', 'None', 'null', 'NaN']:
        return "0,00"
    
    try:
        # Limpar o valor (remover espaÃ§os, moeda, etc.)
        clean_value = str(value_str).strip().replace('R$', '').replace(' ', '').replace('\xa0', '')
        
        # Se estÃ¡ vazio apÃ³s limpeza
        if not clean_value or clean_value == '0':
            return "0,00"
        
        # Se jÃ¡ estÃ¡ no formato brasileiro correto (X.XXX,XX), manter
        if ',' in clean_value:
            parts = clean_value.split(',')
            if len(parts) == 2 and len(parts[1]) == 2:
                # Verificar se parte inteira tem pontos como separador de milhar
                if '.' in parts[0] or parts[0].isdigit():
                    return clean_value
        
        # Remover pontos que sÃ£o separadores de milhar no formato brasileiro
        # mas manter o Ãºltimo ponto se for decimal
        if ',' not in clean_value and '.' in clean_value:
            # Formato americano: 1234.56 ou 1,234.56
            clean_value = clean_value.replace(',', '')  # Remove vÃ­rgulas (separador de milhar americano)
            parts = clean_value.split('.')
            integer_part = parts[0]
            decimal_part = parts[1][:2] if len(parts) > 1 else "00"
        elif ',' in clean_value:
            # Formato brasileiro: 1.234,56 ou jÃ¡ estÃ¡ com vÃ­rgula decimal
            parts = clean_value.replace('.', '').split(',')  # Remove pontos, split por vÃ­rgula
            integer_part = parts[0]
            decimal_part = parts[1][:2] if len(parts) > 1 else "00"
        else:
            # Sem decimal, assumir valor inteiro
            integer_part = clean_value.replace('.', '').replace(',', '')
            decimal_part = "00"
        
        # Garantir que decimal tenha 2 dÃ­gitos
        if len(decimal_part) == 1:
            decimal_part += "0"
        elif len(decimal_part) == 0:
            decimal_part = "00"
        
        # Converter para float
        float_value = float(f"{integer_part}.{decimal_part}")
        
        # Formatar no padrÃ£o brasileiro: 1.255,00
        if float_value >= 1000:
            # Valores >= 1000: usar ponto para milhar
            formatted = f"{float_value:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
        else:
            # Valores < 1000: apenas vÃ­rgula decimal
            formatted = f"{float_value:.2f}".replace('.', ',')
        
        return formatted
        
    except (ValueError, TypeError) as e:
        logging.warning(f"âš ï¸ Erro ao formatar valor '{value_str}': {e}")
        return str(value_str).strip()  # Retornar original se houver erro

def format_percentage_brazilian(percentage_str):
    """Formata percentuais para o padrÃ£o brasileiro: 1,85%"""
    if not percentage_str or str(percentage_str).strip() in ['', 'nan', 'None', 'null', 'NaN']:
        return "0,00%"
    
    try:
        # Limpar o valor (remover %, espaÃ§os, etc.)
        clean_value = str(percentage_str).strip().replace('%', '').replace(' ', '')
        
        if not clean_value or clean_value == '0':
            return "0,00%"
        
        # Converter para float
        percentage_float = float(clean_value.replace(',', '.'))
        
        # Formatar no padrÃ£o brasileiro: X,XX%
        formatted = f"{percentage_float:.2f}".replace('.', ',')
        return f"{formatted}%"
        
    except (ValueError, TypeError) as e:
        logging.warning(f"âš ï¸ Erro ao formatar percentual '{percentage_str}': {e}")
        return f"{str(percentage_str).strip()}%"

def load_organ_mapping():
    """Carrega o mapeamento de Ã³rgÃ£os do arquivo CSV atualizado - MELHORADO sem dependÃªncia de usuÃ¡rio"""
    try:
        # Ler o arquivo de mapeamento atualizado usando caminho relativo
        csv_path = os.path.join(os.path.dirname(__file__), 'relat_orgaos.csv')
        # Tentar diferentes encodings
        try:
            df = pd.read_csv(csv_path, encoding='utf-8', sep=';')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(csv_path, encoding='latin-1', sep=';')
            except:
                df = pd.read_csv(csv_path, encoding='iso-8859-1', sep=';')
        
        # Formato REAL do arquivo: BANCO;ORGÃƒO STORM;TABELA BANCO;CODIGO TABELA STORM;OPERAÃ‡ÃƒO STORM;TAXA STORM
        # NOTA: Campo USUARIO DIGITADOR STORM foi removido para evitar problemas futuros com mudanÃ§as de usuÃ¡rio
        mapping = {}
        detailed_mapping = {}  # Mapeamento por BANCO|ORGÃƒO|OPERAÃ‡ÃƒO (mÃºltiplas opÃ§Ãµes)
        tabela_mapping = {}     # Mapeamento por BANCO|ORGÃƒO|OPERAÃ‡ÃƒO|TABELA (mais especÃ­fico)
        bank_organ_mapping = {} # Mapeamento por BANCO|ORGÃƒO (mais genÃ©rico para fallback)
        
        for _, row in df.iterrows():
            banco = ' '.join(str(row.get('BANCO', '')).strip().upper().split())
            # FIX ENCODING: Usar nomes de colunas como o pandas lÃª (com caracteres corrompidos)
            orgao = ' '.join(str(row.get('ORGï¿½O STORM', '') or row.get('Ã“RGÃƒO STORM', '')).strip().upper().split())
            # CRÃTICO: Normalizar tabela removendo TODOS os espaÃ§os extras (incluindo espaÃ§os iniciais)
            tabela_banco_raw = str(row.get('TABELA BANCO', '')).strip()
            tabela_banco = ' '.join(tabela_banco_raw.split())  # Remove espaÃ§os extras completamente
            codigo_tabela = str(row.get('CODIGO TABELA STORM', '')).strip()
            operacao_storm = str(row.get('OPERAï¿½ï¿½O STORM', '') or row.get('OPERAÃ‡ÃƒO STORM', '')).strip()
            taxa_storm = str(row.get('TAXA STORM', '')).strip()
            
            if banco and banco != 'NAN' and codigo_tabela and codigo_tabela != 'NAN':
                # Mapeamento simples (primeira ocorrÃªncia por hierarquia)
                if banco not in mapping:
                    mapping[banco] = {}
                if orgao and orgao != 'NAN':
                    if orgao not in mapping[banco]:
                        mapping[banco][orgao] = {}
                    if operacao_storm and operacao_storm != 'NAN':
                        if operacao_storm not in mapping[banco][orgao]:
                            mapping[banco][orgao][operacao_storm] = codigo_tabela
                
                # Mapeamento detalhado por BANCO|ORGÃƒO|OPERAÃ‡ÃƒO (guarda todas as opÃ§Ãµes)
                key = f"{banco}|{orgao}|{operacao_storm}"
                if key not in detailed_mapping:
                    detailed_mapping[key] = []
                detailed_mapping[key].append({
                    'codigo_tabela': codigo_tabela,
                    'tabela_banco': tabela_banco,
                    'orgao_storm': orgao,  # ORGÃƒO padronizado Storm
                    'operacao_storm': operacao_storm,
                    'taxa_storm': taxa_storm
                })
                
                # Mapeamento por TABELA (mais especÃ­fico e confiÃ¡vel)
                if tabela_banco and tabela_banco != 'NAN':
                    # CRÃTICO: Salvar a chave com tabela em UPPERCASE para matching consistente
                    tabela_key = f"{banco}|{orgao}|{operacao_storm}|{tabela_banco.upper()}"
                    tabela_mapping[tabela_key] = {
                        'codigo_tabela': codigo_tabela,
                        'tabela_banco': tabela_banco,  # Manter original para exibiÃ§Ã£o
                        'orgao_storm': orgao,  # ORGÃƒO padronizado Storm
                        'operacao_storm': operacao_storm,
                        'taxa_storm': taxa_storm
                    }
                
                # Mapeamento genÃ©rico por BANCO|ORGÃƒO (para fallback quando operaÃ§Ã£o nÃ£o bate exatamente)
                bank_organ_key = f"{banco}|{orgao}"
                if bank_organ_key not in bank_organ_mapping:
                    bank_organ_mapping[bank_organ_key] = []
                bank_organ_mapping[bank_organ_key].append({
                    'codigo_tabela': codigo_tabela,
                    'tabela_banco': tabela_banco,
                    'orgao_storm': orgao,
                    'operacao_storm': operacao_storm,
                    'taxa_storm': taxa_storm
                })
        
        logging.info(f"Mapeamento carregado: {len(mapping)} bancos, {len(detailed_mapping)} combinaÃ§Ãµes banco+orgao+operacao, {len(tabela_mapping)} por tabela especÃ­fica, {len(bank_organ_mapping)} por banco+orgao")
        return mapping, detailed_mapping, tabela_mapping, bank_organ_mapping
    except Exception as e:
        logging.error(f"Erro ao carregar mapeamento de Ã³rgÃ£os: {str(e)}")
        return {}, {}, {}, {}

# Carregar mapeamento global - ATUALIZADO sem dependÃªncia de usuÃ¡rio
ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING = load_organ_mapping()

def reload_organ_mapping():
    """Recarrega o mapeamento de Ã³rgÃ£os para pegar novos cÃ³digos de tabela adicionados"""
    global ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING
    try:
        logging.info("ðŸ”„ Recarregando mapeamento de Ã³rgÃ£os...")
        ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING = load_organ_mapping()
        logging.info("âœ… Mapeamento recarregado com sucesso!")
        return True
    except Exception as e:
        logging.error(f"âŒ Erro ao recarregar mapeamento: {str(e)}")
        return False

def read_file_optimized(file_content: bytes, filename: str) -> pd.DataFrame:
    """Leitura otimizada de arquivos com mÃºltiplas tentativas e melhor detecÃ§Ã£o de separadores"""
    file_ext = filename.lower().split('.')[-1]
    
    try:
        if file_ext == 'csv':
            # Tentar diferentes encodings e separadores
            for encoding in ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']:
                for sep in [';', ',', '\t', '|']:
                    try:
                        df = pd.read_csv(
                            io.BytesIO(file_content), 
                            encoding=encoding, 
                            sep=sep,
                            low_memory=False,
                            na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                            dtype=str  # Manter tudo como string inicialmente
                        )
                        
                        # Verificar se temos mÃºltiplas colunas ou se precisa dividir
                        if len(df.columns) == 1 and sep != ';':
                            # Tentar dividir a Ãºnica coluna por diferentes separadores
                            first_col = df.columns[0]
                            if ';' in first_col or ',' in first_col or '\t' in first_col:
                                continue  # Tentar prÃ³ximo separador
                        
                        if len(df.columns) > 1 or (len(df.columns) == 1 and len(df) > 0):
                            logging.info(f"Arquivo lido com encoding {encoding} e separador '{sep}', {len(df.columns)} colunas")
                            return df
                            
                    except (UnicodeDecodeError, pd.errors.ParserError, Exception) as e:
                        continue
            
            # Se chegou aqui, tentar Ãºltimo recurso: detectar automaticamente o separador
            try:
                content_str = file_content.decode('utf-8', errors='ignore')
                # Verificar qual separador aparece mais nas primeiras linhas
                first_lines = content_str.split('\n')[:5]
                separators = {';': 0, ',': 0, '\t': 0, '|': 0}
                
                for line in first_lines:
                    for sep in separators:
                        separators[sep] += line.count(sep)
                
                best_sep = max(separators, key=separators.get)
                if separators[best_sep] > 0:
                    df = pd.read_csv(
                        io.BytesIO(file_content), 
                        encoding='utf-8', 
                        sep=best_sep,
                        low_memory=False,
                        na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                        dtype=str,
                        on_bad_lines='skip'
                    )
                    logging.info(f"Arquivo lido com separador auto-detectado '{best_sep}', {len(df.columns)} colunas")
                    return df
            except Exception as e:
                logging.error(f"Erro na detecÃ§Ã£o automÃ¡tica: {str(e)}")
            
            raise ValueError("NÃ£o foi possÃ­vel ler o arquivo CSV com nenhum separador")
            
        elif file_ext in ['xlsx', 'xls']:
            # Verificar se Ã© arquivo PAULISTA - precisa pular primeiras linhas
            filename_lower = filename.lower()
            is_paulista = any(indicator in filename_lower for indicator in ['paulista', 'af5eebb7'])
            
            logging.info(f"ðŸ” Arquivo: {filename_lower}, Ã‰ PAULISTA? {is_paulista}")
            
            if is_paulista:
                logging.info(f"ðŸ¦ Detectado arquivo PAULISTA: {filename}, aplicando leitura especial...")
                try:
                    # PAULISTA: pular primeiras 2 linhas, usar linha 3 como cabeÃ§alho
                    df = pd.read_excel(
                        io.BytesIO(file_content),
                        skiprows=2,  # Pula logo e linha vazia
                        na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                        dtype=str
                    )
                    logging.info(f"ðŸ¦ PAULISTA lido com skip=2: {len(df.columns)} colunas, primeiras: {list(df.columns)[:5]}")
                    return df
                except Exception as e:
                    logging.error(f"âŒ Erro na leitura especial PAULISTA: {str(e)}")
                    # Fallback para leitura normal
            
            # Tentar ler com diferentes configuraÃ§Ãµes
            try:
                # Primeiro tentar leitura normal
                df = pd.read_excel(
                    io.BytesIO(file_content),
                    na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                    dtype=str
                )
                
                # Se o DataFrame estÃ¡ vazio ou tem sÃ³ NaN, tentar pular linhas
                if df.empty or df.dropna(how='all').empty:
                    raise ValueError("DataFrame vazio apÃ³s primeira tentativa")
                
                # PAULISTA DETECTION: Verificar se tem "RelaÃ§Ã£o de Propostas" nas primeiras linhas
                if not df.empty and len(df) > 0:
                    first_few_rows = df.head(3).astype(str)
                    content_text = ' '.join(first_few_rows.values.flatten()).lower()
                    
                    if 'relaÃ§Ã£o de propostas' in content_text or 'analÃ­tico' in content_text:
                        logging.info(f"ðŸ¦ PAULISTA detectado por conteÃºdo! Aplicando leitura especial...")
                        try:
                            # Recarregar pulando primeiras linhas
                            df_paulista = pd.read_excel(
                                io.BytesIO(file_content),
                                skiprows=2,  # Pula logo e "RelaÃ§Ã£o de Propostas"
                                na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                                dtype=str
                            )
                            logging.info(f"ðŸ¦ PAULISTA relido: {len(df_paulista.columns)} colunas: {list(df_paulista.columns)[:5]}")
                            return df_paulista
                        except Exception as e:
                            logging.error(f"âŒ Erro na releitura PAULISTA: {str(e)}")
                
                # Verificar se a primeira linha parece ser cabeÃ§alho de metadados
                # (ex: "RelatÃ³rio de...", "Banco:", etc.)
                if not df.empty:
                    first_row = df.iloc[0].astype(str).str.lower()
                    metadata_indicators = ['relatÃ³rio', 'relatorio', 'banco:', 'data:', 'perÃ­odo', 
                                          'periodo', 'extraÃ§Ã£o', 'extracao', 'total:', 'pÃ¡gina']
                    
                    # Se encontrar indicadores de metadados, tentar pular linhas
                    if any(indicator in ' '.join(first_row.values) for indicator in metadata_indicators):
                        logging.info("Detectado cabeÃ§alho de metadados, tentando pular linhas...")
                        
                        # Tentar pular de 1 a 10 linhas
                        for skip_rows in range(1, 11):
                            try:
                                df_attempt = pd.read_excel(
                                    io.BytesIO(file_content),
                                    skiprows=skip_rows,
                                    na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                                    dtype=str
                                )
                                
                                # Verificar se agora temos dados vÃ¡lidos
                                if not df_attempt.empty and len(df_attempt.columns) > 3:
                                    # Verificar se tem mais dados que a tentativa anterior
                                    valid_rows = df_attempt.dropna(how='all')
                                    if len(valid_rows) > 0:
                                        logging.info(f"Excel lido pulando {skip_rows} linhas, {len(df_attempt.columns)} colunas")
                                        return df_attempt
                            except:
                                continue
                
                # Se chegou aqui, usar o DataFrame original
                logging.info(f"Excel lido normalmente, {len(df.columns)} colunas")
                return df
                
            except Exception as e:
                # Ãšltima tentativa: ler todas as sheets e pegar a primeira com dados
                logging.warning(f"Tentativa normal falhou: {str(e)}, tentando ler todas as sheets...")
                try:
                    excel_file = pd.ExcelFile(io.BytesIO(file_content))
                    for sheet_name in excel_file.sheet_names:
                        try:
                            df = pd.read_excel(
                                io.BytesIO(file_content),
                                sheet_name=sheet_name,
                                na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                                dtype=str
                            )
                            if not df.empty and len(df.columns) > 1:
                                logging.info(f"Excel lido da sheet '{sheet_name}', {len(df.columns)} colunas")
                                return df
                        except:
                            continue
                except Exception as sheet_error:
                    logging.error(f"Erro ao ler sheets: {str(sheet_error)}")
                
                raise ValueError(f"NÃ£o foi possÃ­vel ler o arquivo Excel: {str(e)}")
        elif file_ext in ['txt']:
            # TXT pode ser CSV com separadores variados
            try:
                # Tentar como CSV primeiro
                content_str = file_content.decode('utf-8')
            except:
                try:
                    content_str = file_content.decode('latin-1')
                except:
                    content_str = file_content.decode('cp1252')
            
            # Detectar separador
            first_lines = content_str.split('\n')[:5]
            separators = {';': 0, ',': 0, '\t': 0, '|': 0}
            
            for line in first_lines:
                for sep in separators:
                    separators[sep] += line.count(sep)
            
            best_sep = max(separators, key=separators.get)
            if separators[best_sep] > 0:
                df = pd.read_csv(
                    io.BytesIO(file_content), 
                    encoding='utf-8' if file_content.decode('utf-8', errors='ignore') else 'latin-1',
                    sep=best_sep,
                    low_memory=False,
                    na_values=['', 'NaN', 'NULL', 'null', 'N/A', 'n/a'],
                    dtype=str,
                    on_bad_lines='skip'
                )
                logging.info(f"Arquivo TXT lido com separador '{best_sep}', {len(df.columns)} colunas")
                return df
            else:
                raise ValueError("Arquivo TXT sem separadores reconhecÃ­veis")
        else:
            raise ValueError(f"Formato nÃ£o suportado: {file_ext}. Formatos aceitos: CSV, XLSX, XLS, TXT")
            
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Erro ao ler arquivo {filename}: {str(e)}")

def detect_bank_type_enhanced(df: pd.DataFrame, filename: str) -> str:
    """DetecÃ§Ã£o melhorada de tipo de banco baseada na estrutura real dos arquivos"""
    filename_lower = filename.lower()
    df_columns = [str(col).lower().strip() for col in df.columns]
    
    logging.info(f"Detectando banco para arquivo: {filename}")
    logging.info(f"Colunas encontradas: {df_columns[:10]}...")  # Mostrar apenas primeiras 10
    
    # DetecÃ§Ã£o por nome do arquivo - mais confiÃ¡vel
    if any(indicator in filename_lower for indicator in ['storm', 'contratos', 'digitados']):
        return "STORM"
    elif 'averbai' in filename_lower:
        return "AVERBAI"
    elif 'digio' in filename_lower:
        return "DIGIO"
    elif 'prata' in filename_lower:
        return "PRATA"
    elif 'vctex' in filename_lower:
        return "VCTEX"
    elif 'daycoval' in filename_lower:
        return "DAYCOVAL"
    
    # DetecÃ§Ã£o por estrutura de colunas especÃ­fica
    
    # Verificar se Ã© Storm
    storm_indicators = ['ade', 'banco emprÃ©stimo', 'status do contrato']
    storm_matches = sum(1 for indicator in storm_indicators if any(indicator in col for col in df_columns))
    if storm_matches >= 2:
        return "STORM"
    
    # Verificar se Ã© AVERBAI (tem colunas especÃ­ficas como Id, IdTableComissao)
    averbai_indicators = ['id', 'idtablecomissao', 'tipoproduto', 'loginconsultor']
    averbai_matches = sum(1 for indicator in averbai_indicators if any(indicator in col for col in df_columns))
    if averbai_matches >= 2:
        return "AVERBAI"
    
    # Verificar se Ã© DIGIO (melhorada - mais especÃ­fica)
    if len(df.columns) > 50 and sum(1 for col in df_columns if 'unnamed:' in col) > 20:
        # Verificar dados especÃ­ficos do Digio em mÃºltiplas linhas
        if not df.empty:
            all_data = ""
            # Verificar primeiras 5 linhas para ser mais preciso
            for i in range(min(5, len(df))):
                row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_data += " " + row_data
                
            logging.info(f"ðŸ” DIGIO check - dados: {all_data[:200]}...")
            
            # Indicadores Ãºnicos do DIGIO (nÃ£o confundem com DAYCOVAL)
            digio_unique_indicators = ['banco digio', 'digio s.a', 'propostas cadastradas', 'tkt', 'status: ativo', 'status: cancelado', 'status: pago']
            found_digio_indicators = [ind for ind in digio_unique_indicators if ind in all_data]
            
            if found_digio_indicators:
                logging.info(f"âœ… DIGIO detectado! Indicadores Ãºnicos: {found_digio_indicators}")
                return "DIGIO"
                
            # Se nÃ£o tem indicadores Ãºnicos, verificar se NÃƒO Ã© DAYCOVAL
            # DAYCOVAL tem indicadores especÃ­ficos que DIGIO nÃ£o tem
            daycoval_exclusive_indicators = ['banco daycoval', 'qfz solucoes', 'tp. operaÃ§Ã£o']
            found_daycoval_indicators = [ind for ind in daycoval_exclusive_indicators if ind in all_data]
            
            if not found_daycoval_indicators:
                # NÃ£o Ã© DAYCOVAL, pode ser DIGIO se tem estrutura similar
                logging.info(f"ðŸ“Š DIGIO assumido por estrutura (sem indicadores DAYCOVAL)")
                return "DIGIO"
    
    # Verificar se Ã© PRATA (tem colunas especÃ­ficas)
    prata_indicators = ['corban master', 'nÃºmero da proposta', 'prata digital', 'shake de morango']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        prata_matches = sum(1 for indicator in prata_indicators if indicator in first_row_data)
        if prata_matches >= 1:
            return "PRATA"
    
    # Verificar se Ã© VCTEX (tem colunas especÃ­ficas)
    vctex_indicators = ['corban master', 'nÃºmero do contrato', "it's solucoes", 'tabela vamo']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        vctex_matches = sum(1 for indicator in vctex_indicators if indicator in first_row_data)
        if vctex_matches >= 1:
            return "VCTEX"
    
    # Verificar se Ã© DAYCOVAL (melhorada)
    # 1. Por nome do arquivo
    if 'daycoval' in filename_lower:
        logging.info(f"âœ… DAYCOVAL detectado por nome do arquivo: {filename}")
        return "DAYCOVAL"
    
    # 2. Por estrutura de colunas especÃ­ficas - FORMATO CSV CORRETO
    daycoval_csv_indicators = ['proposta', 'data cadastro', 'banco', 'orgao', 'codigo tabela', 'tipo de operacao', 'numero parcelas']
    daycoval_csv_matches = sum(1 for indicator in daycoval_csv_indicators if any(indicator in col for col in df_columns))
    if daycoval_csv_matches >= 5:
        logging.info(f"âœ… DAYCOVAL detectado por colunas CSV: {daycoval_csv_matches} matches")
        return "DAYCOVAL"
    
    # 3. Por estrutura de colunas antigas (Unnamed)
    daycoval_column_indicators = ['cliente', 'cpf/cnpj', 'matrÃ­cula', 'dt.cad.', 'dt.base', 'vlr.oper', 'prz. em meses', 'tx.am']
    daycoval_col_matches = sum(1 for indicator in daycoval_column_indicators if any(indicator in col for col in df_columns))
    if daycoval_col_matches >= 5:
        logging.info(f"âœ… DAYCOVAL detectado por colunas antigas: {daycoval_col_matches} matches")
        return "DAYCOVAL"
    
    # 3. Por estrutura Unnamed + conteÃºdo
    unnamed_count = sum(1 for col in df_columns if 'unnamed:' in col)
    logging.info(f"ðŸ” DAYCOVAL Check - Colunas: {len(df.columns)}, Unnamed: {unnamed_count}")
    
    if len(df.columns) > 20 and unnamed_count > 15:
        # Verificar dados especÃ­ficos do Daycoval em mÃºltiplas linhas
        if not df.empty:
            all_data = ""
            for i in range(min(5, len(df))):
                row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_data += " " + row_data
            
            logging.info(f"ðŸ” DAYCOVAL primeiras linhas: {all_data[:300]}")
            
            # Indicadores Ãºnicos do DAYCOVAL (nÃ£o confundem com DIGIO)
            daycoval_unique_indicators = ['banco daycoval', 'qfz solucoes', 'tp. operaÃ§Ã£o', 'detalhado']
            found_daycoval_indicators = [ind for ind in daycoval_unique_indicators if ind in all_data]
            
            # Verificar se NÃƒO tem indicadores do DIGIO
            digio_exclusive_indicators = ['banco digio', 'digio s.a', 'tkt', 'status: ativo', 'status: cancelado', 'status: pago']
            found_digio_indicators = [ind for ind in digio_exclusive_indicators if ind in all_data]
            
            if found_daycoval_indicators and not found_digio_indicators:
                logging.info(f"âœ… DAYCOVAL detectado! Indicadores Ãºnicos: {found_daycoval_indicators}")
                return "DAYCOVAL"
            else:
                logging.info(f"âš ï¸ DAYCOVAL nÃ£o detectado - indicadores DAYCOVAL: {found_daycoval_indicators}, indicadores DIGIO: {found_digio_indicators}")
    
    # DetecÃ§Ã£o por nome do arquivo SANTANDER (prioridade)
    if 'santander' in filename_lower:
        logging.info(f"âœ… SANTANDER detectado por nome do arquivo")
        return "SANTANDER"
    
    # Verificar se Ã© SANTANDER por colunas especÃ­ficas
    # Colunas reais do SANTANDER: COD, COD. BANCO, CPF, CLIENTE, CONVENIO, PRODUTO, STATUS, etc.
    santander_column_indicators = ['cod. banco', 'convenio', 'produto', 'qtde parcelas', 'valor bruto', 'valor liquido', 'cod digitador']
    santander_col_matches = sum(1 for indicator in santander_column_indicators if any(indicator in col for col in df_columns))
    
    if santander_col_matches >= 4:
        logging.info(f"âœ… SANTANDER detectado por colunas ({santander_col_matches} matches)")
        return "SANTANDER"
    
    # Verificar se tem "SANTANDER" nos dados (formato relatÃ³rio final)
    if not df.empty:
        banco_col = next((col for col in df.columns if 'banco' in str(col).lower()), None)
        if banco_col and any('SANTANDER' in str(val).upper() for val in df[banco_col].dropna().head(10)):
            logging.info(f"âœ… SANTANDER detectado por conteÃºdo da coluna BANCO")
            return "SANTANDER"
    
    # Verificar se Ã© CREFAZ (melhorada)
    # 1. Por nome do arquivo
    if 'crefaz' in filename_lower:
        return "CREFAZ"
    
    # 2. Por colunas especÃ­ficas do CREFAZ
    crefaz_column_indicators = ['data cadastro', 'nÃºmero da proposta', 'cpf', 'cliente', 'cidade', 'valor liberado', 'prazo', 'status', 'agente']
    crefaz_col_matches = sum(1 for indicator in crefaz_column_indicators if any(indicator in col for col in df_columns))
    if crefaz_col_matches >= 5:
        return "CREFAZ"
    
    # 3. Por conteÃºdo (indicadores de energia/boleto)
    crefaz_content_indicators = ['produto', 'conveniada', 'cpfl', 'cosern', 'celpe', 'enel', 'cod operaÃ§Ã£o', 'energia', 'boleto']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        crefaz_matches = sum(1 for indicator in crefaz_content_indicators if indicator in first_row_data)
        if crefaz_matches >= 2:
            return "CREFAZ"
    
    # Verificar se Ã© QUERO MAIS CREDITO (PRIORIDADE ALTA - antes do Paulista)
    # 1. Por nome do arquivo
    if 'quero' in filename_lower and 'mais' in filename_lower:
        logging.info("ðŸŽ¯ QUERO MAIS detectado por nome do arquivo")
        return "QUERO_MAIS"
    
    # 2. Por estrutura de colunas Unnamed especÃ­ficas
    if len(df.columns) > 40 and sum(1 for col in df_columns if 'unnamed:' in col) > 30:
        # Verificar indicadores especÃ­ficos do QUERO MAIS (ANTES do Paulista!)
        quero_mais_indicators = ['capital consig', 'quero mais credito', 'relatÃ³rio de produÃ§Ã£o', 'promotora', 'grupo qfz', 'cpf correspondente', 'convÃªnio correspondente']
        if not df.empty:
            # Verificar nas primeiras 5 linhas para maior precisÃ£o
            all_data = ""
            for i in range(min(5, len(df))):
                row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_data += " " + row_data
            
            logging.info(f"ðŸ” QUERO MAIS check - dados: {all_data[:200]}...")
            
            # Indicadores Ãºnicos do QUERO MAIS (nÃ£o confundem com PAULISTA)
            quero_mais_unique = ['capital consig', 'quero mais', 'promotora', 'grupo qfz', 'cpf correspondente']
            found_quero_indicators = [ind for ind in quero_mais_unique if ind in all_data]
            
            # Verificar se NÃƒO tem indicadores do PAULISTA
            paulista_exclusive = ['banco paulista', 'relaÃ§Ã£o de propostas', 'espÃ©cie benefÃ­cio', 'analÃ­tico']
            found_paulista_indicators = [ind for ind in paulista_exclusive if ind in all_data]
            
            if found_quero_indicators and not found_paulista_indicators:
                logging.info(f"âœ… QUERO MAIS detectado! Indicadores Ãºnicos: {found_quero_indicators}")
                return "QUERO_MAIS"
            else:
                logging.info(f"âš ï¸ QUERO MAIS nÃ£o detectado - indicadores QUERO: {found_quero_indicators}, PAULISTA: {found_paulista_indicators}")
    
    # Verificar se Ã© BANCO PAN (tem estrutura especÃ­fica de cartÃ£o)
    pan_indicators = ['nÂº proposta', 'nÂº operaÃ§Ã£o', 'tipo de operaÃ§Ã£o', 'cÃ³digo do produto', 'nome do produto']
    pan_matches = sum(1 for indicator in pan_indicators if any(indicator in col for col in df_columns))
    if pan_matches >= 3:
        return "PAN"
    
    # Verificar se Ã© C6 BANK (melhorada)
    # 1. Por nome do arquivo
    if 'c6' in filename_lower:
        return "C6"
    
    # 2. Por indicadores nas colunas
    c6_column_indicators = ['nome entidade', 'numero do contrato', 'proposta', 'data da operacao']
    c6_column_matches = sum(1 for indicator in c6_column_indicators if any(indicator in col for col in df_columns))
    if c6_column_matches >= 3:
        return "C6"
    
    # 3. Por conteÃºdo dos dados
    c6_indicators = ['c6 bank', 'c6 consignado', 'banco c6']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        if any(indicator in first_row_data for indicator in c6_indicators):
            return "C6"
    
    # Verificar se Ã© FACTA92 (melhorada)
    # 1. Por nome do arquivo
    if 'facta' in filename_lower or 'relatÃ³riovista' in filename_lower.replace(' ', ''):
        return "FACTA92"
    
    # 2. Por colunas especÃ­ficas do FACTA92
    facta_indicators = ['codigo', 'data_cadastro', 'data_registro', 'proposta', 'convenio', 'averbador', 'tipo_operacao', 'tipo_tabela']
    facta_matches = sum(1 for indicator in facta_indicators if any(indicator in col for col in df_columns))
    if facta_matches >= 4:
        return "FACTA92"
    
    # Verificar se Ã© PAULISTA (detecÃ§Ã£o melhorada)
    # 1. Por nome do arquivo
    if filename and 'paulista' in filename.lower():
        return "PAULISTA"
    
    # 2. Por colunas especÃ­ficas do Paulista
    paulista_column_indicators = ['nÂº proposta', 'contrato', 'data captura', 'cpf/cnpj proponente', 'nome do proponente', 'matrÃ­cula']
    paulista_col_matches = sum(1 for indicator in paulista_column_indicators if any(indicator in col for col in df_columns))
    if paulista_col_matches >= 4:
        return "PAULISTA"
    
    # 3. Por indicadores na primeira linha
    paulista_indicators = ['banco paulista', 'relaÃ§Ã£o de propostas', 'analÃ­tico', 'espÃ©cie benefÃ­cio']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        paulista_matches = sum(1 for indicator in paulista_indicators if indicator in first_row_data)
        if paulista_matches >= 2:
            return "PAULISTA"
    
    # 3. Por estrutura de colunas Unnamed especÃ­ficas do Paulista (MELHORADA)
    paulista_columns = ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 14', 'Unnamed: 18']
    paulista_col_matches = sum(1 for col in paulista_columns if col in df_columns)
    if paulista_col_matches >= 5 and len(df_columns) > 20:  # Paulista tem muitas colunas
        # Verificar se tem dados que parecem do Paulista em qualquer linha
        if not df.empty:
            # Procurar em todas as linhas por palavras-chave do Paulista
            all_data = ""
            for i in range(min(5, len(df))):  # Verificar atÃ© 5 primeiras linhas
                row_data = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_data += " " + row_data
            
            logging.info(f"ðŸ” PAULISTA check - dados: {all_data[:200]}...")
            
            # Indicadores Ãºnicos do PAULISTA (nÃ£o confundem com QUERO MAIS)
            paulista_unique_indicators = ['banco paulista', 'relaÃ§Ã£o de propostas', 'espÃ©cie benefÃ­cio', 'analÃ­tico']
            found_paulista_indicators = [ind for ind in paulista_unique_indicators if ind in all_data]
            
            # Verificar se NÃƒO tem indicadores do QUERO MAIS
            quero_mais_exclusive = ['capital consig', 'quero mais', 'promotora', 'grupo qfz', 'cpf correspondente']
            found_quero_indicators = [ind for ind in quero_mais_exclusive if ind in all_data]
            
            # PAULISTA sÃ³ se tem indicadores Ãºnicos E nÃ£o tem indicadores do QUERO MAIS
            if found_paulista_indicators and not found_quero_indicators:
                logging.info(f"âœ… PAULISTA detectado! Indicadores Ãºnicos: {found_paulista_indicators}")
                return "PAULISTA"
            elif found_paulista_indicators and found_quero_indicators:
                logging.warning(f"ðŸ”„ Conflito PAULISTA vs QUERO MAIS - priorizando QUERO MAIS: {found_quero_indicators}")
                return "QUERO_MAIS"  # Em caso de dÃºvida, priorizar QUERO MAIS
            else:
                logging.info(f"âš ï¸ PAULISTA nÃ£o detectado - indicadores PAULISTA: {found_paulista_indicators}, QUERO: {found_quero_indicators}")
                
            # Fallback: se tem estrutura Unnamed mas nÃ£o tem indicadores Ãºnicos claros
            generic_keywords = ['inss', 'aposentad', 'pensÃ£o', 'consignado', 'benefici', 'cpf', 'proposta']
            keyword_matches = sum(1 for word in generic_keywords if word in all_data)
            
            # SÃ³ usar fallback se nÃ£o conflitar com QUERO MAIS
            if keyword_matches >= 3 and not found_quero_indicators:
                logging.info(f"ðŸ“Š PAULISTA assumido por estrutura + keywords genÃ©ricos (sem conflito QUERO MAIS)")
                return "PAULISTA"
    
    # Verificar se Ã© TOTALCASH (tem estrutura especÃ­fica)
    totalcash_indicators = ['totalcash', 'total cash']
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        if any(indicator in first_row_data for indicator in totalcash_indicators):
            return "TOTALCASH"
    
    # Verificar se Ã© BRB (Banco de BrasÃ­lia)
    brb_indicators = ['id card', 'nome do cliente', 'benefÃ­cio', 'cpf do beneficiÃ¡rio', 'data da proposta', 'data da pagamento', 'nÂº contrato']
    brb_matches = sum(1 for indicator in brb_indicators if any(indicator in col for col in df_columns))
    if brb_matches >= 4:
        # Confirmar com dados
        if not df.empty:
            first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
            if 'brb' in first_row_data or 'banco de brasÃ­lia' in first_row_data or 'q-faz' in first_row_data:
                return "BRB"
    
    # Verificar se Ã© QUALIBANKING (melhorada)
    # 1. Por nome do arquivo
    if 'quali' in filename_lower or 'qualibanking' in filename_lower:
        return "QUALIBANKING"
    
    # 2. Por colunas especÃ­ficas
    qualibanking_indicators = ['cÃ³digo', 'tipo', 'etapa', 'nome do produto', 'nome da tabela', 'cÃ³digo da tabela', 'tipo de produto', 'tipo de operaÃ§Ã£o', 'data de cadastro', 'valor da operaÃ§Ã£o']
    qualibanking_matches = sum(1 for indicator in qualibanking_indicators if any(indicator in col for col in df_columns))
    if qualibanking_matches >= 5:
        return "QUALIBANKING"
    
    # 3. Por padrÃ£o do nÃºmero de contrato (QUA0000...)
    if not df.empty:
        for col in df.columns:
            if 'contrato' in str(col).lower() or 'cÃ³digo' in str(col).lower():
                try:
                    sample_val = str(df[col].iloc[0]).upper()
                    if sample_val.startswith('QUA'):
                        return "QUALIBANKING"
                except:
                    continue
    
    # 4. Por conteÃºdo dos dados
    if not df.empty:
        first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
        if 'qualibanking' in first_row_data or 'quali' in first_row_data:
            return "QUALIBANKING"
    
    # Verificar se Ã© MERCANTIL (Banco Mercantil do Brasil)
    mercantil_indicators = ['numeroproposta', 'codigoconvenio', 'nomeconvenio', 'codigoproduto', 'nomeproduto', 'modalidadecredito', 'situacaoproposta']
    mercantil_matches = sum(1 for indicator in mercantil_indicators if any(indicator in col for col in df_columns))
    if mercantil_matches >= 4:
        # Confirmar com dados
        if not df.empty:
            first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
            if 'mercantil' in first_row_data or 'credfranco' in first_row_data or 'qfz solucoes' in first_row_data:
                return "MERCANTIL"
    
    # Verificar se Ã© AMIGOZ
    amigoz_indicators = ['nr proposta', 'id banksoft', 'vulnerabilidade', 'aceite cliente vulneravel', 'grau de escolaridade', 'tipo de cartÃ£o']
    amigoz_matches = sum(1 for indicator in amigoz_indicators if any(indicator in col for col in df_columns))
    if amigoz_matches >= 3:
        # Confirmar com dados
        if not df.empty:
            first_row_data = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
            if 'amigoz' in first_row_data or 'cartÃ£o benefÃ­cio' in first_row_data or 'cartÃ£o consignado' in first_row_data:
                return "AMIGOZ"
    
    # DetecÃ§Ã£o adicional por conteÃºdo dos dados
    if not df.empty:
        sample_data = str(df.iloc[0]).lower() if len(df) > 0 else ""
        
        if 'averbai' in sample_data or 'fixo 30' in sample_data:
            return "AVERBAI"
        elif 'digio' in sample_data or 'tkt' in sample_data:
            return "DIGIO"
        elif 'prata digital' in sample_data or 'shake de morango' in sample_data:
            return "PRATA"
        elif 'vctex' in sample_data or "it's solucoes" in sample_data:
            return "VCTEX"
        elif 'daycoval' in sample_data:
            return "DAYCOVAL"
    
    # DetecÃ§Ã£o por padrÃµes de colunas
    if any('proposta' in col for col in df_columns):
        # Se tem muitas colunas unnamed, pode ser Digio ou Daycoval
        if sum(1 for col in df_columns if 'unnamed:' in col) > 20:
            # Distinguir entre DIGIO e DAYCOVAL pela primeira linha
            if not df.empty:
                first_row_content = ' '.join([str(val).lower() for val in df.iloc[0].values if pd.notna(val)])
                if 'daycoval' in first_row_content or 'nr.prop' in first_row_content or 'tp. operaÃ§Ã£o' in first_row_content:
                    return "DAYCOVAL"
                else:
                    return "DIGIO"
            return "DIGIO"  # Default se nÃ£o conseguir distinguir
        elif sum(1 for col in df_columns if 'unnamed:' in col) > 10:
            return "DAYCOVAL"
        # Se tem ID, provavelmente Ã© Averbai
        elif any('id' in col for col in df_columns):
            return "AVERBAI"
        # Se tem "nÃºmero da proposta", Ã© Prata
        elif any('nÃºmero da proposta' in col for col in df_columns):
            return "PRATA"
        # Se tem "nÃºmero do contrato", Ã© VCTEX
        elif any('nÃºmero do contrato' in col for col in df_columns):
            return "VCTEX"
    
    # Se nÃ£o conseguiu detectar, tentar por estrutura geral
    logging.warning(f"NÃ£o foi possÃ­vel detectar automaticamente o tipo de banco para: {filename}")
    logging.warning(f"NÃºmero de colunas: {len(df.columns)}")
    logging.warning(f"Colunas Unnamed: {sum(1 for col in df_columns if 'unnamed:' in col)}")
    
    # Ãšltima tentativa: anÃ¡lise de conteÃºdo mais especÃ­fica
    if not df.empty and len(df.columns) > 1:
        # Verificar conteÃºdo da primeira linha nÃ£o vazia
        for idx, row in df.iterrows():
            row_content = ' '.join([str(val).lower() for val in row.values if pd.notna(val) and str(val).strip()])
            if row_content and len(row_content) > 10:  # Linha com conteÃºdo substantivo
                if 'banco digio' in row_content:
                    return "DIGIO"
                elif 'banco daycoval' in row_content:
                    return "DAYCOVAL"
                elif 'banco paulista' in row_content or 'paulista' in row_content:
                    return "PAULISTA"
                elif 'prata digital' in row_content:
                    return "PRATA"
                elif "it's solucoes" in row_content:
                    return "VCTEX"
                elif 'averbai' in row_content or 'saque fgts' in row_content:
                    return "AVERBAI"
                break
        
        # Tentativa final para Paulista: estrutura Unnamed + palavras-chave
        if len(df_columns) > 20 and sum(1 for col in df_columns if 'unnamed:' in col) > 15:
            # Procurar palavras-chave do Paulista em qualquer parte do DataFrame
            all_text = ""
            for i in range(min(5, len(df))):
                row_text = ' '.join([str(val).lower() for val in df.iloc[i].values if pd.notna(val)])
                all_text += " " + row_text
            
            paulista_keywords = ['inss', 'aposentad', 'pensÃ£o', 'consignado', 'benefici', 'cpf', 'proposta', 'contrato']
            keyword_matches = sum(1 for word in paulista_keywords if word in all_text)
            
            if keyword_matches >= 2:
                return "PAULISTA"
    
    raise HTTPException(status_code=400, detail=f"Tipo de banco nÃ£o reconhecido para: {filename}. Estrutura: {len(df.columns)} colunas, {sum(1 for col in df_columns if 'unnamed:' in col)} colunas 'Unnamed'. Primeiras colunas: {df_columns[:5]}")

def process_storm_data_enhanced(df: pd.DataFrame) -> Dict[str, str]:
    """Processamento aprimorado dos dados da Storm"""
    storm_proposals = {}
    
    logging.info(f"Processando Storm com colunas: {list(df.columns)}")
    
    for _, row in df.iterrows():
        proposta = ""
        status = ""
        
        # Tentar diferentes formatos de colunas (case-insensitive)
        for idx, col in enumerate(df.columns):
            col_lower = str(col).lower()
            
            # Procurar coluna de proposta/ADE
            if any(term in col_lower for term in ['ade', 'proposta']) and not proposta:
                proposta = str(row.iloc[idx]).strip()
                
            # Procurar coluna de status
            if any(term in col_lower for term in ['status', 'situacao']) and not status:
                status = str(row.iloc[idx]).strip().lower()
        
        # Fallback: usar posiÃ§Ãµes fixas se nÃ£o encontrou pelos nomes
        if not proposta and len(df.columns) >= 1:
            proposta = str(row.iloc[0]).strip()
        if not status and len(df.columns) >= 3:
            status = str(row.iloc[2]).strip().lower()
        elif not status and len(df.columns) >= 2:
            status = str(row.iloc[1]).strip().lower()
        
        # Validar se proposta Ã© numÃ©rica (ADE vÃ¡lido)
        if proposta and proposta != 'nan' and proposta not in ['ADE', 'ade', 'PROPOSTA']:
            # Limpar proposta (remover caracteres nÃ£o numÃ©ricos exceto dÃ­gitos)
            proposta_clean = ''.join(c for c in proposta if c.isdigit())
            if proposta_clean and len(proposta_clean) >= 4:  # ADE deve ter pelo menos 4 dÃ­gitos
                normalized_status = STATUS_MAPPING.get(status, status.upper() if status else "AGUARDANDO")
                storm_proposals[proposta_clean] = normalized_status
                logging.info(f"Proposta processada: {proposta_clean} -> {normalized_status}")
    
    logging.info(f"Storm processada: {len(storm_proposals)} propostas")
    return storm_proposals

def normalize_operation_for_matching(operation: str) -> str:
    """Normaliza operaÃ§Ã£o para comparaÃ§Ã£o flexÃ­vel (remove case sensitivity e preposiÃ§Ãµes)"""
    if not operation:
        return ""
    
    # Normalizar bÃ¡sico
    normalized = ' '.join(operation.strip().split())
    
    # Converter para lowercase para comparaÃ§Ã£o
    normalized_lower = normalized.lower()
    
    # Mapear variaÃ§Ãµes conhecidas para forma canÃ´nica
    operation_mappings = {
        'refinanciamento da portabilidade': 'REFINANCIAMENTO DA PORTABILIDADE',
        'refinanciamento de portabilidade': 'REFINANCIAMENTO DA PORTABILIDADE', 
        'refin portabilidade': 'REFINANCIAMENTO DA PORTABILIDADE',
        'portabilidade': 'PORTABILIDADE',
        'refinanciamento': 'REFINANCIAMENTO',
        'margem livre': 'MARGEM LIVRE',
        'margem livre (novo)': 'MARGEM LIVRE (NOVO)',
        'novo': 'MARGEM LIVRE (NOVO)'
    }
    
    # Buscar mapeamento exato primeiro
    if normalized_lower in operation_mappings:
        return operation_mappings[normalized_lower]
    
    # Buscar por palavras-chave
    if 'portabilidade' in normalized_lower and 'refin' in normalized_lower:
        return 'REFINANCIAMENTO DA PORTABILIDADE'
    elif 'portabilidade' in normalized_lower:
        return 'PORTABILIDADE'
    elif 'refinanciamento' in normalized_lower:
        return 'REFINANCIAMENTO'
    elif 'margem' in normalized_lower:
        return 'MARGEM LIVRE (NOVO)'
    
    # Se nÃ£o encontrou, retorna normalizado em uppercase
    return normalized.upper()

def apply_mapping_daycoval_corrected(organ: str, operation_type: str) -> dict:
    """
    ðŸ”§ DAYCOVAL - Mapeamento direto por Ã“rgÃ£o + OperaÃ§Ã£o
    DAYCOVAL nÃ£o tem cÃ³digo da tabela no arquivo, entÃ£o mapeia por Ã³rgÃ£o e operaÃ§Ã£o
    """
    organ_normalized = organ.upper().strip()
    operation_normalized = operation_type.upper().strip()
    
    logging.info(f"ðŸ”§ DAYCOVAL Mapeamento - ORGAO: '{organ_normalized}', OPERACAO: '{operation_normalized}'")
    
    # Mapear para os padrÃµes do CSV do DAYCOVAL
    # CÃ³digos especÃ­ficos encontrados no CSV: 803463, 801307, 805994, 821121, 231880
    
    if organ_normalized in ["INSS"]:
        if "PORTABILIDADE" in operation_normalized and "REFIN" in operation_normalized:
            # Refinanciamento da Portabilidade - INSS
            return {
                "codigo_tabela": "805994",  # PadrÃ£o DAYCOVAL no CSV
                "banco_storm": "BANCO DAYCOVAL",
                "orgao_storm": "INSS",
                "operacao_storm": "Refinanciamento da Portabilidade",
                "taxa_storm": "2.14"  # Taxa padrÃ£o DAYCOVAL
            }
        elif "PORTABILIDADE" in operation_normalized:
            # Portabilidade + Refin - INSS
            return {
                "codigo_tabela": "803463",  # CÃ³digo comum DAYCOVAL
                "banco_storm": "BANCO DAYCOVAL",
                "orgao_storm": "INSS",
                "operacao_storm": "Portabilidade + Refin",
                "taxa_storm": "2.14"
            }
        elif "REFINANCIAMENTO" in operation_normalized:
            # Refinanciamento - INSS
            return {
                "codigo_tabela": "801307",  # Outro cÃ³digo DAYCOVAL
                "banco_storm": "BANCO DAYCOVAL",
                "orgao_storm": "INSS", 
                "operacao_storm": "Refinanciamento",
                "taxa_storm": "2.14"
            }
        else:
            # Margem Livre Novo - INSS
            return {
                "codigo_tabela": "821121",  # CÃ³digo DAYCOVAL padrÃ£o
                "banco_storm": "BANCO DAYCOVAL",
                "orgao_storm": "INSS",
                "operacao_storm": "Margem Livre (Novo)",
                "taxa_storm": "2.14"
            }
            
    elif organ_normalized in ["SPPREV"]:
        # SPPREV - usar cÃ³digo especÃ­fico
        return {
            "codigo_tabela": "231880",  # CÃ³digo DAYCOVAL para SPPREV
            "banco_storm": "BANCO DAYCOVAL", 
            "orgao_storm": "SPPREV",
            "operacao_storm": operation_type,  # Manter operaÃ§Ã£o original
            "taxa_storm": "2.14"
        }
        
    elif organ_normalized in ["EDUCACAO"]:
        # EducaÃ§Ã£o - usar cÃ³digo geral
        return {
            "codigo_tabela": "803463",  # CÃ³digo geral DAYCOVAL
            "banco_storm": "BANCO DAYCOVAL",
            "orgao_storm": "EDUCACAO", 
            "operacao_storm": operation_type,
            "taxa_storm": "2.14"
        }
    
    # Default - INSS Margem Livre
    logging.warning(f"âš ï¸ DAYCOVAL Mapeamento nÃ£o especÃ­fico para {organ_normalized} + {operation_normalized}, usando default INSS")
    return {
        "codigo_tabela": "803463",  # CÃ³digo mais comum no CSV
        "banco_storm": "BANCO DAYCOVAL",
        "orgao_storm": "INSS", 
        "operacao_storm": "Margem Livre (Novo)",
        "taxa_storm": "2.14"
    }

def _detect_santander_orgao(row: dict) -> str:
    """Detectar Ã³rgÃ£o do SANTANDER baseado no CONVENIO/PRODUTO"""
    convenio = str(row.get('CONVENIO', '')).strip().upper()
    produto = str(row.get('PRODUTO', '')).strip().upper()
    
    if 'PREF' in convenio or 'PREFEITURA' in convenio or 'AGUDOS' in convenio or 'RANCHARIA' in convenio:
        return 'PREF. DE AGUDOS - SP'  # PadrÃ£o para prefeituras
    elif 'INSS' in convenio or 'INSS' in produto:
        return 'INSS'
    elif 'SEGURO' in convenio or 'SEGURO' in produto:
        return 'INSS'  # Seguro vinculado ao INSS
    else:
        return 'INSS'  # Default

def _get_santander_operation_type(row: dict) -> str:
    """Extrair tipo de operaÃ§Ã£o do SANTANDER baseado no PRODUTO"""
    produto = str(row.get('PRODUTO', '')).strip().upper()
    
    if 'NOVO' in produto and 'REFIN' in produto:
        return 'Margem Livre (Novo)'  # Priorizar NOVO quando ambos estÃ£o presentes
    elif 'REFIN' in produto:
        return 'Refinanciamento'
    elif 'NOVO' in produto:
        return 'Margem Livre (Novo)'
    elif 'SEGURO' in produto:
        return 'Margem Livre (Novo)'  # Seguro Ã© geralmente operaÃ§Ã£o nova
    else:
        return 'Margem Livre (Novo)'  # Default

def _extract_santander_codigo_tabela(produto_str: str) -> str:
    """Extrair cÃ³digo tabela do campo PRODUTO do SANTANDER"""
    if not produto_str:
        return ""
    
    produto_str = str(produto_str).strip()
    
    # PadrÃ£o: "21387 - 810021387 - 1 OFERTA NOVO COM SEGURO"
    # Queremos extrair o cÃ³digo do meio (810021387)
    import re
    
    # Buscar padrÃ£o nÃºmero - nÃºmero - texto
    pattern = r'(\d+)\s*-\s*(\d+)\s*-'
    match = re.search(pattern, produto_str)
    if match:
        return match.group(2)  # Segundo nÃºmero Ã© o cÃ³digo
    
    # Se nÃ£o encontrar o padrÃ£o, buscar nÃºmeros individuais
    numbers = re.findall(r'\d+', produto_str)
    if len(numbers) >= 2:
        # Pegar o maior nÃºmero (provavelmente o cÃ³digo)
        return max(numbers, key=len)
    elif len(numbers) == 1:
        return numbers[0]
    
    return ""

def apply_mapping_santander_direct_code(codigo_tabela: str) -> dict:
    """Mapeamento direto para BANCO SANTANDER por cÃ³digo tabela extraÃ­do"""
    try:
        if not codigo_tabela or not codigo_tabela.isdigit():
            return {}
        
        # Procurar cÃ³digo no mapeamento
        for key, details in TABELA_MAPPING.items():
            if details.get('codigo_tabela') == codigo_tabela and 'BANCO SANTANDER' in key:
                logging.info(f"âœ… SANTANDER cÃ³digo {codigo_tabela}: {details.get('orgao_storm')} | {details.get('operacao_storm')} | {details.get('taxa_storm')}")
                return {
                    'orgao_storm': details.get('orgao_storm', ''),
                    'operacao_storm': details.get('operacao_storm', ''),
                    'taxa_storm': details.get('taxa_storm', ''),
                    'codigo_tabela': codigo_tabela
                }
        
        logging.warning(f"âš ï¸ SANTANDER cÃ³digo {codigo_tabela}: NÃ£o encontrado no mapeamento")
        return {}
        
    except Exception as e:
        logging.error(f"âŒ Erro no mapeamento direto Santander: {e}")
        return {}

def apply_mapping_averbai_corrected(organ: str, operation_type: str, tabela: str = "") -> dict:
    """CorreÃ§Ã£o especÃ­fica para AVERBAI - evita cÃ³digos 1005/1016 trocados com 994/992"""
    try:
        # Filtrar apenas registros AVERBAI do mapeamento
        averbai_candidates = []
        
        for key, details in TABELA_MAPPING.items():
            parts = key.split('|')
            if len(parts) == 4 and parts[0].upper() == 'AVERBAI':
                averbai_candidates.append((key, details))
        
        # Normalizar entradas
        organ_normalized = ' '.join(organ.strip().upper().split()) if organ else ""
        operation_normalized = normalize_operation_for_matching(operation_type)
        tabela_normalized = ' '.join(tabela.strip().upper().split()) if tabela else ""
        
        logging.info(f"ðŸ”Ž AVERBAI CORRIGIDO - Buscando: {organ_normalized} | {operation_normalized} | '{tabela_normalized}'")
        
        # Criar lista de candidatos com scoring inteligente
        scored_candidates = []
        
        for key, details in averbai_candidates:
            parts = key.split('|')
            csv_banco, csv_orgao, csv_operacao, csv_tabela = parts
            
            # Normalizar dados do CSV
            csv_orgao_norm = ' '.join(csv_orgao.upper().split())
            csv_operacao_norm = ' '.join(csv_operacao.upper().split())
            csv_tabela_norm = ' '.join(csv_tabela.upper().split())
            
            # Score inicial: 0
            total_score = 0
            match_details = []
            
            # 1. Ã“RGÃƒO deve ser EXATO (obrigatÃ³rio)
            if csv_orgao_norm == organ_normalized:
                total_score += 1000  # Score alto para Ã³rgÃ£o correto
                match_details.append("ORGAO_EXATO")
            else:
                continue  # Pular se Ã³rgÃ£o nÃ£o bate
            
            # 2. OPERAÃ‡ÃƒO (muito importante) - MELHORADA para distinguir 1005 vs 1016
            if csv_operacao_norm == operation_normalized:
                total_score += 500  # Score alto para operaÃ§Ã£o exata
                match_details.append("OPERACAO_EXATA")
            elif operation_normalized in csv_operacao_norm or csv_operacao_norm in operation_normalized:
                # CORREÃ‡ÃƒO ESPECÃFICA: Distinguir "Refinanciamento da Portabilidade" vs "Refinanciamento Da Portabilidade"
                # Problema: 1016 com "Refinanciamento Da Portabilidade" estava sobrepondo 1005 com "Refinanciamento da Portabilidade"
                
                # Verificar se Ã© match de case sensitivity (da vs Da)
                if csv_operacao_norm.replace("DA", "da") == operation_normalized.replace("DA", "da"):
                    # Match exato ignorando case de "da/Da" - dar score alto mas menor que exato
                    total_score += 450  # Score alto mas menor que operaÃ§Ã£o exata
                    match_details.append("OPERACAO_CASE_SIMILAR")
                else:
                    total_score += 200  # Score mÃ©dio para operaÃ§Ã£o parcial
                    match_details.append("OPERACAO_PARCIAL")
            else:
                # Verificar palavras-chave comuns
                op_words_input = set(operation_normalized.split())
                op_words_csv = set(csv_operacao_norm.split())
                common_op_words = op_words_input.intersection(op_words_csv)
                if common_op_words:
                    total_score += len(common_op_words) * 50
                    match_details.append(f"OPERACAO_PALAVRAS({len(common_op_words)})")
                else:
                    continue  # Pular se operaÃ§Ã£o nÃ£o tem nada a ver
            
            # 3. TABELA (decisivo para desempate) - PRIORIDADE ABSOLUTA
            if tabela_normalized and csv_tabela_norm:
                if csv_tabela_norm == tabela_normalized:
                    total_score += 2000  # MATCH PERFEITO - PRIORIDADE ABSOLUTA (dobrado)
                    match_details.append("TABELA_EXATA")
                else:
                    # AnÃ¡lise por palavras mais sofisticada
                    tabela_words = set(w for w in tabela_normalized.split() if len(w) > 2)
                    csv_words = set(w for w in csv_tabela_norm.split() if len(w) > 2)
                    
                    if tabela_words and csv_words:
                        common_words = tabela_words.intersection(csv_words)
                        
                        if tabela_words == csv_words:
                            total_score += 800  # Mesmo conjunto de palavras
                            match_details.append("TABELA_PALAVRAS_IDENTICAS")
                        elif len(common_words) >= len(tabela_words) * 0.8:  # 80% das palavras
                            total_score += 600
                            match_details.append(f"TABELA_ALTA_SIMILARIDADE({len(common_words)}/{len(tabela_words)})")
                        elif len(common_words) >= len(tabela_words) * 0.5:  # 50% das palavras
                            total_score += 400
                            match_details.append(f"TABELA_MEDIA_SIMILARIDADE({len(common_words)}/{len(tabela_words)})")
                        elif common_words:
                            total_score += len(common_words) * 100
                            match_details.append(f"TABELA_ALGUMAS_PALAVRAS({len(common_words)})")
                    
                    # Bonus para substring match
                    if tabela_normalized in csv_tabela_norm or csv_tabela_norm in tabela_normalized:
                        total_score += 200
                        match_details.append("TABELA_SUBSTRING")
            
            # 4. Bonus para taxas mais altas (desempate)
            try:
                taxa_str = details.get('taxa_storm', '0%')
                taxa_float = float(taxa_str.replace('%', '').replace(',', '.'))
                total_score += taxa_float * 10  # Bonus pequeno para taxa mais alta
                match_details.append(f"TAXA_BONUS({taxa_float})")
            except:
                pass
            
            # 5. PriorizaÃ§Ã£o de cÃ³digos - CORREÃ‡ÃƒO ESPECÃFICA para 1005 vs 1016
            try:
                codigo_str = details.get('codigo_tabela', '0')
                codigo_int = int(codigo_str)
                
                # CORREÃ‡ÃƒO ESPECÃFICA: Quando nÃ£o hÃ¡ tabela especÃ­fica, priorizar cÃ³digos menores
                # Problema: 1016 (taxa 1,85%) estava ganhando de 1005 (taxa 1,80%) sem tabela
                if not tabela_normalized or len(tabela_normalized) == 0:
                    # Sem tabela especÃ­fica: priorizar cÃ³digos menores (mais estabelecidos)
                    if codigo_int <= 1005:  # CÃ³digos "antigos" tÃªm prioridade
                        total_score += 100
                        match_details.append(f"CODIGO_ESTABELECIDO({codigo_int})")
                    else:
                        # CÃ³digos novos tÃªm menor prioridade quando nÃ£o hÃ¡ tabela especÃ­fica
                        total_score += 50
                        match_details.append(f"CODIGO_NOVO_SEM_TABELA({codigo_int})")
                else:
                    # Com tabela especÃ­fica: cÃ³digos maiores podem ter ligeira vantagem
                    if codigo_int >= 1000:
                        total_score += codigo_int * 0.05  # Reduzido de 0.1 para 0.05
                        match_details.append(f"CODIGO_NOVO({codigo_int})")
            except:
                pass
            
            # Adicionar candidato se tem score mÃ­nimo
            if total_score >= 1000:  # Pelo menos Ã³rgÃ£o correto
                scored_candidates.append({
                    'details': details,
                    'score': total_score,
                    'match_info': ' + '.join(match_details),
                    'tabela_csv': csv_tabela_norm
                })
        
        # Ordenar candidatos por score (maior primeiro)
        scored_candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # Log dos candidatos
        logging.info(f"ðŸ“Š AVERBAI - Encontrados {len(scored_candidates)} candidatos:")
        for i, candidate in enumerate(scored_candidates[:3]):  # Top 3
            codigo = candidate['details'].get('codigo_tabela', 'N/A')
            taxa = candidate['details'].get('taxa_storm', 'N/A')
            logging.info(f"   {i+1}. Score {candidate['score']}: CÃ³digo {codigo} | Taxa {taxa}")
            logging.info(f"      Match: {candidate['match_info']}")
        
        if not scored_candidates:
            logging.error(f"âŒ AVERBAI - Nenhum candidato encontrado para {organ_normalized} | {operation_normalized}")
            return {}
        
        # Retornar o melhor candidato
        best = scored_candidates[0]
        result = best['details']
        
        logging.info(f"âœ… AVERBAI RESULTADO CORRIGIDO: CÃ³digo {result.get('codigo_tabela')} | Score {best['score']}")
        
        return result
        
    except Exception as e:
        logging.error(f"âŒ Erro no mapeamento AVERBAI corrigido: {str(e)}")
        return {}

def apply_mapping(bank_name: str, organ: str, operation_type: str, usuario: str = "", tabela: str = "") -> dict:
    """Aplica mapeamento automÃ¡tico MELHORADO com correÃ§Ã£o especÃ­fica para AVERBAI"""
    try:
        # Normalizar nomes para busca (remover espaÃ§os extras e converter para uppercase)
        bank_normalized = ' '.join(bank_name.strip().upper().split()) if bank_name else ""
        organ_normalized = ' '.join(organ.strip().upper().split()) if organ else ""
        operation_normalized = normalize_operation_for_matching(operation_type)
        
        # Normalizar tabela (SEM modificaÃ§Ãµes - preservar nomes originais)
        tabela_normalized = ' '.join(tabela.strip().upper().split()) if tabela else ""
        
        # CORREÃ‡ÃƒO ESPECÃFICA PARA AVERBAI - usar funÃ§Ã£o especializada
        if bank_normalized == "AVERBAI":
            logging.info(f"ðŸ”§ AVERBAI detectado - usando correÃ§Ã£o especÃ­fica")
            return apply_mapping_averbai_corrected(organ, operation_type, tabela)
            
        # CORREÃ‡ÃƒO ESPECÃFICA PARA DAYCOVAL - usar funÃ§Ã£o especializada
        if bank_normalized == "BANCO DAYCOVAL" or bank_normalized == "DAYCOVAL":
            logging.info(f"ðŸ”§ DAYCOVAL detectado - usando correÃ§Ã£o especÃ­fica por Ã³rgÃ£o+operaÃ§Ã£o")
            return apply_mapping_daycoval_corrected(organ, operation_type)
            
        # CORREÃ‡ÃƒO ESPECÃFICA PARA SANTANDER - usar mapeamento direto por cÃ³digo
        if bank_normalized == "BANCO SANTANDER" or bank_normalized == "SANTANDER":
            # Para Santander, usar o parÃ¢metro tabela como cÃ³digo_tabela
            if tabela_normalized and tabela_normalized.isdigit():
                logging.info(f"ðŸ¦ SANTANDER detectado - usando mapeamento direto por cÃ³digo: {tabela_normalized}")
                return apply_mapping_santander_direct_code(tabela_normalized)
            else:
                logging.warning(f"âš ï¸ SANTANDER sem cÃ³digo vÃ¡lido ({tabela_normalized}), usando busca tradicional")
        
        logging.info(f"ðŸ” Buscando mapeamento: BANCO={bank_normalized} | ORGAO={organ_normalized} | OPERACAO={operation_normalized} | TABELA={tabela_normalized}")
        
        logging.info(f"ðŸ” Buscando mapeamento: BANCO={bank_normalized} | ORGAO={organ_normalized} | OPERACAO={operation_normalized} | TABELA={tabela_normalized}")
        
        # PRIORIDADE 1: Busca EXATA por BANCO + ORGÃƒO + OPERAÃ‡ÃƒO + TABELA (mais especÃ­fico e confiÃ¡vel)
        if tabela_normalized:
            best_match = None
            best_match_score = 0
            
            # Log detalhado para AVERBAI
            is_averbai = bank_normalized == "AVERBAI"
            if is_averbai:
                logging.info(f"ðŸ”Ž AVERBAI - Iniciando busca por tabela: '{tabela_normalized}' (len={len(tabela_normalized)})")
            
            for key, details in TABELA_MAPPING.items():
                parts = key.split('|')
                if len(parts) == 4:
                    key_banco, key_orgao, key_operacao, key_tabela = parts
                    # Normalizar keys removendo espaÃ§os extras
                    key_banco_norm = ' '.join(key_banco.upper().split())
                    key_orgao_norm = ' '.join(key_orgao.upper().split())
                    key_operacao_norm = ' '.join(key_operacao.upper().split())
                    key_tabela_norm = ' '.join(key_tabela.upper().split())
                    
                    # Busca EXATA para banco
                    if bank_normalized != key_banco_norm:
                        continue
                    
                    # Busca FLEXÃVEL para Ã³rgÃ£o (pode variar ligeiramente)
                    organ_match = (
                        organ_normalized == key_orgao_norm or
                        organ_normalized in key_orgao_norm or 
                        key_orgao_norm in organ_normalized
                    )
                    
                    if not organ_match:
                        continue
                    
                    # Para tabela, usar matching inteligente com diferentes nÃ­veis de precisÃ£o
                    match_score = 0
                    
                    if tabela_normalized == key_tabela_norm:
                        match_score = 5  # Match exato (melhor)
                    else:
                        # AnÃ¡lise por palavras para casos com formataÃ§Ã£o diferente
                        tabela_words = set(tabela_normalized.split())
                        key_words = set(key_tabela_norm.split())
                        
                        # Remover palavras muito curtas que sÃ£o ruÃ­do
                        tabela_words_filtered = {w for w in tabela_words if len(w) > 2}
                        key_words_filtered = {w for w in key_words if len(w) > 2}
                        
                        # Verificar se todas as palavras significativas batem
                        if tabela_words_filtered and key_words_filtered:
                            if tabela_words_filtered == key_words_filtered:
                                match_score = 4  # Mesmo conjunto de palavras, ordem diferente
                            elif tabela_words_filtered.issubset(key_words_filtered):
                                match_score = 3  # Tabela do banco contÃ©m todas as palavras do CSV
                            elif key_words_filtered.issubset(tabela_words_filtered):
                                match_score = 3  # CSV contÃ©m todas as palavras da tabela do banco
                            else:
                                # Calcular palavras em comum
                                common_words = tabela_words_filtered.intersection(key_words_filtered)
                                if len(common_words) >= min(2, len(tabela_words_filtered) // 2):
                                    match_score = 2  # Pelo menos metade das palavras batem
                        
                        # Fallback: matching por substring (menos confiÃ¡vel)
                        if match_score == 0:
                            if tabela_normalized in key_tabela_norm or key_tabela_norm in tabela_normalized:
                                match_score = 1
                    
                    # Guardar melhor match
                    if match_score > best_match_score:
                        best_match_score = match_score
                        best_match = details
                        best_match_key = key_tabela_norm
                        
                        # Log detalhado para AVERBAI
                        if is_averbai:
                            logging.info(f"  âœ¨ MELHOR MATCH atÃ© agora: score={match_score}, tabela_csv='{key_tabela_norm}', codigo={details.get('codigo_tabela', 'N/A')}, taxa={details.get('taxa_storm', 'N/A')}")
            
            if best_match:
                if is_averbai:
                    logging.info(f"âœ… AVERBAI - Resultado FINAL: score={best_match_score}, key='{best_match_key}', Codigo={best_match['codigo_tabela']}, Taxa={best_match['taxa_storm']}, Operacao={best_match['operacao_storm']}")
                else:
                    logging.info(f"âœ… Mapeamento por TABELA (score={best_match_score}): Codigo={best_match['codigo_tabela']} | Taxa={best_match['taxa_storm']} | Operacao={best_match['operacao_storm']}")
                return best_match
            
            # Log se tabela nÃ£o foi encontrada
            if is_averbai:
                logging.error(f"âŒ AVERBAI - TABELA NÃƒO ENCONTRADA: '{tabela_normalized}' - Tentando fallback genÃ©rico")
            else:
                logging.warning(f"âš ï¸ Tabela '{tabela_normalized}' nÃ£o encontrada, tentando fallback genÃ©rico")
        
        # PRIORIDADE 2: Busca por BANCO + ORGÃƒO + OPERAÃ‡ÃƒO (usa DETAILED_MAPPING)
        detail_key_candidates = []
        
        for bank_key, organs in ORGAN_MAPPING.items():
            bank_key_norm = ' '.join(bank_key.upper().split())
            # Busca EXATA para banco
            if bank_normalized == bank_key_norm:
                for organ_key, operations in organs.items():
                    organ_key_norm = ' '.join(organ_key.upper().split())
                    # Busca FLEXÃVEL para Ã³rgÃ£o
                    organ_match = (
                        organ_normalized == organ_key_norm or
                        organ_normalized in organ_key_norm or 
                        organ_key_norm in organ_normalized
                    )
                    if organ_match:
                        for op_key, table_code in operations.items():
                            op_key_norm = ' '.join(op_key.upper().split())
                            # Busca FLEXÃVEL para operaÃ§Ã£o
                            operation_match = (
                                operation_normalized == op_key_norm or
                                operation_normalized in op_key_norm or 
                                op_key_norm in operation_normalized
                            )
                            if operation_match:
                                detail_key = f"{bank_key}|{organ_key}|{op_key}"
                                detail_key_candidates.append(detail_key)
        
        # Processar candidatos do mapeamento detalhado com PRIORIZAÃ‡ÃƒO INTELIGENTE
        if detail_key_candidates:
            best_candidate = None
            best_score = 0
            
            for detail_key in detail_key_candidates:
                options = DETAILED_MAPPING.get(detail_key, [])
                if options:
                    details = options[0]  # Usar primeira opÃ§Ã£o da lista
                    
                    # Calcular score de especificidade da chave
                    parts = detail_key.split('|')
                    if len(parts) >= 3:
                        operation_part = parts[2]
                        
                        # Score baseado em especificidade
                        score = 0
                        
                        # 1. Match exato de operaÃ§Ã£o (case sensitive) tem prioridade mÃ¡xima
                        if operation_part == operation_type:
                            score += 1000
                        
                        # 2. Match de operaÃ§Ã£o normalizada
                        elif operation_part.upper() == operation_normalized:
                            score += 500
                        
                        # 3. OperaÃ§Ãµes mais especÃ­ficas (mais palavras) tÃªm prioridade
                        word_count = len(operation_part.split())
                        score += word_count * 50
                        
                        # 4. Taxa mais alta tem ligeira prioridade (desempate)
                        taxa = details.get('taxa_storm', '0%').replace('%', '').replace(',', '.')
                        try:
                            taxa_float = float(taxa)
                            score += taxa_float
                        except:
                            pass
                        
                        # Log detalhado para AVERBAI
                        if bank_normalized == "AVERBAI":
                            logging.info(f"ðŸ† Candidato: '{detail_key}' | Score={score} | CÃ³digo={details.get('codigo_tabela')} | Taxa={details.get('taxa_storm')}")
                        
                        if score > best_score:
                            best_score = score
                            best_candidate = (detail_key, details)
            
            if best_candidate:
                detail_key, details = best_candidate
                logging.info(f"âœ… MELHOR CANDIDATO (score={best_score}): {detail_key} -> Codigo={details['codigo_tabela']} | Taxa={details['taxa_storm']}")
                return details
        
        # PRIORIDADE 3: Busca genÃ©rica por BANCO + ORGÃƒO (fallback mais amplo)
        bank_organ_key = f"{bank_normalized}|{organ_normalized}"
        if bank_organ_key in BANK_ORGAN_MAPPING:
            options = BANK_ORGAN_MAPPING[bank_organ_key]
            if options:
                # Tentar encontrar a operaÃ§Ã£o mais compatÃ­vel
                best_option = None
                best_op_score = 0
                
                for option in options:
                    op_storm = option.get('operacao_storm', '').upper()
                    # Calcular compatibilidade da operaÃ§Ã£o com priorizaÃ§Ã£o inteligente
                    if operation_normalized == op_storm:
                        op_score = 100  # Match exato - mÃ¡xima prioridade
                    elif operation_normalized in op_storm:
                        # OperaÃ§Ã£o buscada estÃ¡ contida na operaÃ§Ã£o do Storm
                        # Dar prioridade a operaÃ§Ãµes mais especÃ­ficas (mais palavras)
                        word_count_bonus = len(op_storm.split()) * 5
                        op_score = 50 + word_count_bonus  # Substring match + bonus por especificidade
                    elif op_storm in operation_normalized:
                        # OperaÃ§Ã£o Storm estÃ¡ contida na operaÃ§Ã£o buscada
                        op_score = 40  # Substring match reverso
                    elif any(word in op_storm for word in operation_normalized.split()) or any(word in operation_normalized for word in op_storm.split()):
                        # Palavras em comum
                        common_words = len(set(operation_normalized.split()) & set(op_storm.split()))
                        op_score = 10 + (common_words * 2)  # Base + bonus por palavra comum
                    else:
                        op_score = 0
                    
                    # Log detalhado para AVERBAI debug
                    if bank_normalized == "AVERBAI" and operation_normalized and op_score > 0:
                        logging.info(f"ðŸ” AVERBAI Score: '{operation_normalized}' vs '{op_storm}' = {op_score} (CÃ³digo: {option.get('codigo_tabela', 'N/A')})")
                    
                    if op_score > best_op_score:
                        best_op_score = op_score
                        best_option = option
                
                if best_option:
                    logging.info(f"âœ… Mapeamento GENÃ‰RICO por BANCO+ORGAO: Codigo={best_option['codigo_tabela']} | Taxa={best_option['taxa_storm']} | Operacao={best_option['operacao_storm']}")
                    return best_option
                else:
                    # Se nÃ£o encontrou por operaÃ§Ã£o, usar a primeira opÃ§Ã£o
                    first_option = options[0]
                    logging.warning(f"âš ï¸ Usando primeira opÃ§Ã£o genÃ©rica para {bank_normalized}+{organ_normalized}: Codigo={first_option['codigo_tabela']} | Taxa={first_option['taxa_storm']}")
                    return first_option
        
        # Se chegou atÃ© aqui, nÃ£o encontrou nenhum mapeamento
        logging.error(f"âŒ NENHUM mapeamento encontrado para: {bank_normalized} -> {organ_normalized} -> {operation_normalized}")
        return {}
        
    except Exception as e:
        logging.error(f"âŒ Erro no mapeamento: {str(e)}")
        return {}

def normalize_bank_data(df: pd.DataFrame, bank_type: str) -> pd.DataFrame:
    """Normaliza dados do banco para estrutura padrÃ£o usando mapeamento correto baseado no arquivo"""
    # Garantir acesso Ã s variÃ¡veis globais
    global ORGAN_MAPPING, DETAILED_MAPPING, TABELA_MAPPING, BANK_ORGAN_MAPPING
    
    normalized_data = []
    
    logging.info(f"=" * 100)
    logging.info(f"ðŸ”§ INICIANDO normalize_bank_data para {bank_type} com {len(df)} registros")
    logging.info(f"   Colunas disponÃ­veis: {list(df.columns)}")
    
    # Debug especÃ­fico para PAULISTA
    if bank_type == "PAULISTA":
        logging.error(f"ðŸ¦ NORMALIZE_BANK_DATA: PAULISTA com {len(df)} linhas")
        for i in range(min(3, len(df))):
            row_data = df.iloc[i].to_dict()
            logging.error(f"   Linha {i}: Unnamed:0='{row_data.get('Unnamed: 0', 'N/A')}'")
    
    logging.info(f"=" * 100)
    
    # VALIDAÃ‡ÃƒO: Remover linhas completamente vazias
    df = df.dropna(how='all')
    
    # VALIDAÃ‡ÃƒO: Verificar se ainda tem dados
    if df.empty:
        logging.error(f"âŒ {bank_type}: DataFrame vazio apÃ³s remover linhas vazias")
        return pd.DataFrame()
    
    # VALIDAÃ‡ÃƒO: Verificar se tem pelo menos 3 colunas
    if len(df.columns) < 3:
        logging.error(f"âŒ {bank_type}: Muito poucas colunas ({len(df.columns)}) - Colunas: {list(df.columns)}")
        return pd.DataFrame()
    

    logging.info(f"âœ… DataFrame passou validaÃ§Ãµes - {len(df)} registros, {len(df.columns)} colunas")
    
    logging.info(f"ApÃ³s limpeza: {len(df)} registros vÃ¡lidos com {len(df.columns)} colunas")
    
    for idx, row in df.iterrows():
        logging.info(f"ðŸ” PROCESSANDO linha {idx}: {dict(row)}")
        
        # Pular linhas que sÃ£o claramente cabeÃ§alhos ou metadados
        row_str = ' '.join([str(val).lower() for val in row.values if pd.notna(val)])
        
        # DEBUG: Log para PAULISTA mostrando a row_str construÃ­da
        if bank_type == "PAULISTA":
            logging.info(f"ðŸ” PAULISTA linha {idx}: row_str = '{row_str[:100]}...'")
        
        # Detectar linhas de metadados/cabeÃ§alho
        metadata_indicators = [
            'relatÃ³rio', 'relatorio', 'total de registros', 'total:', 'pÃ¡gina',
            'data de emissÃ£o', 'data de extraÃ§Ã£o', 'banco:', 'perÃ­odo',
            'nome do banco', 'agencia:', 'conta:', 'saldo:'
        ]
        
        # Detectar linhas de cabeÃ§alho especÃ­ficas do BANCO PAULISTA
        paulista_header_indicators = [
            'nÂº proposta', 'numero proposta', 'data captura', 'banco paulista',
            'cpf/cnpj proponente', 'nome do proponente', 'valor solicitado',
            'quant. parcelas', 'usuÃ¡rio digitador', 'usuario digitador',
            'relaÃ§Ã£o de propostas', 'analÃ­tico', 'relatÃ³rio', 'relatorio'
        ]
        
        # Verificar se deve pular linha de cabeÃ§alho
        is_header = any(indicator in row_str for indicator in metadata_indicators + paulista_header_indicators)
        
        if bank_type == "PAULISTA":
            # Log bem detalhado para PAULISTA
            primeira_col = row.get('Unnamed: 0', '')
            logging.error(f"ðŸ” PAULISTA linha {idx}: Primeira coluna = '{primeira_col}'")
            logging.error(f"ðŸ” PAULISTA linha {idx}: row_str = '{row_str[:50]}...'")
            logging.error(f"ðŸ” PAULISTA linha {idx}: Ã‰ cabeÃ§alho? {is_header}")
            if is_header:
                matched_indicators = [ind for ind in metadata_indicators + paulista_header_indicators if ind in row_str]
                logging.error(f"ðŸ“‹ PAULISTA: Indicadores encontrados: {matched_indicators}")
        
        if is_header:
            if bank_type == "PAULISTA":
                logging.error(f"ðŸ“‹ PAULISTA: Pulando linha de cabeÃ§alho: {row_str[:50]}...")
            else:
                logging.debug(f"Pulando linha de cabeÃ§alho/metadados: {row_str[:100]}")
            continue
        else:
            # Esta linha NÃƒO Ã© cabeÃ§alho - vai processar
            if bank_type == "PAULISTA":
                logging.error(f"âœ… PAULISTA linha {idx}: VAI PROCESSAR - Primeira coluna: '{row.get('Unnamed: 0', '')}')")
        
        normalized_row = {}
        
        logging.debug(f"ðŸ”§ Normalizando linha {idx} para banco: {bank_type}")
        
        if bank_type == "AVERBAI":
            # Mapeamento AVERBAI - Baseado na estrutura REAL do map_relat_atualizados.txt
            # Detectar tipo de operaÃ§Ã£o do campo TipoProduto ou outro campo
            tipo_produto = str(row.get('TipoProduto', '')).strip()
            tipo_operacao_averbai = "Margem Livre (Novo)"  # padrÃ£o
            orgao_averbai = ""
            
            # Identificar ORGAO e tipo de operaÃ§Ã£o baseado nos campos do arquivo
            tipo_produto_upper = tipo_produto.upper()
            
            # Detectar tipo de operaÃ§Ã£o PRIMEIRO (isso afeta o Ã³rgÃ£o)
            if 'PORTABILIDADE' in tipo_produto_upper and 'REFIN' in tipo_produto_upper:
                tipo_operacao_averbai = "Refinanciamento Da Portabilidade"  # âœ… Corrigido para maiÃºsculo "Da"
            elif 'PORTABILIDADE' in tipo_produto_upper:
                tipo_operacao_averbai = "Portabilidade"
            elif 'REFIN' in tipo_produto_upper:
                tipo_operacao_averbai = "Refinanciamento"
            else:
                tipo_operacao_averbai = "Margem Livre (Novo)"
            
            # Detectar ORGAO - CRÃTICO: Portabilidade/Refinanciamento sÃ£o sempre INSS no CSV!
            if tipo_operacao_averbai in ["Portabilidade", "Refinanciamento Da Portabilidade", "Refinanciamento"]:
                # Portabilidade e Refinanciamento estÃ£o cadastrados como INSS no CSV
                orgao_averbai = 'INSS'
            elif 'INSS' in tipo_produto_upper or 'SAQUE INSS' in tipo_produto_upper:
                orgao_averbai = 'INSS'
            elif 'FGTS' in tipo_produto_upper or 'SAQUE FGTS' in tipo_produto_upper:
                orgao_averbai = 'FGTS'
            else:
                orgao_averbai = 'FGTS'  # Default
            
            # Normalizar nome da tabela AVERBAI removendo espaÃ§os extras e variaÃ§Ãµes
            tabela_raw = str(row.get('Tabela', '')).strip()
            # Normalizar: remover espaÃ§os extras no inÃ­cio/fim e mÃºltiplos espaÃ§os internos
            tabela_normalizada = ' '.join(tabela_raw.split()) if tabela_raw else ""
            
            # ðŸŽ¯ SOLUÃ‡ÃƒO DEFINITIVA: Usar cÃ³digo direto do arquivo AVERBAI!
            # Campo IdTableComissao jÃ¡ tem o cÃ³digo correto (1005, 1016, 994, 992, etc)
            codigo_tabela_direto = str(row.get('IdTableComissao', '')).strip()
            cpf_cliente = str(row.get('CpfCliente', '')).strip()
            
            # ï¿½ FUNÃ‡ÃƒO para formatar CPF no padrÃ£o brasileiro
            def format_cpf(cpf_str):
                """Formata CPF para o padrÃ£o 000.000.000-00"""
                if not cpf_str:
                    return ""
                
                # Remover tudo que nÃ£o Ã© nÃºmero
                cpf_numbers = ''.join(filter(str.isdigit, str(cpf_str)))
                
                # Verificar se tem 11 dÃ­gitos
                if len(cpf_numbers) != 11:
                    logging.warning(f"âš ï¸ CPF invÃ¡lido (nÃ£o tem 11 dÃ­gitos): '{cpf_str}' -> '{cpf_numbers}'")
                    return cpf_str  # Retornar original se invÃ¡lido
                
                # Formatar: 000.000.000-00
                cpf_formatted = f"{cpf_numbers[0:3]}.{cpf_numbers[3:6]}.{cpf_numbers[6:9]}-{cpf_numbers[9:11]}"
                return cpf_formatted
            
            # Formatar CPF
            cpf_cliente = format_cpf(cpf_cliente)
            
            # ï¿½ðŸ’° FUNÃ‡ÃƒO para formatar valores no padrÃ£o brasileiro
            def format_brazilian_currency(value_str):
                """Converte valores para formato brasileiro: 1.500,39 ou 87,58"""
                if not value_str or str(value_str).strip() in ['', 'nan', 'None', '0']:
                    return "0,00"
                
                try:
                    # Limpar o valor (remover espaÃ§os, moeda, etc.)
                    clean_value = str(value_str).strip().replace('R$', '').replace(' ', '')
                    
                    # Se jÃ¡ estÃ¡ no formato brasileiro, manter
                    if ',' in clean_value and clean_value.count(',') == 1:
                        parts = clean_value.split(',')
                        if len(parts[1]) == 2:  # Duas casas decimais apÃ³s vÃ­rgula
                            return clean_value
                    
                    # Converter do formato americano (ponto decimal)
                    if '.' in clean_value:
                        # Separar parte inteira e decimal
                        parts = clean_value.split('.')
                        integer_part = parts[0]
                        decimal_part = parts[1][:2] if len(parts) > 1 else "00"  # Max 2 decimais
                    else:
                        # Sem decimal, assumir valor inteiro
                        integer_part = clean_value
                        decimal_part = "00"
                    
                    # Garantir que decimal tenha 2 dÃ­gitos
                    if len(decimal_part) == 1:
                        decimal_part += "0"
                    elif len(decimal_part) == 0:
                        decimal_part = "00"
                    
                    # Converter para float para formatar
                    float_value = float(f"{integer_part}.{decimal_part}")
                    
                    # Formatar no padrÃ£o brasileiro
                    if float_value >= 1000:
                        # Valores altos: 1.500,39
                        formatted = f"{float_value:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
                    else:
                        # Valores baixos: 87,58
                        formatted = f"{float_value:.2f}".replace('.', ',')
                    
                    return formatted
                    
                except (ValueError, TypeError) as e:
                    logging.warning(f"âš ï¸ AVERBAI: Erro ao formatar valor '{value_str}': {e}")
                    return str(value_str)  # Retornar original se houver erro
            
            # ðŸ” LOG DEBUG: Campos importantes do AVERBAI
            logging.info(f"ðŸ” AVERBAI Debug - Id: {row.get('Id', 'N/A')}, IdTableComissao: '{codigo_tabela_direto}', CpfCliente: '{cpf_cliente}', NomeCliente: '{row.get('NomeCliente', 'N/A')}'")
            
            # Buscar dados do cÃ³digo no CSV para pegar Ã³rgÃ£o e taxa corretos
            orgao_final = orgao_averbai  # Default baseado no TipoProduto
            taxa_final = ""
            operacao_final = tipo_operacao_averbai
            
            if codigo_tabela_direto and codigo_tabela_direto.isdigit():
                # Procurar informaÃ§Ãµes do cÃ³digo no CSV
                for key, details in TABELA_MAPPING.items():
                    if details.get('codigo_tabela', '') == codigo_tabela_direto:
                        # Usar dados oficiais do CSV
                        orgao_csv = details.get('orgao_storm', '')
                        taxa_csv = details.get('taxa_storm', '')
                        operacao_csv = details.get('operacao_storm', '')
                        
                        if orgao_csv:
                            orgao_final = orgao_csv
                        if taxa_csv:
                            taxa_final = taxa_csv
                        if operacao_csv:
                            operacao_final = operacao_csv
                            
                        logging.info(f"âœ… AVERBAI cÃ³digo {codigo_tabela_direto}: {orgao_final} | {operacao_final} | {taxa_final} | CPF: {cpf_cliente}")
                        break
                else:
                    # CÃ³digo nÃ£o encontrado no CSV - usar detecÃ§Ã£o automÃ¡tica
                    logging.warning(f"âš ï¸ AVERBAI cÃ³digo {codigo_tabela_direto}: NÃ£o encontrado no CSV, usando detecÃ§Ã£o automÃ¡tica")
            else:
                # Sem cÃ³digo - usar nome da tabela como antes
                codigo_tabela_direto = tabela_normalizada
                logging.warning(f"âš ï¸ AVERBAI sem IdTableComissao, usando nome da tabela: '{tabela_normalizada}'")
            
            # ðŸ’° Formatar valores no padrÃ£o brasileiro
            valor_operacao_br = format_brazilian_currency(row.get('ValorOperacao', ''))
            valor_liberado_br = format_brazilian_currency(row.get('ValorLiquido', ''))
            valor_parcela_br = format_brazilian_currency(row.get('ValorParcela', ''))
            
            # ðŸ“Š Organizar taxa conforme tabela (garantir formato percentual)
            taxa_formatada = taxa_final
            if taxa_formatada and '%' not in taxa_formatada:
                # Se nÃ£o tem %, adicionar
                try:
                    # Tentar converter para float e formatar
                    taxa_num = float(taxa_formatada.replace(',', '.'))
                    taxa_formatada = f"{taxa_num:.2f}%".replace('.', ',')
                except:
                    taxa_formatada = f"{taxa_formatada}%"
            
            logging.info(f"ðŸ’° AVERBAI Proposta {row.get('Id', 'N/A')}: Valores formatados - OPERAÃ‡ÃƒO: {valor_operacao_br}, LIBERADO: {valor_liberado_br}, PARCELA: {valor_parcela_br}, TAXA: {taxa_formatada}")
            
            normalized_row = {
                "PROPOSTA": str(row.get('Id', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data', '')).strip(),
                "BANCO": "AVERBAI",
                "ORGAO": orgao_final,  # Ã“rgÃ£o correto do CSV ou detectado
                "TIPO_OPERACAO": operacao_final,  # OperaÃ§Ã£o correta do CSV ou detectada
                "NUMERO_PARCELAS": str(row.get('Prazo', '')).strip(),
                "VALOR_OPERACAO": valor_operacao_br,  # ðŸ’° FORMATO BRASILEIRO
                "VALOR_LIBERADO": valor_liberado_br,  # ðŸ’° FORMATO BRASILEIRO
                "USUARIO_BANCO": str(row.get('LoginConsultor', '')).strip(),
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('DataFinalizaÃ§Ã£o', '')).strip() if 'DataFinalizaÃ§Ã£o' in df.columns else "",
                "CPF": cpf_cliente,  # CPF jÃ¡ extraÃ­do e validado
                "NOME": str(row.get('NomeCliente', '')).strip(),
                "DATA_NASCIMENTO": str(row.get('DataNascimento', '')).strip() if 'DataNascimento' in df.columns else "",
                "VALOR_PARCELAS": valor_parcela_br,  # ðŸ’° FORMATO BRASILEIRO
                "CODIGO_TABELA": codigo_tabela_direto,  # ðŸŽ¯ CÃ“DIGO DIRETO DO ARQUIVO!
                "TAXA": taxa_formatada,  # ðŸ“Š TAXA ORGANIZADA CONFORME TABELA
                "OBSERVACOES": str(row.get('ObservaÃ§Ãµes', row.get('Observacoes', row.get('Obs', '')))).strip()
            }
            
        elif bank_type == "DIGIO":
            # Mapeamento BANCO DIGIO S.A. - Suporte para duas estruturas:
            # 1. Estrutura com colunas Unnamed (arquivo XLS original do banco)
            # 2. Estrutura com cabeÃ§alhos nomeados (arquivo CSV exportado)
            
            # Verificar se Ã© estrutura com Unnamed ou com cabeÃ§alhos nomeados
            # Checar se a maioria das colunas sÃ£o Unnamed
            unnamed_count = sum(1 for col in row.index if 'unnamed:' in str(col).lower())
            total_count = len(row.index)
            has_unnamed_structure = unnamed_count > (total_count * 0.5)  # Mais de 50% Unnamed
            
            logging.info(f"ðŸ” DIGIO estrutura: {unnamed_count} Unnamed de {total_count} colunas ({unnamed_count/total_count*100:.1f}%)")
            
            if not has_unnamed_structure:
                # Estrutura com cabeÃ§alhos nomeados (CSV exportado)
                logging.info("ðŸ” DIGIO: Detectada estrutura com cabeÃ§alhos nomeados")
                proposta = str(row.get('PROPOSTA', '')).strip()
                tipo_operacao = str(row.get('TIPO DE OPERACAO', row.get('TIPO_OPERACAO', ''))).strip()
                data_cadastro = str(row.get('DATA CADASTRO', row.get('DATA_CADASTRO', ''))).strip()
                situacao = str(row.get('SITUACAO', '')).strip()
                data_lancamento = str(row.get('DATA DE PAGAMENTO', row.get('DATA_PAGAMENTO', ''))).strip()
                nome_orgao_raw = str(row.get('ORGAO', '')).strip()
                usuario_digitador = str(row.get('USUARIO BANCO', row.get('USUARIO_BANCO', ''))).strip()
                cpf_cliente = str(row.get('CPF', '')).strip()
                nome_cliente = str(row.get('NOME', '')).strip()
                data_nascimento = str(row.get('DATA DE NASCIMENTO', row.get('DATA_NASCIMENTO', ''))).strip()
                qtd_parcelas = str(row.get('NUMERO PARCELAS', row.get('NUMERO_PARCELAS', ''))).strip()
                vlr_parcela = str(row.get('VALOR PARCELAS', row.get('VALOR_PARCELAS', ''))).strip()
                vlr_financiado = str(row.get('VALOR OPERACAO', row.get('VALOR_OPERACAO', ''))).strip()
                vlr_lib1 = str(row.get('VALOR LIBERADO', row.get('VALOR_LIBERADO', ''))).strip()
                
                # CÃ³digo de tabela e nome de convÃªnio
                cod_convenio = str(row.get('CODIGO TABELA', row.get('CODIGO_TABELA', ''))).strip()
                nome_convenio = cod_convenio  # No CSV exportado, sÃ³ temos o cÃ³digo
                
                nome_tabela_para_busca = cod_convenio
                
            else:
                # Estrutura com Unnamed (XLS original do banco)
                logging.info("ðŸ” DIGIO: Detectada estrutura com colunas Unnamed (XLS original)")
                proposta = str(row.get('Unnamed: 3', '')).strip()
                tipo_operacao = str(row.get('Unnamed: 4', '')).strip()
                data_cadastro = str(row.get('Unnamed: 8', '')).strip()
                situacao = str(row.get('Unnamed: 9', '')).strip()
                data_lancamento = str(row.get('Unnamed: 13', '')).strip()
                nome_orgao_raw = str(row.get('Unnamed: 25', '')).strip()
                usuario_digitador = str(row.get('Unnamed: 29', '')).strip()
                cpf_cliente = str(row.get('Unnamed: 31', '')).strip()
                nome_cliente = str(row.get('Unnamed: 32', '')).strip()
                data_nascimento = str(row.get('Unnamed: 33', '')).strip()
                qtd_parcelas = str(row.get('Unnamed: 48', '')).strip()
                vlr_parcela = str(row.get('Unnamed: 49', '')).strip()
                vlr_financiado = str(row.get('Unnamed: 50', '')).strip()
                
                # DIGIO: Extrair cÃ³digos e nomes de convÃªnio (Unnamed: 53 e 54)
                cod_convenio = str(row.get('Unnamed: 53', '')).strip()  # Ex: 002035, 001717
                nome_convenio = str(row.get('Unnamed: 54', '')).strip()  # Ex: "PORT+REFIN VINCULADO-1-96X-1,39 A 1,85-T"
                
                vlr_lib1 = str(row.get('Unnamed: 59', '')).strip()
                
                # DIGIO: Usar COD_CONVENIO numÃ©rico (ex: 002035 â†’ 2035)
                # Remover zeros Ã  esquerda se houver
                if cod_convenio and cod_convenio.isdigit():
                    cod_convenio = str(int(cod_convenio))  # Remove leading zeros: 002035 â†’ 2035
                
                nome_tabela_para_busca = nome_convenio if nome_convenio else cod_convenio
            
            # Log para debug do DIGIO
            logging.info(f"ðŸ” DIGIO campos principais: Proposta={proposta}, TipoOp='{tipo_operacao}', Orgao='{nome_orgao_raw}'")
            logging.info(f"ðŸ” DIGIO tabela: COD_CONVENIO='{cod_convenio}' | NOME_CONVENIO='{nome_convenio}'")
            logging.info(f"ðŸ” DIGIO QtdParc={qtd_parcelas}, VlrFinanc={vlr_financiado}")

            
            # MELHORADO: DetecÃ§Ã£o inteligente de ORGAO DIGIO baseada no map_relat_atualizados.txt
            def detect_digio_organ(nome_orgao, nome_empregador="", cod_empregador=""):
                """Detecta Ã³rgÃ£o do DIGIO baseado nos campos NOME_ORGAO, NOME_EMPREGADOR"""
                orgao_upper = nome_orgao.upper() if nome_orgao else ""
                empregador_upper = nome_empregador.upper() if nome_empregador else ""
                
                # Log para debug
                logging.info(f"ðŸ›ï¸ DIGIO detectando Ã³rgÃ£o: NOME_ORGAO='{orgao_upper}' | NOME_EMPREGADOR='{empregador_upper}' | COD_EMP='{cod_empregador}'")
                
                # Baseado nos exemplos do mapeamento:
                # NOME_ORGAO: INSS, PREFEITURA DE B, PREFEITURA DE L, PREFEITURA DE S
                # NOME_EMPREGADOR: INSS, PREF BAURU SP, PREF LINS - SP, PREF SERTAOZINH
                
                # Prioridade 1: INSS (mais comum)
                if 'INSS' in orgao_upper or 'INSS' in empregador_upper:
                    return 'INSS'
                
                # Prioridade 2: Prefeituras especÃ­ficas (usar formato do empregador que Ã© mais especÃ­fico)
                if 'BAURU' in empregador_upper:
                    return 'PREF BAURU SP'
                elif 'LINS' in empregador_upper:
                    return 'PREF LINS - SP'
                elif 'AGUDOS' in empregador_upper:
                    return 'PREF AGUDOS - S'
                elif 'JABOTICABA' in empregador_upper:
                    return 'PREF JABOTICABA'
                elif 'SERTAOZINHO' in empregador_upper or 'SERTAOZINH' in empregador_upper:
                    return 'PREF SERTAOZINHO - SP'
                elif 'PREFEITURA DE B' in orgao_upper:
                    return 'PREF BAURU SP'  # B = BAURU
                elif 'PREFEITURA DE L' in orgao_upper:
                    return 'PREF LINS - SP'  # L = LINS
                elif 'PREFEITURA DE S' in orgao_upper:
                    return 'PREF SERTAOZINHO - SP'  # S = SERTAOZINHO
                elif 'PREFEITURA DE A' in orgao_upper:
                    return 'PREF AGUDOS - S'  # A = AGUDOS
                elif 'PREF' in empregador_upper or 'PREFEITURA' in orgao_upper:
                    # Prefeitura genÃ©rica - usar formato do empregador
                    return empregador_upper if empregador_upper else orgao_upper.replace('PREFEITURA', 'PREF').strip()
                
                # Default: INSS
                return 'INSS'
            
            # Se tem estrutura nomeada, usar o campo ORGAO diretamente, senÃ£o detectar
            if not has_unnamed_structure:
                # JÃ¡ temos o Ã³rgÃ£o normalizado no CSV
                nome_orgao = nome_orgao_raw if nome_orgao_raw else 'INSS'
            else:
                # Detectar Ã³rgÃ£o a partir dos campos Unnamed
                nome_empregador = str(row.get('Unnamed: 23', '')).strip()
                cod_empregador = str(row.get('Unnamed: 17', '')).strip()
                nome_orgao = detect_digio_organ(nome_orgao_raw, nome_empregador, cod_empregador)
            
            # MELHORADO: DetecÃ§Ã£o inteligente de tipo de operaÃ§Ã£o DIGIO
            def detect_digio_operation(tipo_op, tabela_nome=""):
                """Detecta tipo de operaÃ§Ã£o do DIGIO de forma inteligente"""
                tipo_upper = tipo_op.upper() if tipo_op else ""
                tabela_upper = tabela_nome.upper() if tabela_nome else ""
                
                logging.info(f"ðŸ”§ DIGIO detectando operaÃ§Ã£o: tipo='{tipo_upper}' | tabela='{tabela_upper[:50]}...'")
                
                # Analisar tanto o tipo quanto o nome da tabela
                combined_text = f"{tipo_upper} {tabela_upper}"
                
                # Prioridade 1: Refinanciamento + Portabilidade (mais especÃ­fico primeiro)
                if any(x in combined_text for x in ['REFIN DA PORT', 'REFINANCIAMENTO DA PORTABILIDADE', 'REFIN PORT', 'REFIN PORTABILIDADE']):
                    return "Refinanciamento da Portabilidade"
                
                # Prioridade 2: Portabilidade + Refin (diferente do anterior)
                elif any(x in combined_text for x in ['PORT+REFIN', 'PORTABILIDADE + REFIN', 'PORT REFIN']):
                    return "Portabilidade + Refin"
                
                # Prioridade 2.5: Portabilidade simples
                elif 'PORTABILIDADE' in combined_text and 'REFIN' not in combined_text:
                    return "Portabilidade"
                
                # Prioridade 3: Refinanciamento simples
                elif 'REFIN' in combined_text and 'PORT' not in combined_text:
                    return "Refinanciamento"
                
                # Prioridade 4: Margem Livre (mais comum)
                else:
                    return "Margem Livre (Novo)"
            
            # âœ… CORRIGIDO: Se estrutura nomeada, usar TIPO DE OPERACAO do arquivo diretamente
            if not has_unnamed_structure:
                # Arquivo jÃ¡ processado tem tipo correto
                tipo_operacao_norm = tipo_operacao if tipo_operacao else "Margem Livre (Novo)"
                logging.info(f"âœ… DIGIO usando tipo de operaÃ§Ã£o do arquivo: '{tipo_operacao_norm}'")
            else:
                # Estrutura XLS original - detectar tipo
                tipo_operacao_norm = detect_digio_operation(tipo_operacao, nome_convenio)
                logging.info(f"ðŸ” DIGIO tipo detectado: '{tipo_operacao_norm}'")
                
            normalized_row = {
                "PROPOSTA": proposta,
                "DATA_CADASTRO": data_cadastro,
                "BANCO": "BANCO DIGIO S.A.",  # Nome completo como no relat_orgaos.csv
                "ORGAO": nome_orgao,
                "TIPO_OPERACAO": tipo_operacao_norm,
                "NUMERO_PARCELAS": qtd_parcelas,
                "VALOR_OPERACAO": vlr_financiado,
                "VALOR_LIBERADO": vlr_lib1 if vlr_lib1 else vlr_financiado,
                "USUARIO_BANCO": usuario_digitador,
                "SITUACAO": situacao,
                "DATA_PAGAMENTO": data_lancamento,
                "CPF": cpf_cliente,
                "NOME": nome_cliente,
                "DATA_NASCIMENTO": data_nascimento,
                "VALOR_PARCELAS": vlr_parcela,
                "CODIGO_TABELA": cod_convenio,  # âœ… DIGIO: Usar COD_CONVENIO direto (5076, 5077, 1720, etc)
                "TAXA": "",  # Taxa deve vir do arquivo ou ser buscada depois
                "OBSERVACOES": str(row.get('Unnamed: 11', row.get('ObservaÃ§Ãµes', ''))).strip()  # NOME_ATIVIDADE como observaÃ§Ã£o
            }
            
            # âœ… DIGIO: NÃƒO aplicar mapeamento! 
            # O arquivo DIGIO jÃ¡ vem com cÃ³digos corretos (5076, 5077, 1720, 2055, etc)
            # Diferente de VCTEX que precisa converter "Tabela EXP" â†’ "TabelaEXP"
            logging.info(f"âœ… DIGIO FINAL: Proposta={proposta} | CÃ³digo='{cod_convenio}' | Ã“rgÃ£o='{normalized_row['ORGAO']}' | OperaÃ§Ã£o='{normalized_row['TIPO_OPERACAO']}'")
            logging.info(f"   â””â”€ Usando cÃ³digo direto do arquivo (SEM mapeamento)")
            
            
        elif bank_type == "PRATA":
            # Mapeamento baseado na estrutura real do Prata
            # PRATA usa prazo em MESES, precisa dividir por 12
            prazo = str(row.get('Prazo proposta', '')).strip()
            numero_parcelas = ""
            if prazo and prazo.isdigit():
                numero_parcelas = str(int(prazo) // 12) if int(prazo) >= 12 else prazo
            
            # PRATA: Pegar campo Usuario e limpar (remover nome entre parÃªnteses)
            usuario_prata = str(row.get('Nome do Vendedor', '')).strip()
            if not usuario_prata:
                usuario_prata = str(row.get('UsuÃ¡rio (acesso login)', '')).strip()
            
            # Limpar: remover tudo apÃ³s o email (ex: "lprodrigues@q-faz.com (LARIANA PITON RODRIGUES)" â†’ "lprodrigues@q-faz.com")
            if '(' in usuario_prata:
                usuario_prata = usuario_prata.split('(')[0].strip()
            
            normalized_row = {
                "PROPOSTA": str(row.get('NÃºmero da Proposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data da operaÃ§Ã£o', '')).strip(),
                "BANCO": "BANCO PRATA DIGITAL",
                "ORGAO": "FGTS",
                "TIPO_OPERACAO": "Margem Livre (Novo)",
                "NUMERO_PARCELAS": numero_parcelas,
                "VALOR_OPERACAO": str(row.get('Valor da EmissÃ£o', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor Desembolso', '')).strip(),
                "USUARIO_BANCO": usuario_prata,
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data do Desembolso', '')).strip(),
                "CPF": str(row.get('CPF do Cliente', '')).strip(),
                "NOME": str(row.get('Nome do Cliente', '')).strip(),
                "DATA_NASCIMENTO": "",
                "VALOR_PARCELAS": "",  # PRATA nÃ£o fornece valor da parcela
                "CODIGO_TABELA": str(row.get('Tabela', '')).strip(),  # Nome da tabela do banco
                "TAXA": "",  # Vazio para buscar no relat_orgaos.csv
                "OBSERVACOES": str(row.get('ObservaÃ§Ãµes', row.get('Observacoes', row.get('Obs', '')))).strip()
            }
            
        elif bank_type == "VCTEX":
            # Mapeamento BANCO VCTEX - Estrutura REAL dos arquivos
            # VCTEX usa prazo em MESES, precisa dividir por 12
            prazo_vctex = str(row.get('Prazo proposta', '')).strip()
            numero_parcelas_vctex = ""
            if prazo_vctex and prazo_vctex.isdigit():
                numero_parcelas_vctex = str(int(prazo_vctex) // 12) if int(prazo_vctex) >= 12 else prazo_vctex
            
            # FunÃ§Ã£o para busca flexÃ­vel de datas VCTEX - CORRIGIDA
            def get_vctex_date_field(row, date_type='cadastro'):
                """Busca flexÃ­vel por campo de data no VCTEX - prioridade corrigida"""
                
                logging.debug(f"ðŸ” VCTEX buscando campo de data tipo '{date_type}' nas colunas disponÃ­veis")
                
                if date_type == 'cadastro':
                    # Prioridade ALTA para campos especÃ­ficos de criaÃ§Ã£o/operaÃ§Ã£o
                    cadastro_high_priority = [
                        'Data da operaÃ§Ã£o', 'Data da operacao', 'Data operacao', 'Data Operacao',
                        'Data de criacao', 'Data de criaÃ§Ã£o', 'Data criacao', 'Data criaÃ§Ã£o',
                        'Data contrato', 'Data Contrato', 'Data do contrato', 'Data do Contrato',
                        'Data inclusao', 'Data inclusÃ£o', 'Data de inclusao', 'Data de inclusÃ£o'
                    ]
                    
                    # Prioridade MÃ‰DIA para campos de cadastro genÃ©ricos  
                    cadastro_medium_priority = [
                        'Data cadastro', 'Data Cadastro', 'Data de cadastro', 'Data de Cadastro',
                        'Data assinatura', 'Data Assinatura', 'DT_OPERACAO', 'DT_CADASTRO', 'DT_CRIACAO'
                    ]
                    
                    # Buscar por prioridade
                    for field_list in [cadastro_high_priority, cadastro_medium_priority]:
                        for field in field_list:
                            if field in row and str(row.get(field, '')).strip() and str(row.get(field, '')).strip() != 'nan':
                                found_date = str(row.get(field, '')).strip()
                                logging.info(f"âœ… VCTEX DATA_CADASTRO encontrada em '{field}': {found_date}")
                                return found_date
                            
                elif date_type == 'pagamento':
                    # Prioridade ALTA para campos especÃ­ficos de pagamento/finalizaÃ§Ã£o
                    pagamento_high_priority = [
                        'Data pagamento OperaÃ§Ã£o', 'Data pagamento Operacao', 'Data pagamento',
                        'Data de pagamento', 'Data Pagamento', 'Data liquidacao', 'Data liquidaÃ§Ã£o',
                        'Data liberacao', 'Data liberaÃ§Ã£o', 'Data credito', 'Data crÃ©dito'
                    ]
                    
                    # Prioridade MÃ‰DIA para campos de finalizaÃ§Ã£o/vencimento
                    pagamento_medium_priority = [
                        'Data finalizacao', 'Data finalizaÃ§Ã£o', 'Data vencimento', 'Data Vencimento',
                        'Data de vencimento', 'Data de Vencimento', 'DT_PAGAMENTO', 'DT_FINALIZACAO', 'DT_LIQUIDACAO'
                    ]
                    
                    # Buscar por prioridade
                    for field_list in [pagamento_high_priority, pagamento_medium_priority]:
                        for field in field_list:
                            if field in row and str(row.get(field, '')).strip() and str(row.get(field, '')).strip() != 'nan':
                                found_date = str(row.get(field, '')).strip()
                                logging.info(f"âœ… VCTEX DATA_PAGAMENTO encontrada em '{field}': {found_date}")
                                return found_date
                
                # ðŸš« EVITAR campos genÃ©ricos que causam confusÃ£o entre cadastro e pagamento
                # Vamos ser mais restritivos para evitar pegar campos errados
                logging.warning(f"âš ï¸ VCTEX: Nenhum campo especÃ­fico de {date_type} encontrado!")
                
                # Log das colunas disponÃ­veis para debug
                available_columns = [col for col in row.index if 'data' in col.lower() or 'date' in col.lower()]
                if available_columns:
                    logging.info(f"ðŸ” VCTEX: Colunas relacionadas a datas disponÃ­veis: {available_columns}")
                
                return ""  # Retornar vazio ao invÃ©s de tentar campo genÃ©rico
            
            # FunÃ§Ã£o para validar e normalizar formato de data - MELHORADA
            def validate_and_normalize_date(date_str, field_name=""):
                """Valida, normaliza e detecta problemas em datas do VCTEX"""
                if not date_str or date_str.strip() == "" or str(date_str).strip().lower() in ['nan', 'none', 'null']:
                    logging.debug(f"ðŸ” VCTEX {field_name}: Campo vazio ou invÃ¡lido")
                    return ""
                
                date_clean = str(date_str).strip()
                
                # Verificar padrÃµes de data vÃ¡lidos com regex mais rigoroso
                import re
                from datetime import datetime
                
                # PadrÃµes aceitos com validaÃ§Ã£o mais rigorosa
                date_patterns = [
                    (r'^\d{1,2}/\d{1,2}/\d{4}$', '%d/%m/%Y'),     # DD/MM/YYYY
                    (r'^\d{1,2}/\d{1,2}/\d{2}$', '%d/%m/%y'),     # DD/MM/YY  
                    (r'^\d{1,2}-\d{1,2}-\d{4}$', '%d-%m-%Y'),     # DD-MM-YYYY
                    (r'^\d{1,2}-\d{1,2}-\d{2}$', '%d-%m-%y'),     # DD-MM-YY
                    (r'^\d{4}-\d{1,2}-\d{1,2}$', '%Y-%m-%d'),     # YYYY-MM-DD
                    (r'^\d{1,2}\.\d{1,2}\.\d{4}$', '%d.%m.%Y'),   # DD.MM.YYYY
                    (r'^\d{4}/\d{1,2}/\d{1,2}$', '%Y/%m/%d')      # YYYY/MM/DD
                ]
                
                # Tentar validar com cada padrÃ£o
                for pattern, date_format in date_patterns:
                    if re.match(pattern, date_clean):
                        try:
                            # Tentar fazer o parse da data para validar se Ã© real
                            parsed_date = datetime.strptime(date_clean, date_format)
                            
                            # Verificar se a data faz sentido (nÃ£o muito antiga nem futura)
                            current_year = datetime.now().year
                            if parsed_date.year < 1990 or parsed_date.year > current_year + 1:
                                logging.warning(f"âš ï¸ VCTEX {field_name}: Ano suspeito ({parsed_date.year}) na data '{date_clean}'")
                            
                            logging.info(f"âœ… VCTEX {field_name}: Data vÃ¡lida '{date_clean}' (formato: {date_format})")
                            return date_clean
                            
                        except ValueError as ve:
                            logging.warning(f"âš ï¸ VCTEX {field_name}: Data invÃ¡lida '{date_clean}' - erro: {ve}")
                            continue
                
                # ðŸ”§ MODO FLEXÃVEL: Se validaÃ§Ã£o rigorosa falhou, aceitar formatos razoÃ¡veis
                logging.warning(f"âš ï¸ VCTEX {field_name}: Formato nÃ£o padrÃ£o: '{date_clean}' - aplicando modo flexÃ­vel")
                
                # ðŸ”§ CORREÃ‡ÃƒO ESPECÃFICA: Tratar timestamps (DD/MM/YYYY HH:MM:SS)
                if len(date_clean) > 10:  # PossÃ­vel timestamp/datetime
                    logging.info(f"ðŸ”§ VCTEX {field_name}: Detectado timestamp: '{date_clean}'")
                    
                    # PadrÃ£o brasileiro: DD/MM/YYYY HH:MM:SS
                    timestamp_br_match = re.match(r'^(\d{1,2}/\d{1,2}/\d{4})\s+\d{1,2}:\d{2}', date_clean)
                    if timestamp_br_match:
                        date_only = timestamp_br_match.group(1)
                        logging.info(f"âœ… VCTEX {field_name}: ExtraÃ­do data brasileira: '{date_only}'")
                        return date_only
                    
                    # PadrÃ£o ISO: YYYY-MM-DD HH:MM:SS
                    timestamp_iso_match = re.match(r'^(\d{4}-\d{2}-\d{2})\s+\d{1,2}:\d{2}', date_clean) 
                    if timestamp_iso_match:
                        date_only = timestamp_iso_match.group(1)
                        logging.info(f"âœ… VCTEX {field_name}: ExtraÃ­do data ISO: '{date_only}'")
                        return date_only
                    
                    # Tentar extrair primeiros 10 caracteres se parecer data
                    if re.match(r'^\d{4}-\d{2}-\d{2}', date_clean):
                        date_part = date_clean[:10]
                        logging.info(f"ðŸ”§ VCTEX {field_name}: Extraindo primeiros 10 chars: '{date_part}'")
                        return date_part
                
                # ðŸ†˜ MODO EMERGÃŠNCIA: Se tem qualquer padrÃ£o de data, ACEITAR!
                # Formatos que podem existir no mundo real
                flexible_patterns = [
                    r'\d{1,4}[/.-]\d{1,2}[/.-]\d{1,4}',  # Qualquer separador com nÃºmeros
                    r'\d{8}',  # DDMMYYYY ou YYYYMMDD
                    r'\d{6}',  # DDMMYY ou YYMMDD
                ]
                
                for pattern in flexible_patterns:
                    if re.search(pattern, date_clean):
                        logging.warning(f"âš ï¸ VCTEX {field_name}: ACEITANDO formato flexÃ­vel: '{date_clean}'")
                        return date_clean  # Aceitar como estÃ¡
                
                # Se realmente nÃ£o parece data de jeito nenhum
                logging.error(f"âŒ VCTEX {field_name}: Realmente nÃ£o parece data: '{date_clean}' - retornando vazio")
                return ""
            
            # Pegar campos brutos
            convenio_raw = str(row.get('ConvÃªnio', row.get('Nome da entidade consignataria', ''))).strip().upper()
            tabela_raw = str(row.get('Tabela', row.get('Nome tabela juros', ''))).strip()
            taxa_raw = str(row.get('Taxa Juros Aplicada', row.get('Taxa de juros', ''))).strip()
            
            # ðŸŽ¯ VCTEX: Usar cÃ³digo EXATO do arquivo para buscar no relat_orgaos.csv
            # O relat_orgaos.csv tem: "TABELA BANCO" (ex: "Tabela EXP") â†’ "CODIGO TABELA STORM" (ex: "TabelaEXP")
            # Preservar tabela_raw EXATAMENTE como vem do arquivo para fazer matching correto
            logging.info(f"ðŸ“‹ VCTEX: Tabela do arquivo: '{tabela_raw}' (serÃ¡ usada para buscar CODIGO TABELA STORM no CSV)")
            
            # Normalizar ORGAO usando CONVENIO e TABELA como indicadores
            orgao_vctex = ""
            
            # Primeiro, tentar pelo campo ConvÃªnio
            if 'FGTS' in convenio_raw or 'FUNDO' in convenio_raw:
                orgao_vctex = 'FGTS'
            elif 'INSS' in convenio_raw or 'PREVID' in convenio_raw:
                orgao_vctex = 'INSS'
            # Se ConvÃªnio nÃ£o ajudou, usar a TABELA como indicador
            elif 'INSS' in tabela_raw.upper():
                orgao_vctex = 'INSS'
            elif 'FGTS' in tabela_raw.upper():
                orgao_vctex = 'FGTS'
            # Detectar por nome de tabela tÃ­picas
            elif any(x in tabela_raw.upper() for x in ['VAMO', 'EXPONENCIAL', 'RELAX', 'VCT', 'EXP']):
                # Se tem tabela tÃ­pica de FGTS e nÃ£o mencionou INSS
                orgao_vctex = 'FGTS'
            else:
                # Default para INSS se nÃ£o conseguiu determinar
                orgao_vctex = 'INSS'
            
            # Garantir que TAXA tenha valor, mesmo que seja 0,00%
            if not taxa_raw or taxa_raw == 'nan':
                taxa_raw = '0,00%'
            elif '%' not in taxa_raw:
                taxa_raw = taxa_raw + '%' if taxa_raw else '0,00%'
            
            # ðŸ” DEBUG: Log das colunas de data disponÃ­veis
            date_columns = [col for col in row.index if any(word in col.lower() for word in ['data', 'date'])]
            logging.info(f"ðŸ” VCTEX Proposta {row.get('NÃºmero do Contrato', 'N/A')}: Colunas de data disponÃ­veis: {date_columns}")
            
            # Buscar datas usando funÃ§Ã£o flexÃ­vel
            data_cadastro_raw = get_vctex_date_field(row, 'cadastro')
            data_pagamento_raw = get_vctex_date_field(row, 'pagamento')
            
            # ðŸ” DEBUG: Log das datas brutas encontradas
            logging.info(f"ðŸ” VCTEX Proposta {row.get('NÃºmero do Contrato', 'N/A')}: Datas brutas - CADASTRO_RAW: '{data_cadastro_raw}' | PAGAMENTO_RAW: '{data_pagamento_raw}'")
            
            # Validar e normalizar datas
            data_cadastro_vctex = validate_and_normalize_date(data_cadastro_raw, "DATA_CADASTRO")
            data_pagamento_vctex = validate_and_normalize_date(data_pagamento_raw, "DATA_PAGAMENTO")
            
            # ðŸ” DEBUG: Log das datas processadas
            logging.info(f"ðŸ” VCTEX Proposta {row.get('NÃºmero do Contrato', 'N/A')}: Datas processadas - CADASTRO_FINAL: '{data_cadastro_vctex}' | PAGAMENTO_FINAL: '{data_pagamento_vctex}'")
            
            # ðŸ”§ CORREÃ‡ÃƒO ROBUSTA: Verificar e corrigir datas trocadas do VCTEX
            if data_cadastro_vctex and data_pagamento_vctex:
                try:
                    from datetime import datetime
                    cadastro_dt = None
                    pagamento_dt = None
                    
                    # Tentar formatos comuns de data brasileira e internacional
                    date_formats = [
                        '%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d', '%d.%m.%Y',  # Formatos bÃ¡sicos
                        '%d/%m/%y', '%d-%m-%y', '%y-%m-%d', '%d.%m.%y',  # Ano de 2 dÃ­gitos
                        '%Y/%m/%d', '%Y.%m.%d'  # Formatos internacionais
                    ]
                    
                    # Tentar converter CADASTRO
                    for fmt in date_formats:
                        try:
                            cadastro_dt = datetime.strptime(data_cadastro_vctex, fmt)
                            break
                        except:
                            continue
                    
                    # Tentar converter PAGAMENTO  
                    for fmt in date_formats:
                        try:
                            pagamento_dt = datetime.strptime(data_pagamento_vctex, fmt)
                            break
                        except:
                            continue
                    
                    # âœ… VALIDAÃ‡ÃƒO: Se conseguiu converter ambas, verificar ordem lÃ³gica
                    if cadastro_dt and pagamento_dt:
                        # Calcular diferenÃ§a em dias
                        diferenca_dias = (pagamento_dt - cadastro_dt).days
                        
                        if diferenca_dias < 0:
                            # Pagamento anterior ao cadastro = ERRO LÃ“GICO!
                            logging.error(f"ðŸ”„ VCTEX Proposta {row.get('NÃºmero do Contrato', 'N/A')}: DATAS LOGICAMENTE INCORRETAS!")
                            logging.error(f"   âŒ CADASTRO: {data_cadastro_vctex} ({cadastro_dt.strftime('%d/%m/%Y')})")
                            logging.error(f"   âŒ PAGAMENTO: {data_pagamento_vctex} ({pagamento_dt.strftime('%d/%m/%Y')})")
                            logging.error(f"   âŒ DiferenÃ§a: {diferenca_dias} dias (IMPOSSÃVEL!)")
                            
                            # CORREÃ‡ÃƒO AUTOMÃTICA: Trocar as datas
                            data_cadastro_vctex, data_pagamento_vctex = data_pagamento_vctex, data_cadastro_vctex
                            logging.warning(f"   ðŸ”§ CORRIGIDO - CADASTRO: {data_cadastro_vctex} | PAGAMENTO: {data_pagamento_vctex}")
                            
                        elif diferenca_dias > 365:
                            # Muito tempo entre cadastro e pagamento = suspeito
                            logging.warning(f"âš ï¸ VCTEX Proposta {row.get('NÃºmero do Contrato', 'N/A')}: DiferenÃ§a suspeita de {diferenca_dias} dias entre cadastro e pagamento")
                        else:
                            # Datas em ordem lÃ³gica
                            logging.info(f"âœ… VCTEX Proposta {row.get('NÃºmero do Contrato', 'N/A')}: Datas em ordem correta ({diferenca_dias} dias)")
                    
                    else:
                        logging.warning(f"âš ï¸ VCTEX: NÃ£o foi possÃ­vel validar formato das datas - CADASTRO: '{data_cadastro_vctex}' | PAGAMENTO: '{data_pagamento_vctex}'")
                        
                except Exception as e:
                    logging.error(f"âŒ VCTEX: Erro ao validar datas: {e}")
            
            # ðŸ“Š LOG COMPLETO das datas VCTEX para debug
            logging.info(f"ðŸ“… VCTEX Proposta {row.get('NÃºmero do Contrato', 'N/A')}: DATAS ORIGINAIS DO ARQUIVO")
            logging.info(f"   âœ… DATA_CADASTRO_FINAL: '{data_cadastro_vctex}'")
            logging.info(f"   âœ… DATA_PAGAMENTO_FINAL: '{data_pagamento_vctex}'")
            logging.info(f"   ðŸ¦ BANCO: VCTEX | Ã“RGÃƒO: {orgao_vctex} | TAXA: {taxa_raw}")
            
            # ðŸ’° VCTEX: FormataÃ§Ã£o de valores no padrÃ£o brasileiro (1.234,56)
            def format_vctex_value(value_str):
                """Formata valores do VCTEX no padrÃ£o brasileiro com ponto e vÃ­rgula"""
                if not value_str or str(value_str).strip() in ['', 'nan', 'NaN', 'None', '0']:
                    return "0,00"
                
                try:
                    # Limpar o valor (remover R$, espaÃ§os, etc.)
                    clean_value = str(value_str).strip().replace('R$', '').replace(' ', '')
                    
                    # Se jÃ¡ estÃ¡ no formato brasileiro, verificar se estÃ¡ correto
                    if ',' in clean_value:
                        # Formato brasileiro: 1.234,56 ou 87,58
                        parts = clean_value.split(',')
                        if len(parts) == 2 and len(parts[1]) == 2:
                            # JÃ¡ estÃ¡ formatado corretamente
                            return clean_value
                        else:
                            # Tem vÃ­rgula mas nÃ£o estÃ¡ formatado corretamente
                            # Converter para ponto e processar
                            clean_value = clean_value.replace('.', '').replace(',', '.')
                    
                    # Converter para float
                    value_float = float(clean_value)
                    
                    # Formatar no padrÃ£o brasileiro
                    if value_float >= 1000:
                        # Valores >= 1000: usar ponto como separador de milhar
                        # Ex: 1.234,56
                        formatted = f"{value_float:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
                    else:
                        # Valores < 1000: sÃ³ vÃ­rgula decimal
                        # Ex: 87,58
                        formatted = f"{value_float:.2f}".replace('.', ',')
                    
                    return formatted
                    
                except (ValueError, TypeError) as e:
                    logging.warning(f"âš ï¸ VCTEX: Erro ao formatar valor '{value_str}': {e}")
                    return str(value_str)  # Retornar original se houver erro
            
            # Formatar valores antes de adicionar ao normalized_row
            valor_operacao_raw = str(row.get('Valor da operacao', str(row.get('Valor Liberado', '')))).strip()
            valor_liberado_raw = str(row.get('Valor Liberado', '')).strip()
            valor_parcela_raw = str(row.get('Parcela', row.get('Valor parcela', ''))).strip()
            
            valor_operacao_formatado = format_vctex_value(valor_operacao_raw)
            valor_liberado_formatado = format_vctex_value(valor_liberado_raw)
            valor_parcela_formatado = format_vctex_value(valor_parcela_raw)
            
            logging.info(f"ðŸ’° VCTEX Proposta {row.get('NÃºmero do Contrato', 'N/A')}: Valores formatados - OP: {valor_operacao_formatado}, LIB: {valor_liberado_formatado}, PARC: {valor_parcela_formatado}")
            
            normalized_row = {
                "PROPOSTA": str(row.get('NÃºmero do Contrato', row.get('Identificacao da operacao', ''))).strip(),
                "DATA_CADASTRO": data_cadastro_vctex,
                "BANCO": "BANCO VCTEX",
                "ORGAO": orgao_vctex,
                "TIPO_OPERACAO": "Margem Livre (Novo)",  # VCTEX normalmente sÃ³ tem esse tipo
                "NUMERO_PARCELAS": numero_parcelas_vctex,
                "VALOR_OPERACAO": valor_operacao_formatado,  # ðŸ’° FORMATADO
                "VALOR_LIBERADO": valor_liberado_formatado,  # ðŸ’° FORMATADO
                "USUARIO_BANCO": str(row.get('UsuÃ¡rio (acesso login)', row.get('CPF Usuario', ''))).strip(),
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": data_pagamento_vctex,
                "CPF": str(row.get('CPF', '')).strip(),
                "NOME": str(row.get('Nome do Cliente', row.get('Nome', ''))).strip(),
                "DATA_NASCIMENTO": str(row.get('Data de nascimento', '')).strip() if 'Data de nascimento' in df.columns else "",
                "VALOR_PARCELAS": valor_parcela_formatado,  # ðŸ’° FORMATADO
                "CODIGO_TABELA": tabela_raw,  # Nome COMPLETO da tabela (usado para buscar no dicionÃ¡rio)
                "TAXA": taxa_raw,  # Taxa do arquivo (mas serÃ¡ substituÃ­da pelo mapeamento se encontrar)
                "OBSERVACOES": str(row.get('ObservaÃ§Ã£o', row.get('ObservaÃ§Ãµes', row.get('Observacoes', row.get('Obs', ''))))).strip()  # Campo observaÃ§Ãµes do VCTEX
            }
            
        elif bank_type == "DAYCOVAL":
            # ðŸ”§ DAYCOVAL - Detectar formato (CSV correto vs Unnamed)
            
            # Verificar se jÃ¡ estÃ¡ no formato CSV correto
            has_correct_columns = any(col in ['PROPOSTA', 'DATA CADASTRO', 'BANCO', 'ORGAO'] for col in row.keys())
            
            if has_correct_columns:
                # âœ… FORMATO CSV CORRETO - Mapear diretamente
                logging.info(f"âœ… DAYCOVAL linha {idx}: FORMATO CSV CORRETO DETECTADO")
                
                normalized_row = {
                    "PROPOSTA": str(row.get('PROPOSTA', '')).strip(),
                    "ADE": str(row.get('PROPOSTA', '')).strip(),  # ADE = mesma proposta
                    "DATA_CADASTRO": str(row.get('DATA CADASTRO', '')).strip(),
                    "BANCO": "BANCO DAYCOVAL",
                    "ORGAO": str(row.get('ORGAO', '')).strip(),
                    "TIPO_OPERACAO": str(row.get('TIPO DE OPERACAO', '')).strip(),
                    "NUMERO_PARCELAS": str(row.get('NUMERO PARCELAS', '')).strip(),
                    "VALOR_OPERACAO": str(row.get('VALOR OPERACAO', '')).strip(),
                    "VALOR_LIBERADO": str(row.get('VALOR LIBERADO', '')).strip(),
                    "USUARIO_BANCO": str(row.get('USUARIO BANCO', '')).strip(),
                    "SITUACAO": str(row.get('SITUACAO', '')).strip(),
                    "DATA_PAGAMENTO": str(row.get('DATA DE PAGAMENTO', '')).strip(),
                    "CPF": str(row.get('CPF', '')).strip(),
                    "NOME": str(row.get('NOME', '')).strip().upper(),
                    "DATA_NASCIMENTO": str(row.get('DATA DE NASCIMENTO', '')).strip(),
                    "CODIGO_TABELA": str(row.get('CODIGO TABELA', '')).strip(),
                    "VALOR_PARCELAS": str(row.get('VALOR PARCELAS', '')).strip(),
                    "TAXA": str(row.get('TAXA', '')).strip(),
                    "OBSERVACOES": f"Processado via CSV correto | {str(row.get('ENDERECO', row.get('ENDEREÃ‡O', '')))}"
                }
                
                logging.info(f"âœ… DAYCOVAL CSV: {normalized_row['PROPOSTA']} | {normalized_row['NOME']} | {normalized_row['SITUACAO']}")
                
            else:
                # âœ… FORMATO ANTIGO - NÃ£o processar por enquanto
                logging.info(f"âš ï¸ DAYCOVAL linha {idx}: FORMATO ANTIGO - nÃ£o processado")  
                normalized_row = None
                # ðŸ”§ FORMATO ANTIGO COM "Unnamed:" - Manter processamento existente
                # Estrutura DAYCOVAL:
                # Unnamed: 0 = NR.PROP., Unnamed: 1 = Tp. OperaÃ§Ã£o, Unnamed: 2 = CLIENTE, Unnamed: 3 = CPF/CNPJ
                # Unnamed: 4 = MATRÃCULA, Unnamed: 5 = DT.CAD., Unnamed: 6 = DT.BASE
                # Unnamed: 11 = Prz. em Meses, Unnamed: 12 = TX.AM, Unnamed: 13 = VLR.LIQ
                # Unnamed: 16 = VLR.OPER, Unnamed: 18 = VLR.PARC, Unnamed: 23 = DESCRIÃ‡ÃƒO EMPREGADOR
                # Unnamed: 27 = SituaÃ§Ã£o_Atual_da_Proposta, Unnamed: 36 = Data da liberaÃ§Ã£o
                
                logging.info(f"=" * 80)
                logging.info(f"ðŸ¦ DAYCOVAL linha {idx}: FORMATO ANTIGO DETECTADO")
                logging.info(f"   Colunas disponÃ­veis: {list(row.keys())[:10]}")
                logging.info(f"=" * 80)
                
                # ðŸ” Debug: Verificar estrutura completa dos valores importantes
                logging.error(f"ðŸ” DAYCOVAL DEBUG - Valores importantes:")
                campos_importantes = [10, 11, 12, 13, 16, 17, 18, 26, 27, 36, 38]
                for i in campos_importantes:
                    col_name = f'Unnamed: {i}'
                    col_value = str(row.get(col_name, 'N/A')).strip()[:30]
                    logging.error(f"   {col_name}: '{col_value}'")
                
                # Extrair campos principais do DAYCOVAL - ajustado para estrutura real
                # Se a primeira coluna nÃ£o Ã© Unnamed: 0, usar o nome real da coluna
                primeira_coluna = 'BANCO DAYCOVAL S/A - Consignado'  # Nome real da primeira coluna
                proposta_raw = str(row.get(primeira_coluna, row.get('Unnamed: 0', ''))).strip()  # NR.PROP.
                tipo_operacao_raw = str(row.get('Unnamed: 1', '')).strip()  # Tp. OperaÃ§Ã£o
                cliente_raw = str(row.get('Unnamed: 2', '')).strip()  # CLIENTE
                cpf_raw = str(row.get('Unnamed: 3', '')).strip()  # CPF/CNPJ
                matricula_raw = str(row.get('Unnamed: 4', '')).strip()  # MATRÃCULA
                data_cadastro_raw = str(row.get('Unnamed: 5', '')).strip()  # DT.CAD.
                data_base_raw = str(row.get('Unnamed: 6', '')).strip()  # DT.BASE
                prazo_meses_raw = str(row.get('Unnamed: 11', '')).strip()  # Prz. em Meses
                taxa_raw = str(row.get('Unnamed: 12', '')).strip()  # TX.AM
                valor_liquido_raw = str(row.get('Unnamed: 13', '')).strip()  # VLR.LIQ
                valor_operacao_raw = str(row.get('Unnamed: 16', '')).strip()  # VLR.OPER
                valor_parcela_raw = str(row.get('Unnamed: 18', '')).strip()  # VLR.PARC
                descricao_empregador_raw = str(row.get('Unnamed: 23', '')).strip()  # DESCRIÃ‡ÃƒO EMPREGADOR
                situacao_raw = str(row.get('Unnamed: 27', '')).strip()  # SituaÃ§Ã£o_Atual_da_Proposta
                data_liberacao_raw = str(row.get('Unnamed: 36', '')).strip()  # Data da liberaÃ§Ã£o
            
            # Normalizar campos para detecÃ§Ã£o
            tipo_op = tipo_operacao_raw.upper()
            orgao_descricao = descricao_empregador_raw.upper()
            
            # Logs detalhados para debug
            logging.info(f"ï¿½ DAYCOVAL extraÃ­do:")
            logging.info(f"   Proposta: {proposta_raw}")
            logging.info(f"   Tipo OperaÃ§Ã£o: {tipo_operacao_raw}")
            logging.info(f"   Cliente: {cliente_raw[:30] if cliente_raw else 'N/A'}...")
            logging.info(f"   CPF: {cpf_raw}")
            logging.info(f"   SituaÃ§Ã£o: {situacao_raw}")
            logging.info(f"   Ã“rgÃ£o: {descricao_empregador_raw}")
            logging.info(f"   Valor OperaÃ§Ã£o: {valor_operacao_raw}")
            
            # FunÃ§Ã£o para detectar Ã³rgÃ£o do DAYCOVAL
            def detect_daycoval_orgao(descricao_empregador):
                """Detecta Ã³rgÃ£o baseado na descriÃ§Ã£o do empregador"""
                desc_upper = descricao_empregador.upper()
                
                if 'INSS' in desc_upper:
                    return "INSS"
                elif 'SPPREV' in desc_upper:
                    return "SPPREV"
                elif 'EDUC' in desc_upper or 'SEC EDU' in desc_upper:
                    return "EDUCACAO"
                elif 'SEFAZ' in desc_upper:
                    return "SEFAZ"
                else:
                    return "INSS"  # Default
            
            # FunÃ§Ã£o para detectar operaÃ§Ã£o do DAYCOVAL  
            def detect_daycoval_operacao(tipo_operacao):
                """Detecta tipo de operaÃ§Ã£o baseado no campo Tp. OperaÃ§Ã£o"""
                tipo_upper = tipo_operacao.upper()
                
                if 'PORTABILIDADE' in tipo_upper and 'REFINANCIAMENTO' in tipo_upper:
                    return "Refinanciamento da Portabilidade"
                elif 'PORTABILIDADE' in tipo_upper:
                    return "Portabilidade + Refin"
                elif 'REFINANCIAMENTO' in tipo_upper:
                    return "Refinanciamento"
                elif 'NOVA' in tipo_upper:
                    return "Margem Livre (Novo)"
                else:
                    return "Margem Livre (Novo)"  # Default
            
            # Detectar Ã³rgÃ£o e operaÃ§Ã£o usando as funÃ§Ãµes
            orgao_detectado = detect_daycoval_orgao(descricao_empregador_raw)
            operacao_detectada = detect_daycoval_operacao(tipo_operacao_raw)
            
            # Verificar se nÃ£o sÃ£o cabeÃ§alhos Ã³bvios (menos restritivo)
            if (proposta_raw.upper() in ['PROPOSTAS CADASTRADAS', 'DETALHADO', 'NR.PROP.'] or
                'PROPOSTAS CADASTRADAS' in proposta_raw.upper()):
                logging.info(f"â­ï¸ DAYCOVAL linha {idx}: Detectado cabeÃ§alho - pulando")
                normalized_row = None
            else:
                # Aplicar formataÃ§Ã£o brasileira
                cpf_formatted = format_cpf_global(cpf_raw)
                valor_operacao_formatted = format_value_brazilian(valor_operacao_raw)
                valor_liberado_formatted = format_value_brazilian(valor_liquido_raw)
                valor_parcela_formatted = format_value_brazilian(valor_parcela_raw)
                taxa_formatted = format_percentage_brazilian(taxa_raw)
            
                logging.info(f"âœ… DAYCOVAL formatado:")
                logging.info(f"   CPF: {cpf_formatted}")
                logging.info(f"   Valor OperaÃ§Ã£o: {valor_operacao_formatted}")
                logging.info(f"   Valor Liberado: {valor_liberado_formatted}")
                logging.info(f"   Ã“rgÃ£o: {orgao_detectado}")
                logging.info(f"   OperaÃ§Ã£o: {operacao_detectada}")
                
                # âœ… SEMPRE CRIAR normalized_row - Deixar validaÃ§Ã£o final decidir
                logging.info(f"âœ… DAYCOVAL linha {idx}: Processando proposta {proposta_raw}")
                
                # Normalizar campos obrigatÃ³rios - Valores seguros
                proposta_final = str(proposta_raw).strip() if proposta_raw and str(proposta_raw).strip() not in ['nan', 'None', ''] else f"DAYC_{idx}"
                nome_final = str(cliente_raw).strip().upper() if cliente_raw and str(cliente_raw).strip() not in ['nan', 'None', ''] else "NOME NAO INFORMADO"
                cpf_final = cpf_formatted if cpf_formatted and cpf_formatted != "000.000.000-00" else "000.000.000-00"
                
                normalized_row = {
                    "PROPOSTA": proposta_final,  # Unnamed: 0
                    "ADE": proposta_final,  # Campo ADE = mesma proposta
                    "DATA_CADASTRO": str(data_cadastro_raw) if data_cadastro_raw else "",  # Unnamed: 5 - DT.CAD.
                    "BANCO": "BANCO DAYCOVAL",
                    "ORGAO": orgao_detectado,  # âœ… Detectado do arquivo
                    "TIPO_OPERACAO": operacao_detectada,  # âœ… Detectado do arquivo  
                    "NUMERO_PARCELAS": str(prazo_meses_raw) if prazo_meses_raw else "0",  # Unnamed: 11 - Prz. em Meses
                    "VALOR_OPERACAO": valor_operacao_formatted,  # âœ… Formatado brasileiro
                    "VALOR_LIBERADO": valor_liberado_formatted,  # âœ… Formatado brasileiro
                    "USUARIO_BANCO": str(row.get('Unnamed: 40', '')).strip(),  # UsuÃ¡rio_Digitador
                    "SITUACAO": str(situacao_raw) if situacao_raw else "",  # Unnamed: 27 - SituaÃ§Ã£o_Atual_da_Proposta
                    "DATA_PAGAMENTO": str(data_liberacao_raw) if data_liberacao_raw else "",  # Unnamed: 36 - Data da liberaÃ§Ã£o
                    "CPF": cpf_final,  # âœ… Formatado brasileiro (XXX.XXX.XXX-XX)
                    "NOME": nome_final,  # âœ… MaiÃºsculas
                    "DATA_NASCIMENTO": "",  # NÃ£o disponÃ­vel no DAYCOVAL
                    "CODIGO_TABELA": str(row.get('Unnamed: 38', '')).strip() if row.get('Unnamed: 38') else "",  # CÃ³digo da tabela
                    "VALOR_PARCELAS": valor_parcela_formatted,  # âœ… Formatado brasileiro
                    "TAXA": taxa_formatted,  # âœ… Formatado brasileiro (X,XX%)
                    "OBSERVACOES": f"MatrÃ­cula: {matricula_raw} | Forma LiberaÃ§Ã£o: {str(row.get('Unnamed: 32', '')).strip()} | {str(row.get('Unnamed: 29', '')).strip()}"
                }
                
                logging.info(f"âœ…âœ…âœ… DAYCOVAL normalized_row criado com sucesso para proposta: {proposta_final}")
                logging.info(f"âœ…âœ…âœ… DAYCOVAL normalized_row: PROPOSTA={proposta_final}, NOME={nome_final}, CPF={cpf_final}")
            
        elif bank_type == "SANTANDER":
            # ðŸ¦ BANCO SANTANDER - Processamento simplificado
            # Campos reais: COD, COD. BANCO, CPF, CLIENTE, CONVENIO, PRODUTO, QTDE PARCELAS, 
            #               VALOR BRUTO, VALOR LIQUIDO, STATUS, DATA, DATA AVERBACAO, COD DIGITADOR
            
            logging.info(f"=" * 80)
            logging.info(f"ðŸ¦ SANTANDER linha {idx}: INICIANDO PROCESSAMENTO")
            logging.info(f"   Colunas disponÃ­veis: {list(row.keys())[:15]}")
            logging.info(f"=" * 80)
            
            import re
            
            # ðŸ”§ FunÃ§Ãµes auxiliares
            def extract_santander_codigo_tabela(produto_str):
                """Extrai cÃ³digo tabela do produto (ex: '810021387' de '21387 - 810021387 - OFERTA')"""
                if not produto_str:
                    return ""
                
                parts = str(produto_str).split(' - ')
                if len(parts) >= 2 and parts[1].strip().isdigit():
                    return parts[1].strip()
                
                # Buscar nÃºmero longo
                numbers = re.findall(r'\d{6,}', str(produto_str))
                return numbers[0] if numbers else ""
            
            def format_santander_value(value_str):
                """Formata valores no padrÃ£o brasileiro"""
                if not value_str or str(value_str).strip() in ['', 'nan', 'NaN', 'None']:
                    return "0,00"
                
                try:
                    value_clean = str(value_str).replace(',', '.')
                    value_float = float(value_clean)
                    
                    if value_float >= 1000:
                        return f"{value_float:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
                    else:
                        return f"{value_float:.2f}".replace('.', ',')
                except (ValueError, TypeError):
                    return str(value_str)
            
            def normalize_santander_status(status_str):
                """Normaliza status para padrÃ£o Storm"""
                if not status_str:
                    return "AGUARDANDO"
                
                # Limpar caracteres especiais e encoding problems
                import unicodedata
                status_clean = str(status_str).strip()
                
                # Remover caracteres de encoding corrompido (ï¿½ e similares)
                status_clean = ''.join(c for c in status_clean if ord(c) < 65536 and c.isprintable() or c.isspace())
                
                # Normalizar para maiÃºsculas e remover acentos
                status_clean = ''.join(
                    c for c in unicodedata.normalize('NFD', status_clean.upper())
                    if unicodedata.category(c) != 'Mn'
                )
                
                # Verificar palavras-chave
                if any(palavra in status_clean for palavra in ['PAG', 'AVERBAD', 'LIBERAD', 'DESEMBOLSA']):
                    return "PAGO"
                elif any(palavra in status_clean for palavra in ['CANCEL', 'REPROV', 'REJEITA', 'NEGAD']):
                    return "CANCELADO"
                elif any(palavra in status_clean for palavra in ['AGUARD', 'ANALISE', 'PENDENT', 'ABERTO', 'DIGITAL']):
                    return "AGUARDANDO"
                else:
                    return "AGUARDANDO"  # PadrÃ£o se nÃ£o reconhecer
            
            # Extrair campos do arquivo
            proposta = str(row.get('COD. BANCO', row.get('COD', row.get('PROPOSTA', '')))).strip()
            cliente = str(row.get('CLIENTE', row.get('NOME', ''))).strip()
            cpf = str(row.get('CPF', '')).strip()
            convenio = str(row.get('CONVENIO', '')).strip().upper()
            produto = str(row.get('PRODUTO', '')).strip()
            parcelas = str(row.get('QTDE PARCELAS', row.get('NUMERO PARCELAS', '96'))).strip()
            valor_bruto = str(row.get('VALOR BRUTO', row.get('VALOR OPERACAO', '0'))).strip()
            valor_liquido = str(row.get('VALOR LIQUIDO', row.get('VALOR LIBERADO', '0'))).strip()
            valor_parcela = str(row.get('VALOR PARCELA', row.get('VALOR PARCELAS', '0'))).strip()
            data_cadastro = str(row.get('DATA', row.get('DATA CADASTRO', ''))).strip()
            status = str(row.get('STATUS', row.get('SITUACAO', 'AGUARDANDO'))).strip()
            data_averbacao = str(row.get('DATA AVERBACAO', row.get('DATA DE PAGAMENTO', ''))).strip()
            cod_digitador = str(row.get('COD DIGITADOR NO BANCO', row.get('USUARIO BANCO', ''))).strip()
            
            logging.info(f"ðŸ“‹ SANTANDER extraÃ­do: Proposta={proposta}, Cliente={cliente[:20] if cliente else 'N/A'}, CPF={cpf[:6] if cpf else 'N/A'}...")
            logging.info(f"ðŸ” SANTANDER validaÃ§Ãµes: proposta='{proposta}', convenio='{convenio[:30] if convenio else 'N/A'}', produto='{produto[:50] if produto else 'N/A'}'")
            
            # âœ… VALIDAÃ‡ÃƒO: Verificar se linha deve ser processada
            should_process = True
            
            if not proposta or proposta.upper() in ['NAN', 'NONE', '', 'COD. BANCO']:
                logging.info(f"â­ï¸ SANTANDER linha {idx}: Pulando - proposta vazia ou cabeÃ§alho")
                should_process = False
                normalized_row = None
            
            # ðŸš« FILTRO ESPECIAL: Propostas SEGURO tÃªm 'S' no final do COD. BANCO
            elif proposta.upper().endswith('S') and len(proposta) > 5:
                # Verificar se Ã© realmente SEGURO (nÃ£o um cÃ³digo normal que termina com S)
                if 'SEGURO' in convenio or 'SEGURO' in produto.upper():
                    logging.info(f"ðŸš« SANTANDER linha {idx}: Filtrando - proposta SEGURO com 'S' no final ({proposta})")
                    should_process = False
                    normalized_row = None
                else:
                    logging.info(f"âœ“ SANTANDER linha {idx}: Proposta termina com 'S' mas NÃƒO Ã© SEGURO - vai processar")
            
            logging.info(f"ðŸ“Š SANTANDER linha {idx}: should_process = {should_process}")
            
            # Processar apenas se nÃ£o foi filtrado
            if should_process:
                # Extrair cÃ³digo tabela
                codigo_tabela = extract_santander_codigo_tabela(produto)
                
                logging.info(f"ðŸ” SANTANDER linha {idx}: cÃ³digo extraÃ­do = '{codigo_tabela}' de produto: {produto[:50] if produto else 'N/A'}...")
                
                # Detectar Ã³rgÃ£o
                if 'PREF' in convenio or 'AGUDOS' in convenio or 'RANCHARIA' in convenio:
                    orgao = 'PREF. DE AGUDOS - SP' if 'AGUDOS' in convenio else 'PREF. DE RANCHARIA - SP'
                elif 'LINS' in convenio:
                    orgao = 'PREF. DE LINS - SP'
                else:
                    orgao = 'INSS'
                
                logging.info(f"ðŸ›ï¸ SANTANDER linha {idx}: Ã³rgÃ£o = {orgao} (convÃªnio: {convenio[:30] if convenio else 'N/A'}...)")
                
                # Detectar tipo de operaÃ§Ã£o
                produto_upper = produto.upper()
                if 'REFIN' in produto_upper:
                    tipo_operacao = 'REFINANCIAMENTO'
                elif 'PORT' in produto_upper:
                    tipo_operacao = 'PORTABILIDADE'
                else:
                    tipo_operacao = 'MARGEM LIVRE (NOVO)'
                
                logging.info(f"ðŸ”§ SANTANDER linha {idx}: tipo_operacao = {tipo_operacao}")
                
                # ðŸš« FILTRO: Remover propostas SEGURO (11111111)
                has_seguro_codigo = codigo_tabela and '11111111' in codigo_tabela
                has_seguro_produto = False  # Desativado - estava filtrando tudo
                is_pure_seguro = 'SEGURO' in produto_upper and not any(p in produto_upper for p in ['OFERTA', 'REFIN', 'PORT'])
                has_todos_convenios = 'TODOS OS CONVENIOS' in produto_upper
                
                logging.info(f"ï¿½ SANTANDER linha {idx}: Verificando SEGURO - has_seguro_codigo={has_seguro_codigo}, has_seguro_produto={has_seguro_produto}, has_todos_convenios={has_todos_convenios}")
                
                if codigo_tabela and (has_seguro_codigo or is_pure_seguro or has_todos_convenios):
                    logging.info(f"FILTRADO por SEGURO - proposta {proposta}")
                    normalized_row = None
                else:
                    logging.info(f"PASSOU nos filtros - vai criar normalized_row")
                    
                    # Criar registro normalizado apenas se passou nos filtros
                    normalized_row = {
                        "PROPOSTA": proposta,
                        "DATA_CADASTRO": data_cadastro,
                        "BANCO": "BANCO SANTANDER",
                        "ORGAO": orgao,
                        "TIPO_OPERACAO": tipo_operacao,
                        "NUMERO_PARCELAS": parcelas,
                        "VALOR_OPERACAO": format_santander_value(valor_bruto),
                        "VALOR_LIBERADO": format_santander_value(valor_liquido),
                        "USUARIO_BANCO": cod_digitador,
                        "SITUACAO": normalize_santander_status(status),
                        "DATA_PAGAMENTO": data_averbacao if normalize_santander_status(status) == "PAGO" else "",
                        "CPF": cpf,
                        "NOME": cliente.upper(),
                        "DATA_NASCIMENTO": "",
                        "CODIGO_TABELA": codigo_tabela,
                        "VALOR_PARCELAS": format_santander_value(valor_parcela),
                        "TAXA": "0,00%",
                        "OBSERVACOES": ""
                    }
                    
                    logging.info(f"âœ…âœ…âœ… SANTANDER linha {idx}: normalized_row CRIADO! Proposta={proposta} | CÃ³digo={codigo_tabela} | Ã“rgÃ£o={orgao} | Status={normalize_santander_status(status)}")
                    logging.info(f"ðŸ“¦ SANTANDER linha {idx}: normalized_row pronto para validaÃ§Ã£o comum")
            else:
                # Se should_process=False, garantir que normalized_row=None jÃ¡ foi definido
                logging.info(f"â­ï¸ SANTANDER linha {idx}: should_process=False, normalized_row=None")
                if 'normalized_row' not in locals():
                    normalized_row = None
        
        elif bank_type == "CREFAZ":
            # Mapeamento BANCO CREFAZ - Campos reais baseados no mapeamento
            # Colunas reais: Data Cadastro, NÃºmero da Proposta, CPF, Cliente, Cidade, Status, Agente, etc.
            
            # ðŸ’° FUNÃ‡ÃƒO para formatar valores no padrÃ£o brasileiro
            def format_crefaz_value(value_str):
                """Converte valores para formato brasileiro: 1.255,00"""
                if not value_str or str(value_str).strip() in ['', 'nan', 'None', '0']:
                    return "0,00"
                
                try:
                    # Limpar o valor (remover espaÃ§os, moeda, etc.)
                    clean_value = str(value_str).strip().replace('R$', '').replace(' ', '')
                    
                    # Se jÃ¡ estÃ¡ no formato brasileiro, manter
                    if ',' in clean_value and clean_value.count(',') == 1:
                        parts = clean_value.split(',')
                        if len(parts[1]) == 2:  # Duas casas decimais apÃ³s vÃ­rgula
                            return clean_value
                    
                    # Converter do formato americano (ponto decimal)
                    if '.' in clean_value:
                        # Separar parte inteira e decimal
                        parts = clean_value.split('.')
                        integer_part = parts[0]
                        decimal_part = parts[1][:2] if len(parts) > 1 else "00"
                    else:
                        # Sem decimal, assumir valor inteiro
                        integer_part = clean_value
                        decimal_part = "00"
                    
                    # Garantir que decimal tenha 2 dÃ­gitos
                    if len(decimal_part) == 1:
                        decimal_part += "0"
                    elif len(decimal_part) == 0:
                        decimal_part = "00"
                    
                    # Converter para float para formatar
                    float_value = float(f"{integer_part}.{decimal_part}")
                    
                    # Formatar no padrÃ£o brasileiro: 1.255,00
                    if float_value >= 1000:
                        # Valores >= 1000: usar ponto para milhar
                        formatted = f"{float_value:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
                    else:
                        # Valores < 1000: apenas vÃ­rgula decimal
                        formatted = f"{float_value:.2f}".replace('.', ',')
                    
                    return formatted
                    
                except (ValueError, TypeError) as e:
                    logging.warning(f"âš ï¸ CREFAZ: Erro ao formatar valor '{value_str}': {e}")
                    return str(value_str)  # Retornar original se houver erro
            
            # Extrair campos
            proposta = str(row.get('NÃºmero da Proposta', row.get('Proposta', ''))).strip()
            cod_operacao = str(row.get('Cod OperaÃ§Ã£o', row.get('Tabela', ''))).strip()
            produto = str(row.get('Produto', '')).strip()
            
            # âœ… VALIDAÃ‡ÃƒO: Pular linhas com cÃ³digo de operaÃ§Ã£o vazio
            if not cod_operacao or cod_operacao.upper() in ['NAN', 'NONE', '']:
                logging.info(f"â­ï¸ CREFAZ: Pulando proposta {proposta} - cÃ³digo de operaÃ§Ã£o vazio")
                continue
            
            # Extrair cÃ³digo digitador
            agente = str(row.get('Agente', row.get('Login Agente', ''))).strip()
            codigo_digitador = str(row.get('Codigo Digitador', row.get('CÃ³digo Digitador', ''))).strip()
            usuario_banco = codigo_digitador if codigo_digitador else agente
            
            # ðŸ” CREFAZ: Detectar Ã“RGÃƒO baseado no CÃ“DIGO (nÃ£o no produto)
            # Os cÃ³digos jÃ¡ vÃªm corretos do arquivo: ENER, CPAUTO, LUZ, BOL, CSD
            cod_upper = cod_operacao.upper()
            
            # Mapear Ã³rgÃ£o baseado no cÃ³digo
            if cod_upper in ['ENER', 'LUZ']:
                orgao = 'ENERGIA'
            elif cod_upper in ['BOL', 'BOLETO']:
                orgao = 'BOLETO'
            elif cod_upper in ['CPAUTO', 'AUTO', 'VEICULO']:
                orgao = 'VEICULOS'
            elif cod_upper in ['CSD', 'CLT', 'TRABALHADOR']:
                orgao = 'CRÃ‰DITO DO TRABALHADOR'
            else:
                # Se cÃ³digo desconhecido, tentar detectar pelo produto
                produto_upper = produto.upper()
                if 'ENERGIA' in produto_upper or 'LUZ' in produto_upper or 'FATURA' in produto_upper:
                    orgao = 'ENERGIA'
                elif 'BOLETO' in produto_upper:
                    orgao = 'BOLETO'
                elif 'VEICULO' in produto_upper or 'AUTO' in produto_upper or 'CARRO' in produto_upper:
                    orgao = 'VEICULOS'
                elif 'TRABALHADOR' in produto_upper or 'CLT' in produto_upper or 'PRIVADO' in produto_upper:
                    orgao = 'CRÃ‰DITO DO TRABALHADOR'
                else:
                    orgao = 'ENERGIA'  # Default
            
            tipo_operacao = 'Margem Livre (Novo)'  # CREFAZ sempre Ã© margem livre
            
            # ðŸ’° Formatar valores no padrÃ£o brasileiro
            valor_operacao_br = format_crefaz_value(row.get('Valor Liberado', row.get('Valor Solicitado', '')))
            valor_liberado_br = format_crefaz_value(row.get('Valor Liberado', ''))
            valor_parcela_br = format_crefaz_value(row.get('Valor da Parcela', row.get('Parcela', '')))
            
            # Criar registro normalizado
            normalized_row = {
                "PROPOSTA": proposta,
                "DATA_CADASTRO": str(row.get('Data Cadastro', '')).strip(),
                "BANCO": "BANCO CREFAZ",
                "ORGAO": orgao,
                "TIPO_OPERACAO": tipo_operacao,
                "NUMERO_PARCELAS": str(row.get('Prazo', '')).strip(),
                "VALOR_OPERACAO": valor_operacao_br,  # ðŸ’° FORMATO BRASILEIRO
                "VALOR_LIBERADO": valor_liberado_br,  # ðŸ’° FORMATO BRASILEIRO
                "USUARIO_BANCO": usuario_banco,
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('AlteraÃ§Ã£o', row.get('Data Pagamento', ''))).strip(),
                "CPF": str(row.get('CPF', '')).strip(),
                "NOME": str(row.get('Cliente', row.get('Nome', ''))).strip(),
                "DATA_NASCIMENTO": "",
                "CODIGO_TABELA": cod_operacao,  # âœ… Usar cÃ³digo diretamente do arquivo (ENER, CPAUTO, LUZ, BOL, CSD)
                "VALOR_PARCELAS": valor_parcela_br,  # ðŸ’° FORMATO BRASILEIRO
                "TAXA": "0,00%",  # CREFAZ nÃ£o tem taxa no relat_orgaos (sempre 0,00%)
                "OBSERVACOES": str(row.get('Motivos', row.get('Observacoes', ''))).strip()
            }
            
            logging.info(f"âœ… CREFAZ processado: Proposta={proposta} | CÃ³digo='{cod_operacao}' | Ã“rgÃ£o='{orgao}'")

        
        elif bank_type == "QUERO_MAIS":
            # Mapeamento BANCO QUERO MAIS CREDITO - ESTRUTURA REAL IDENTIFICADA
            
            # Log para debug das colunas disponÃ­veis
            logging.info(f"ðŸ¦ QUERO MAIS - Colunas disponÃ­veis: {list(row.keys())[:15]}...")
            
            # ESTRUTURA REAL baseada no map_relat_atualizados.txt:
            # Unnamed: 11 = CPF Cliente (213.651.558-62, 141.255.778-03)
            # Unnamed: 19 = Data Cadastro (03/09/2025, 05/09/2025) 
            # Unnamed: 20 = Data Nasc. (10/12/1969, 24/10/1958)
            # Unnamed: 22 = Descr. Tabela (INSS CARTÃƒO BENEFÃCIO_LOAS_CCC, INSS_RMC_ QUERO MAIS_CCC)
            # Unnamed: 24 = Descr. Empregador (INSS BENEFICIO, INSS RMC, GOV SÃƒO PAULO)
            # Unnamed: 25 = Descr. Orgao (INSS BENEFICIO, INSS RMC, GOV SÃƒO PAULO)
            # Unnamed: 33 = Proposta (601967573, 601972997)
            # Unnamed: 37 = Nome do Agente (GRUPO QFZ)
            # Unnamed: 38 = Cliente (ADRIANA NATALINA RANCURA)
            # Unnamed: 40 = Nome usu. cad. Proposta (02579846158_901064) - USUÃRIO COM _
            # Unnamed: 42 = Qtd Parcelas (96)
            # Unnamed: 46 = Tabela utilizada (004713, 006640) - CÃ“DIGO DA TABELA!
            # Unnamed: 48 = Vlr.da parcela (53.13, 194.36)
            # Unnamed: 49 = Valor liberacao 1 (1829.79, 1717.23)
            
            # DetecÃ§Ã£o de Ã³rgÃ£o pela descriÃ§Ã£o correta
            descr_orgao = str(row.get('Unnamed: 25', '')).strip().upper()  # Descr. Orgao
            descr_empregador = str(row.get('Unnamed: 24', '')).strip().upper()  # Descr. Empregador
            
            orgao_text = f"{descr_orgao} {descr_empregador}"
            if 'INSS' in orgao_text:
                orgao = 'INSS'
            elif 'GOV' in orgao_text or 'SÃƒO PAULO' in orgao_text or 'SP' in orgao_text:
                orgao = 'SIAPE'
            elif 'FGTS' in orgao_text:
                orgao = 'FGTS'
            else:
                orgao = 'INSS'  # Default
            
            # Campos mapeados corretamente
            proposta = str(row.get('Unnamed: 33', '')).strip()  # Proposta
            cpf_cliente = str(row.get('Unnamed: 11', '')).strip()  # CPF Cliente
            nome_cliente = str(row.get('Unnamed: 38', '')).strip()  # Cliente
            data_cadastro = str(row.get('Unnamed: 19', '')).strip()  # Data Cadastro
            data_nascimento = str(row.get('Unnamed: 20', '')).strip()  # Data Nasc.
            qtd_parcelas = str(row.get('Unnamed: 42', '')).strip()  # Qtd Parcelas
            codigo_tabela_raw = str(row.get('Unnamed: 46', '')).strip()  # Tabela utilizada (CÃ“DIGO!)
            # Formatar cÃ³digo de tabela com zeros Ã  esquerda (6 dÃ­gitos)
            codigo_tabela = codigo_tabela_raw.zfill(6) if codigo_tabela_raw.isdigit() else codigo_tabela_raw
            valor_parcela = str(row.get('Unnamed: 48', '')).strip()  # Vlr.da parcela
            valor_liberado = str(row.get('Unnamed: 49', '')).strip()  # Valor liberacao 1
            descr_tabela = str(row.get('Unnamed: 22', '')).strip()  # Descr. Tabela
            usuario_cadastro = str(row.get('Unnamed: 40', '')).strip()  # Nome usu. cad. Proposta (com _)
            
            # DeterminaÃ§Ã£o do tipo de operaÃ§Ã£o baseado na descriÃ§Ã£o da tabela
            tipo_operacao = "Margem Livre (Novo)"  # Default
            if descr_tabela:
                descr_upper = descr_tabela.upper()
                if "CARTAO" in descr_upper or "CARTÃƒO" in descr_upper:
                    if "SAQUE" in descr_upper:
                        tipo_operacao = "Cartao c/ saque"  # Sem acentos para evitar corrupÃ§Ã£o
                    else:
                        tipo_operacao = "Cartao s/ saque"  # Sem acentos para evitar corrupÃ§Ã£o
                elif "RMC" in descr_upper:
                    tipo_operacao = "RMC"
                elif "LOAS" in descr_upper:
                    tipo_operacao = "Margem Livre (LOAS)"
            
            # Remover zeros Ã  esquerda do cÃ³digo de tabela (004717 â†’ 4717)
            codigo_tabela_original = str(row.get('Unnamed: 46', '')).strip()
            codigo_tabela_final = codigo_tabela_original.lstrip('0') if codigo_tabela_original else ''
            # Se ficou vazio apÃ³s remover zeros, manter o original
            if not codigo_tabela_final:
                codigo_tabela_final = codigo_tabela_original
            
            # Manter formato original do usuÃ¡rio (jÃ¡ vem correto do banco com underscore)
            usuario_final = usuario_cadastro  # Manter formato original: 36057733894_901064
            
            normalized_row = {
                "PROPOSTA": proposta,
                "DATA_CADASTRO": data_cadastro,
                "BANCO": "BANCO QUERO MAIS CREDITO",
                "ORGAO": orgao,
                "TIPO_OPERACAO": tipo_operacao,  # Determinado pela descriÃ§Ã£o da tabela
                "NUMERO_PARCELAS": qtd_parcelas,
                "VALOR_OPERACAO": valor_liberado,  # Usar valor liberado como operaÃ§Ã£o
                "VALOR_LIBERADO": valor_liberado,
                "USUARIO_BANCO": usuario_final,  # UsuÃ¡rio com formato correto (com _)
                "SITUACAO": "DIGITADA",  # âœ… MANUAL conforme solicitado 
                "DATA_PAGAMENTO": "",   # âœ… MANUAL conforme solicitado (sempre vazio)
                "CPF": cpf_cliente,
                "NOME": nome_cliente,
                "DATA_NASCIMENTO": data_nascimento,
                "CODIGO_TABELA": codigo_tabela_final,  # CÃ³digo sem zeros Ã  esquerda (4717)
                "VALOR_PARCELAS": valor_parcela,
                "TAXA": "0,00%",  # Taxa fixa para QUERO MAIS
                "OBSERVACOES": descr_tabela  # DescriÃ§Ã£o da tabela como observaÃ§Ã£o
            }
            
            # Log para debug dos valores mapeados
            logging.info(f"âœ… QUERO MAIS mapeado: PROPOSTA={proposta}, ORGAO={orgao}, CPF={cpf_cliente}, TIPO_OP={tipo_operacao}")
        
        elif bank_type == "PAN":
            # Mapeamento BANCO PAN - Estrutura de cartÃ£o e saque
            normalized_row = {
                "PROPOSTA": str(row.get('NÂº Proposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data do Cadastro', '')).strip(),
                "BANCO": "BANCO PAN",
                "ORGAO": str(row.get('Nome do Ã“rgÃ£o', row.get('Nome do Empregador', 'INSS'))).strip(),
                "TIPO_OPERACAO": str(row.get('Tipo de OperaÃ§Ã£o', 'CartÃ£o')).strip(),
                "NUMERO_PARCELAS": str(row.get('Quantidade de Parcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor Financiado', '')).strip(),
                "VALOR_LIBERADO": str(row.get('VLR_LIB1', row.get('Valor Financiado', ''))).strip(),
                "USUARIO_BANCO": str(row.get('Nome do UsuÃ¡rio Digitador', '')).strip(),
                "SITUACAO": str(row.get('Nome da Atividade', row.get('SituaÃ§Ã£o da Proposta', ''))).strip(),
                "DATA_PAGAMENTO": str(row.get('Data do LanÃ§amento', '')).strip(),
                "CPF": str(row.get('CPF do Cliente', '')).strip(),
                "NOME": str(row.get('Nome do Cliente', '')).strip(),
                "DATA_NASCIMENTO": str(row.get('Data de Nascimento', '')).strip(),
                "CODIGO_TABELA": str(row.get('Nome do ConvÃªnio', row.get('CÃ³digo do ConvÃªnio', ''))).strip(),
                "VALOR_PARCELAS": str(row.get('Valor da Parcela', '')).strip(),
                "TAXA": "",
                "OBSERVACOES": str(row.get('ObservaÃ§Ãµes', row.get('Observacoes', row.get('Obs', '')))).strip()
            }
        
        elif bank_type == "C6":
            # Mapeamento BANCO C6 BANK - Campos reais do mapeamento
            # Colunas reais: Nome Entidade, Numero do Contrato, Proposta, Data da operacao, etc.
            
            # Detectar Ã³rgÃ£o pelo Nome Entidade
            nome_entidade = str(row.get('Nome Entidade', '')).strip().upper()
            if 'INSS' in nome_entidade:
                orgao = 'INSS'
            elif 'FGTS' in nome_entidade:
                orgao = 'FGTS'
            else:
                orgao = 'INSS'  # Default
            
            normalized_row = {
                "PROPOSTA": str(row.get('NÃºmero do Contrato', row.get('Proposta', row.get('Numero do Contrato', '')))).strip(),
                "DATA_CADASTRO": str(row.get('Data da operaÃ§Ã£o', row.get('Data da operacao', row.get('Data Cadastro', '')))).strip(),
                "BANCO": "BANCO C6 BANK",
                "ORGAO": orgao,
                "TIPO_OPERACAO": str(row.get('Produto', 'Margem Livre (Novo)')).strip(),
                "NUMERO_PARCELAS": str(row.get('Prazo proposta', row.get('Parcelas', ''))).strip(),
                "VALOR_OPERACAO": str(row.get('Valor Liberado', row.get('Valor', ''))).strip(),
                "VALOR_LIBERADO": str(row.get('Valor Liberado', '')).strip(),
                "USUARIO_BANCO": str(row.get('UsuÃ¡rio (acesso login)', row.get('Usuario', ''))).strip(),
                "SITUACAO": str(row.get('Status', row.get('Situacao', ''))).strip(),
                "DATA_PAGAMENTO": str(row.get('Data pagamento OperaÃ§Ã£o', row.get('Data Pagamento', ''))).strip(),
                "CPF": str(row.get('CPF', '')).strip(),
                "NOME": str(row.get('Nome do Cliente', row.get('Nome', ''))).strip(),
                "DATA_NASCIMENTO": str(row.get('Data Nascimento', '')).strip(),
                "CODIGO_TABELA": str(row.get('Tabela', '')).strip(),
                "VALOR_PARCELAS": str(row.get('Parcela', row.get('Valor Parcela', ''))).strip(),
                "TAXA": str(row.get('Taxa Juros Aplicada', '')).strip(),
                "OBSERVACOES": str(row.get('ObservaÃ§Ã£o', row.get('Observacoes', ''))).strip()
            }
        
        elif bank_type == "FACTA92":
            # Mapeamento FACTA92 - Estrutura REAL baseada em map_relat_atualizados.txt
            # Colunas principais: CODIGO, NM_CLIENT, CPF, VL_LIQUIDO, VL_BRUTO, NUMEROPRESTACAO, DS_TABCOM, CORRETOR
            
            # Proposta pode estar em CODIGO, PROPOSTA ou NUMERO_CONTRATO
            proposta = str(row.get('CODIGO', row.get('PROPOSTA', row.get('NUMERO_CONTRATO', '')))).strip()
            
            # Nome do cliente em NM_CLIENT
            nome = str(row.get('NM_CLIENT', row.get('NOME', row.get('CLIENTE', '')))).strip()
            
            # CPF
            cpf = str(row.get('CPF', '')).strip()
            
            # Valores (mapeamento melhorado baseado no arquivo real)
            vl_liquido = str(row.get('VL_LIQUIDO', row.get('VALOR_LIBERADO', row.get('VLR_LIBERADO', '')))).strip()
            vl_bruto = str(row.get('VL_BRUTO', row.get('VALOR_OPERACAO', row.get('VLR_FINANCIADO', '')))).strip()
            vl_parcela = str(row.get('VL_PARCELA', row.get('VALOR_PARCELA', row.get('VLR_PRESTACAO', '')))).strip()
            
            # Parcelas
            num_parcelas = str(row.get('NUMEROPRESTACAO', row.get('NUMERO_PARCELAS', row.get('PARCELAS', row.get('QTD_PARCELAS', ''))))).strip()
            
            # Data de nascimento (se disponÃ­vel)
            data_nascimento = str(row.get('DATA_NASCIMENTO', row.get('DT_NASCIMENTO', ''))).strip()
            
            # Status/SituaÃ§Ã£o
            situacao = str(row.get('STATUS', row.get('SITUACAO', row.get('SITUACAO_PROPOSTA', '')))).strip()
            if not situacao:
                situacao = "PAGO"  # FACTA92 default para pagos
            
            # Tabela - DS_TABCOM tem formato: "60186 - INSS NOVO GOLD MAX PN-S"
            tabela_completa = str(row.get('DS_TABCOM', row.get('TABELA', row.get('TIPO_TABELA', '')))).strip()
            
            # FunÃ§Ã£o para detectar tipo de operaÃ§Ã£o baseado na tabela
            def detect_facta_operation_type(tabela_descricao):
                """Detecta tipo de operaÃ§Ã£o FACTA92 baseado na descriÃ§Ã£o da tabela"""
                if not tabela_descricao:
                    return "Margem Livre (Novo)"
                    
                descricao_upper = tabela_descricao.upper()
                logging.info(f"ðŸ”§ FACTA92 detectando operaÃ§Ã£o: '{descricao_upper[:50]}...'")
                
                # Baseado nos cÃ³digos vistos no relatÃ³rio
                if 'FGTS' in descricao_upper:
                    return "Margem Livre (Novo)"  # FGTS Ã© margem livre
                elif 'CLT' in descricao_upper and 'NOVO' in descricao_upper:
                    return "Margem Livre (Novo)"
                elif 'PORTABILIDADE' in descricao_upper or 'PORT' in descricao_upper:
                    return "Portabilidade"
                elif 'REFINANCIAMENTO' in descricao_upper or 'REFIN' in descricao_upper:
                    return "Refinanciamento"
                else:
                    return "Margem Livre (Novo)"  # Default
            
            # CORRIGIDO: Extrair apenas cÃ³digo numÃ©rico da tabela
            codigo_tabela = ""
            if tabela_completa:
                # Procurar por cÃ³digo numÃ©rico no inÃ­cio
                import re
                match = re.match(r'^(\d+)', tabela_completa)
                if match:
                    codigo_tabela = match.group(1)
                    logging.info(f"âœ… FACTA92 cÃ³digo extraÃ­do: '{tabela_completa}' â†’ '{codigo_tabela}'")
                else:
                    codigo_tabela = tabela_completa  # Fallback
                    logging.warning(f"âš ï¸ FACTA92 nÃ£o conseguiu extrair cÃ³digo de: '{tabela_completa}'")
            
            # UsuÃ¡rio/Corretor
            usuario = str(row.get('LOGIN_CORRETOR', row.get('CORRETOR', row.get('USUARIO', '')))).strip()
            
            # Data
            data_cadastro = str(row.get('DATA_CADASTRO', row.get('DATA_REGISTRO', row.get('DATA', '')))).strip()
            data_pagamento = str(row.get('DATAEFETIVACAO', row.get('DATA_PAGAMENTO_CLIENTE', row.get('DATA_PAGAMENTO', '')))).strip()
            
            # ConvÃªnio e detecÃ§Ã£o de Ã³rgÃ£o melhorada
            convenio = str(row.get('CONVENIO', '')).strip()
            if convenio == '3':
                orgao = 'INSS'
            else:
                # Detectar Ã³rgÃ£o baseado na tabela completa
                if tabela_completa:
                    tabela_upper = tabela_completa.upper()
                    if 'FGTS' in tabela_upper:
                        orgao = 'INSS'  # FGTS usa margem INSS
                    elif 'CLT' in tabela_upper or 'INSS' in tabela_upper:
                        orgao = 'INSS'
                    elif 'SIAPE' in tabela_upper:
                        orgao = 'SIAPE'
                    elif 'PREFEITURA' in tabela_upper or 'PREF' in tabela_upper:
                        orgao = 'PREFEITURA'
                    else:
                        orgao = 'INSS'  # Default
                else:
                    orgao = 'INSS'  # Default
            
            # Log para debug
            logging.info(f"âœ… FACTA92 processado: PROPOSTA={proposta}, TABELA={tabela_completa} â†’ CODIGO={codigo_tabela}, VL_BRUTO={vl_bruto}, VL_LIQ={vl_liquido}")
            
            normalized_row = {
                "PROPOSTA": proposta,
                "DATA_CADASTRO": data_cadastro,
                "BANCO": "FACTA92",
                "ORGAO": orgao,
                "TIPO_OPERACAO": detect_facta_operation_type(tabela_completa),
                "NUMERO_PARCELAS": num_parcelas,
                "VALOR_OPERACAO": vl_bruto if vl_bruto else vl_liquido,
                "VALOR_LIBERADO": vl_liquido,
                "USUARIO_BANCO": usuario,
                "SITUACAO": situacao if situacao else "PAGO",  # Status melhorado
                "DATA_PAGAMENTO": data_pagamento,
                "CPF": cpf,
                "NOME": nome,
                "DATA_NASCIMENTO": data_nascimento,  # Agora mapeado
                "CODIGO_TABELA": codigo_tabela,  # CORRIGIDO: SÃ³ cÃ³digo numÃ©rico
                "VALOR_PARCELAS": vl_parcela,  # CORRIGIDO: Agora mapeado
                "TAXA": str(row.get('TAXA', '')).strip(),  # FACTA92 tem TAXA em formato decimal (1.85)
                "OBSERVACOES": ""
            }
        
        elif bank_type == "PAULISTA":
            logging.error(f"ðŸŽ¯ CHEGOU NO BLOCO PAULISTA! Linha {idx}: '{row.get('Unnamed: 0', '')}'")
            logging.error(f"ðŸ’¥ INICIO DO PROCESSAMENTO PAULISTA - ANTES DE QUALQUER LÃ“GICA")
            logging.info(f"=" * 80)
            logging.info(f"ðŸ¦ PAULISTA linha {idx}: INICIANDO PROCESSAMENTO")
            logging.info(f"   Colunas disponÃ­veis: {list(row.keys())[:10]}")
            
            # ðŸ”§ CORREÃ‡ÃƒO PAULISTA: Verificar se primeira linha contÃ©m cabeÃ§alhos
            primeira_celula = row.get('Unnamed: 0', '')
            logging.error(f"ðŸ”§ PAULISTA: Verificando primeira_celula = '{primeira_celula}'")
            if str(primeira_celula) == 'NÂº Proposta':
                logging.info(f"ï¿½ PAULISTA: Primeira linha Ã© cabeÃ§alho! Pulando...")
                continue  # Pular linha de cabeÃ§alho
            
            logging.error(f"ðŸŽ‰ PAULISTA: Passou da verificaÃ§Ã£o de cabeÃ§alho! Continuando processamento...")
            
            # Debug da linha atual - log detalhado para ver o que estÃ¡ vindo
            logging.error(f"ðŸ” PAULISTA: Iniciando debug da linha atual...")
            logging.info(f"   ðŸ” Proposta bruta: '{primeira_celula}' (tipo: {type(primeira_celula)})")
            logging.info(f"   ðŸ” Segunda coluna (Contrato): '{row.get('Unnamed: 1', '')}'")
            logging.info(f"   ðŸ” CPF: '{row.get('Unnamed: 4', '')}'")
            logging.info(f"   ðŸ” Nome: '{row.get('Unnamed: 5', '')}'")
            
            logging.error(f"ðŸ” PAULISTA: ApÃ³s logs bÃ¡sicos - continuando...")
            logging.info(f"=" * 80)
            
            def detect_paulista_organ(especies_beneficio, produto, proposta=""):
                """Detecta Ã³rgÃ£o do Paulista de forma inteligente"""
                especies_upper = especies_beneficio.upper() if especies_beneficio else ""
                produto_upper = produto.upper() if produto else ""
                proposta_upper = proposta.upper() if proposta else ""
                
                logging.info(f"ðŸ›ï¸ PAULISTA detectando Ã³rgÃ£o: espÃ©cie='{especies_upper[:30]}...' | produto='{produto_upper[:30]}...'")
                
                # AnÃ¡lise combinada de espÃ©cie + produto
                combined_text = f"{especies_upper} {produto_upper} {proposta_upper}"
                
                # INSS - mais comum no Paulista
                if any(x in combined_text for x in ['INSS', 'APOSENT', 'PENSÃƒO', 'PENSAO', 'BENEFICI', 'PREVIDENC']):
                    return "INSS"
                
                # FGTS
                elif any(x in combined_text for x in ['FGTS', 'TRABALHADOR', 'SAQUE']):
                    return "FGTS"
                
                # Prefeituras ou outros Ã³rgÃ£os
                elif any(x in combined_text for x in ['PREFEIT', 'MUNICIPAL', 'SERVIDOR']):
                    return "PREFEITURA"
                
                # PadrÃ£o: INSS (Paulista Ã© principalmente INSS)
                else:
                    return "INSS"
            
            def detect_paulista_operation(produto, especies_beneficio="", status=""):
                """Detecta tipo de operaÃ§Ã£o do Paulista"""
                produto_upper = produto.upper() if produto else ""
                especies_upper = especies_beneficio.upper() if especies_beneficio else ""
                status_upper = status.upper() if status else ""
                
                logging.info(f"ðŸ”§ PAULISTA detectando operaÃ§Ã£o: produto='{produto_upper[:30]}...' | espÃ©cie='{especies_upper[:20]}...'")
                
                combined_text = f"{produto_upper} {especies_upper} {status_upper}"
                
                # Refinanciamento
                if any(x in combined_text for x in ['REFIN', 'REFINANCIAMENTO']):
                    return "Refinanciamento"
                
                # Portabilidade
                elif any(x in combined_text for x in ['PORT', 'PORTABILIDADE']):
                    return "Portabilidade"
                
                # Margem Livre (mais comum)
                elif any(x in combined_text for x in ['MARGEM', 'LIVRE', 'CONSIGNADO', 'NOVO']):
                    return "Margem Livre (Novo)"
                
                # PadrÃ£o
                else:
                    return "Margem Livre (Novo)"
            
            # Mapeamento BANCO PAULISTA - Colunas NOMEADAS (nÃ£o Unnamed!)
            # Baseado no arquivo real: NÂº Proposta | Contrato | Data Captura | etc.
            
            # Mapeamento PAULISTA corrigido baseado em map_relat_atualizados.txt
            # Unnamed: 0='NÂº Proposta', 1='Contrato', 2='Data Captura', 3='Dt Atividade'  
            # Unnamed: 4='CPF/CNPJ Proponente', 5='Nome do Proponente', 6='MatrÃ­cula', 7='EspÃ©cie BenefÃ­cio'
            # Unnamed: 8='Banco', 9='AgÃªncia', 10='Conta', 11='Valor Solicitado'
            # Unnamed: 12='Vl. Liberado', 13='Vl. Troco', 14='Quant. Parcelas', 15='Valor Parcela'
            # Unnamed: 16='Plano', 17='1Âº Vencimento', 18='Produto', 19='Fase', 20='Status'
            # Unnamed: 21='Dta. IntegraÃ§Ã£o', 22='Loja/Sub', 23='Lojista/Master', 24='UsuÃ¡rio Digitador'
            
            proposta = str(row.get('Unnamed: 0', '')).strip()  # NÂº Proposta
            especies_beneficio = str(row.get('Unnamed: 7', '')).strip()  # EspÃ©cie BenefÃ­cio
            produto = str(row.get('Unnamed: 18', '')).strip()  # Produto
            status = str(row.get('Unnamed: 20', '')).strip()  # Status
            
            # Logs detalhados para debug
            logging.info(f"ðŸ“‹ PAULISTA extraÃ­do: Proposta={proposta}, Produto={produto[:30] if produto else 'N/A'}")
            logging.info(f"ðŸ“‹ PAULISTA status='{status}', espÃ©cie='{especies_beneficio[:20] if especies_beneficio else 'N/A'}'")
            
            logging.error(f"ðŸ” PAULISTA: Chegou na validaÃ§Ã£o! Proposta extraÃ­da: '{proposta}'")
            
            # âœ… VALIDAÃ‡ÃƒO: Verificar se linha deve ser processada
            if not proposta or proposta.upper() in ['NAN', 'NONE', '', 'UNNAMED: 0', 'NÂº PROPOSTA', 'PROPOSTA']:
                logging.info(f"â­ï¸ PAULISTA linha {idx}: Pulando - proposta vazia ou cabeÃ§alho ({proposta})")
                normalized_row = None
            else:
                logging.info(f"âœ… PAULISTA linha {idx}: Proposta vÃ¡lida - vai processar")
                
                # Detectar Ã³rgÃ£o e operaÃ§Ã£o
                orgao_detectado = detect_paulista_organ(especies_beneficio, produto, proposta)
                operacao_detectada = detect_paulista_operation(produto, especies_beneficio, status)
                
                # Aplicar formataÃ§Ã£o brasileira usando posiÃ§Ãµes Unnamed corretas do mapeamento
                valor_operacao_raw = str(row.get('Unnamed: 11', '')).strip()  # Valor Solicitado
                valor_parcela_raw = str(row.get('Unnamed: 15', '')).strip()   # Valor Parcela  
                valor_liberado_raw = str(row.get('Unnamed: 12', '')).strip()  # Vl. Liberado
                cpf_raw = str(row.get('Unnamed: 4', '')).strip()             # CPF/CNPJ Proponente
                
                # Usar as funÃ§Ãµes globais de formataÃ§Ã£o
                valor_operacao_formatted = format_value_brazilian(valor_operacao_raw)
                valor_liberado_formatted = format_value_brazilian(valor_liberado_raw)
                valor_parcela_formatted = format_value_brazilian(valor_parcela_raw)
                cpf_formatted = format_cpf_global(cpf_raw)
                
                logging.info(f"âœ… PAULISTA formatado: CPF={cpf_formatted}, Valor={valor_operacao_formatted}, Ã“rgÃ£o={orgao_detectado}")
                
                normalized_row = {
                    "PROPOSTA": proposta,  # Unnamed: 0 = NÂº Proposta
                    "ADE": proposta,  # Campo ADE explÃ­cito = mesma proposta
                    "DATA_CADASTRO": str(row.get('Unnamed: 2', '')).strip(),  # Data Captura
                    "BANCO": "BANCO PAULISTA",
                    "ORGAO": orgao_detectado,
                    "TIPO_OPERACAO": operacao_detectada,
                    "NUMERO_PARCELAS": str(row.get('Unnamed: 14', '')).strip(),  # Quant. Parcelas
                    "VALOR_OPERACAO": valor_operacao_formatted,  # âœ… Formatado brasileiro
                    "VALOR_LIBERADO": valor_liberado_formatted,  # âœ… Formatado brasileiro
                    "USUARIO_BANCO": str(row.get('Unnamed: 24', '')).strip(),   # UsuÃ¡rio Digitador
                    "SITUACAO": status,  # STATUS direto (serÃ¡ normalizado depois)
                    "DATA_PAGAMENTO": str(row.get('Unnamed: 21', '')).strip(),  # Dta. IntegraÃ§Ã£o
                    "CPF": cpf_formatted,  # âœ… Formatado brasileiro (XXX.XXX.XXX-XX)
                    "NOME": str(row.get('Unnamed: 5', '')).strip().upper(),     # Nome do Proponente âœ… MaiÃºsculas
                    "DATA_NASCIMENTO": "",  # NÃ£o disponÃ­vel no PAULISTA
                    "CODIGO_TABELA": str(row.get('Unnamed: 16', '')).strip(),   # Plano - serÃ¡ mapeado pelo Storm depois
                    "VALOR_PARCELAS": valor_parcela_formatted,  # âœ… Formatado brasileiro
                    "TAXA": "0,00%",  # PadrÃ£o brasileiro
                    "OBSERVACOES": f"Contrato: {str(row.get('Unnamed: 1', '')).strip()} | Banco: {str(row.get('Unnamed: 8', '')).strip()} | AgÃªncia: {str(row.get('Unnamed: 9', '')).strip()}"
                }
                
                logging.info(f"âœ…âœ…âœ… PAULISTA normalized_row criado com sucesso!")
                logging.info(f"âœ…âœ…âœ… PAULISTA normalized_row final: {normalized_row}")
        
        elif bank_type == "BRB":
            # Mapeamento BRB (Banco de BrasÃ­lia) - Baseado em map_relat_atualizados.txt
            normalized_row = {
                "PROPOSTA": str(row.get('ID Card', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data da Proposta', '')).strip(),
                "BANCO": "BRB",
                "ORGAO": str(row.get('ORGÃƒO', 'INSS')).strip(),
                "TIPO_OPERACAO": str(row.get('OPERAÃ‡ÃƒO', 'Margem Livre (Novo)')).strip(),
                "NUMERO_PARCELAS": str(row.get('Qtd. Parcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor da Proposta', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor da Proposta', '')).strip(),
                "USUARIO_BANCO": str(row.get('Agente ResponsÃ¡vel', '')).strip(),
                "SITUACAO": str(row.get('Status da Proposta', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data da PAGAMENTO', '')).strip(),
                "CPF": str(row.get('CPF do BeneficiÃ¡rio', '')).strip(),
                "NOME": str(row.get('Nome do cliente', '')).strip(),
                "DATA_NASCIMENTO": "",  # NÃ£o disponÃ­vel
                "CODIGO_TABELA": str(row.get('TABELA', '')).strip(),
                "VALOR_PARCELAS": str(row.get('Valor da Parcela', '')).strip(),
                "TAXA": str(row.get('TAXA', '')).strip(),
                "OBSERVACOES": str(row.get('ObservaÃ§Ãµes', '')).strip()
            }
        
        elif bank_type == "QUALIBANKING":
            # Mapeamento QUALIBANKING - Baseado em map_relat_atualizados.txt
            normalized_row = {
                "PROPOSTA": str(row.get('NÃºmero do Contrato', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data da Proposta', '')).strip(),
                "BANCO": "QUALIBANKING",
                "ORGAO": str(row.get('Tipo de Produto', 'INSS')).strip(),
                "TIPO_OPERACAO": str(row.get('Tipo de OperaÃ§Ã£o', 'Margem Livre (Novo)')).strip(),
                "NUMERO_PARCELAS": str(row.get('Prazo', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor do EmprÃ©stimo', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor LÃ­quido ao Cliente', '')).strip(),
                "USUARIO_BANCO": str(row.get('Login', '')).strip(),
                "SITUACAO": str(row.get('Status', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data do CrÃ©dito ao Cliente', '')).strip(),
                "CPF": str(row.get('CPF', '')).strip(),
                "NOME": str(row.get('Nome', '')).strip(),
                "DATA_NASCIMENTO": "",  # NÃ£o disponÃ­vel
                "CODIGO_TABELA": str(row.get('Nome da Tabela', '')).strip(),
                "VALOR_PARCELAS": str(row.get('Valor da Parcela', '')).strip(),
                "TAXA": str(row.get('Taxa', '')).strip(),
                "OBSERVACOES": str(row.get('Motivo do Status', '')).strip()
            }
        
        elif bank_type == "MERCANTIL":
            # Mapeamento BANCO MERCANTIL - Baseado em map_relat_atualizados.txt
            # Detectar Ã³rgÃ£o pelo nome do convÃªnio
            nome_convenio = str(row.get('NomeConvenio', '')).upper()
            if 'FGTS' in nome_convenio or 'F.G.T.S' in nome_convenio:
                orgao = 'FGTS'
            elif 'INSS' in nome_convenio:
                orgao = 'INSS'
            else:
                orgao = 'INSS'  # Default
            
            # Detectar tipo de operaÃ§Ã£o
            modalidade = str(row.get('ModalidadeCredito', '')).upper()
            if 'FGTS' in modalidade or 'SAQUEANIVERSARIOFGTS' in modalidade:
                tipo_operacao = 'Margem Livre (Novo)'
            elif 'CREDITOPESSOAL' in modalidade:
                tipo_operacao = 'Margem Livre (Novo)'
            else:
                tipo_operacao = 'Margem Livre (Novo)'
            
            normalized_row = {
                "PROPOSTA": str(row.get('NumeroProposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('DataCadastro', '')).strip(),
                "BANCO": "BANCO MERCANTIL",
                "ORGAO": orgao,
                "TIPO_OPERACAO": tipo_operacao,
                "NUMERO_PARCELAS": str(row.get('QuantidadeParcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('ValorFinanciado', '')).strip(),
                "VALOR_LIBERADO": str(row.get('ValorEmprestimo', '')).strip(),
                "USUARIO_BANCO": str(row.get('LoginUsuarioDigitador', '')).strip(),
                "SITUACAO": str(row.get('SituacaoProposta', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('DataPagamentoCliente', '')).strip(),
                "CPF": str(row.get('Cpf', '')).strip(),
                "NOME": str(row.get('Nome', '')).strip(),
                "DATA_NASCIMENTO": str(row.get('DataNascimento', '')).strip(),
                "CODIGO_TABELA": str(row.get('NomeProduto', '')).strip(),
                "VALOR_PARCELAS": str(row.get('ValorParcela', '')).strip(),
                "TAXA": str(row.get('TaxaJurosMes', '')).strip(),
                "OBSERVACOES": ""
            }
        
        elif bank_type == "AMIGOZ":
            # Mapeamento BANCO AMIGOZ - Baseado em map_relat_atualizados.txt
            normalized_row = {
                "PROPOSTA": str(row.get('Nr Proposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data Cadastro', '')).strip(),
                "BANCO": "BANCO AMIGOZ",
                "ORGAO": str(row.get('Convenio', 'INSS')).strip(),
                "TIPO_OPERACAO": str(row.get('Produto', 'CartÃ£o Consignado')).strip(),
                "NUMERO_PARCELAS": str(row.get('Qtd Parcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor Proposta', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor Liberado Cliente', '')).strip(),
                "USUARIO_BANCO": str(row.get('UsuÃ¡rio Digitador', '')).strip(),
                "SITUACAO": str(row.get('Status Proposta', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data IntegraÃ§Ã£o', '')).strip(),
                "CPF": str(row.get('CPF Cliente', '')).strip(),
                "NOME": str(row.get('Nome Cliente', '')).strip(),
                "DATA_NASCIMENTO": "",  # NÃ£o disponÃ­vel diretamente
                "CODIGO_TABELA": str(row.get('Tipo de CartÃ£o', '')).strip(),
                "VALOR_PARCELAS": "",  # NÃ£o disponÃ­vel
                "TAXA": str(row.get('Taxa da OperaÃ§Ã£o', '')).strip(),
                "OBSERVACOES": str(row.get('Restricoes', '')).strip()
            }
        
        elif bank_type == "TOTALCASH":
            # Mapeamento BANCO TOTALCASH - Colunas corretas
            convenio = str(row.get('Convenio', '')).strip()
            if 'INSS' in convenio:
                orgao = 'INSS'
            elif 'FGTS' in convenio:
                orgao = 'FGTS'
            else:
                orgao = 'INSS'  # Default
            
            normalized_row = {
                "PROPOSTA": str(row.get('Nr Proposta', '')).strip(),
                "DATA_CADASTRO": str(row.get('Data Cadastro', '')).strip(),
                "BANCO": "BANCO TOTALCASH",
                "ORGAO": orgao,
                "TIPO_OPERACAO": str(row.get('Produto', 'Margem Livre (Novo)')).strip(),
                "NUMERO_PARCELAS": str(row.get('Qtd Parcelas', '')).strip(),
                "VALOR_OPERACAO": str(row.get('Valor Proposta', '')).strip(),
                "VALOR_LIBERADO": str(row.get('Valor Liberado Cliente', '')).strip(),
                "USUARIO_BANCO": str(row.get('UsuÃ¡rio Digitador', '')).strip(),
                "SITUACAO": str(row.get('Status Proposta', '')).strip(),
                "DATA_PAGAMENTO": str(row.get('Data IntegraÃ§Ã£o', '')).strip(),
                "CPF": str(row.get('CPF Cliente', '')).strip(),
                "NOME": str(row.get('Nome Cliente', '')).strip(),
                "DATA_NASCIMENTO": "",  # NÃ£o disponÃ­vel
                "CODIGO_TABELA": "",  # NÃ£o disponÃ­vel
                "VALOR_PARCELAS": str(row.get('Valor Parcela', '')).strip(),
                "TAXA": str(row.get('Taxa da OperaÃ§Ã£o', '')).strip(),
                "OBSERVACOES": str(row.get('ObservaÃ§Ãµes', row.get('Observacoes', row.get('Obs', '')))).strip()
            }
        
        else:  # GENERICO
            # Tentar mapear colunas automaticamente
            normalized_row = {
                "PROPOSTA": "",
                "DATA_CADASTRO": "",
                "BANCO": bank_type,
                "ORGAO": "",
                "TIPO_OPERACAO": "MARGEM LIVRE (NOVO)",
                "NUMERO_PARCELAS": "",
                "VALOR_OPERACAO": "",
                "VALOR_LIBERADO": "",
                "USUARIO_BANCO": "",
                "SITUACAO": "",
                "DATA_PAGAMENTO": "",
                "CPF": "",
                "NOME": "",
                "DATA_NASCIMENTO": "",
                "TAXA": ""
            }
            
            # Tentar mapear automaticamente baseado em nomes de colunas
            for col in df.columns:
                col_lower = str(col).lower()
                if 'proposta' in col_lower or 'id' in col_lower:
                    normalized_row["PROPOSTA"] = str(row.get(col, '')).strip()
                elif 'data' in col_lower and 'cadastro' in col_lower:
                    normalized_row["DATA_CADASTRO"] = str(row.get(col, '')).strip()
                elif 'nome' in col_lower and 'cliente' in col_lower:
                    normalized_row["NOME"] = str(row.get(col, '')).strip()
                elif 'cpf' in col_lower:
                    normalized_row["CPF"] = str(row.get(col, '')).strip()
                elif 'status' in col_lower or 'situacao' in col_lower:
                    normalized_row["SITUACAO"] = str(row.get(col, '')).strip()
                elif 'valor' in col_lower and ('operacao' in col_lower or 'liberado' in col_lower):
                    normalized_row["VALOR_LIBERADO"] = str(row.get(col, '')).strip()
                    if not normalized_row["VALOR_OPERACAO"]:
                        normalized_row["VALOR_OPERACAO"] = str(row.get(col, '')).strip()
        
        # âœ… VERIFICAÃ‡ÃƒO CRÃTICA: Pular linhas filtradas (normalized_row = None)
        # DEVE VIR ANTES de qualquer acesso a normalized_row.get() ou normalized_row[]
        if normalized_row is None:
            logging.info(f"â­ï¸ [{bank_type}] Linha filtrada (normalized_row=None), pulando mapeamento e validaÃ§Ã£o")
            continue
        
        # Aplicar mapeamento de status (normalizaÃ§Ã£o completa)
        if normalized_row.get("SITUACAO"):
            situacao_original = str(normalized_row["SITUACAO"]).strip()
            situacao_lower = situacao_original.lower()
            
            # Tentar encontrar no mapeamento
            situacao_normalizada = STATUS_MAPPING.get(situacao_lower, None)
            
            # Se nÃ£o encontrou exato, tentar normalizar caracteres especiais e espaÃ§os
            if not situacao_normalizada:
                # Remover acentos e caracteres especiais para busca mais flexÃ­vel
                import unicodedata
                situacao_clean = ''.join(
                    c for c in unicodedata.normalize('NFD', situacao_lower)
                    if unicodedata.category(c) != 'Mn'
                )
                situacao_clean = situacao_clean.replace('/', ' ').replace('-', ' ').strip()
                situacao_clean = ' '.join(situacao_clean.split())  # Remove espaÃ§os mÃºltiplos
                
                # Tentar encontrar novamente
                situacao_normalizada = STATUS_MAPPING.get(situacao_clean, None)
            
            # Se ainda nÃ£o encontrou, fazer busca por palavras-chave
            if not situacao_normalizada:
                situacao_palavras = situacao_lower.lower()
                if any(word in situacao_palavras for word in ['pag', 'integra', 'finaliz', 'quitad', 'liberad', 'desembolsa', 'aprovad']):
                    situacao_normalizada = "PAGO"
                elif any(word in situacao_palavras for word in ['cancel', 'reprov', 'rejeit', 'negad', 'expirad', 'invalid', 'recus', 'desist']):
                    situacao_normalizada = "CANCELADO"
                elif any(word in situacao_palavras for word in ['aguard', 'pendent', 'aberto', 'digital', 'andament', 'analise', 'process', 'formal', 'cadastr', 'enviad']):
                    situacao_normalizada = "AGUARDANDO"
            
            # Aplicar a normalizaÃ§Ã£o (ou manter original se nÃ£o encontrou)
            normalized_row["SITUACAO"] = situacao_normalizada if situacao_normalizada else situacao_original
            
            # Log para debug (apenas se nÃ£o encontrou mapeamento)
            if not situacao_normalizada:
                logging.warning(f"âš  Status nÃ£o mapeado: '{situacao_original}' - mantido como estÃ¡")
        
        # Aplicar mapeamento de cÃ³digo de tabela (sem dependÃªncia de usuÃ¡rio para maior estabilidade)
        # EXCETO para DIGIO, AVERBAI, DAYCOVAL, QUERO_MAIS e SANTANDER que jÃ¡ tÃªm cÃ³digos corretos
        # VCTEX PRECISA de mapeamento: "Tabela EXP" (banco) â†’ "TabelaEXP" (storm)
        if bank_type == "DIGIO":
            # DIGIO jÃ¡ aplicou mapeamento especÃ­fico, pular mapeamento geral
            logging.info(f"ðŸ“Š PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: DIGIO jÃ¡ mapeado, pulando mapeamento geral")
            mapping_result = None
        elif bank_type == "SANTANDER":
            # ðŸ¦ SANTANDER: CÃ³digos jÃ¡ extraÃ­dos corretamente (810021387, 82721387, etc.)
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            logging.info(f"âœ… PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: SANTANDER cÃ³digo direto {codigo_direto}, pulando mapeamento automÃ¡tico")
            mapping_result = None
        elif bank_type == "AVERBAI" and normalized_row.get("CODIGO_TABELA", "").isdigit():
            # ðŸŽ¯ AVERBAI com cÃ³digo direto do arquivo - nÃ£o precisa mapeamento complexo!
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            logging.info(f"âœ… PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: AVERBAI cÃ³digo direto {codigo_direto}, pulando mapeamento complexo")
            mapping_result = None
        elif bank_type == "DAYCOVAL" and normalized_row.get("CODIGO_TABELA", "").isdigit():
            # ðŸŽ¯ DAYCOVAL com cÃ³digo direto do arquivo - mesma lÃ³gica do AVERBAI!
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            logging.info(f"âœ… PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: DAYCOVAL cÃ³digo direto {codigo_direto}, pulando mapeamento complexo")
            mapping_result = None
        elif bank_type == "QUERO_MAIS":
            # ðŸŽ¯ QUERO MAIS - preservar cÃ³digos originais, nÃ£o aplicar mapeamento automÃ¡tico
            codigo_direto = normalized_row.get("CODIGO_TABELA", "")
            logging.info(f"âœ… PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: QUERO MAIS cÃ³digo direto {codigo_direto}, pulando mapeamento automÃ¡tico")
            mapping_result = None
        else:
            banco_para_mapeamento = normalized_row.get("BANCO", "")
            orgao_para_mapeamento = normalized_row.get("ORGAO", "")
            operacao_para_mapeamento = normalized_row.get("TIPO_OPERACAO", "")
            tabela_para_mapeamento = normalized_row.get("CODIGO_TABELA", "")
            
            logging.info(f"ðŸ“Š PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: Aplicando mapeamento com BANCO={banco_para_mapeamento}, ORGAO={orgao_para_mapeamento}, OPERACAO={operacao_para_mapeamento}, TABELA={tabela_para_mapeamento}")
            
            mapping_result = apply_mapping(
                banco_para_mapeamento,
                orgao_para_mapeamento,
                operacao_para_mapeamento,
                "",  # usuario vazio - nÃ£o mais usado para evitar problemas futuros
                tabela_para_mapeamento
            )
        
        # SEMPRE usar dados do relat_orgaos.csv (formato Storm) quando disponÃ­vel
        # Os dados do banco sÃ£o apenas para valores financeiros
        
        # Log ANTES do mapeamento
        logging.info(f"ðŸ“‹ ANTES do mapeamento - PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: ORGAO={normalized_row.get('ORGAO', '')}, CODIGO_TABELA={normalized_row.get('CODIGO_TABELA', '')}, TAXA={normalized_row.get('TAXA', '')}, OPERACAO={normalized_row.get('TIPO_OPERACAO', '')}")
        
        # Se encontrou mapeamento, aplicar TODOS os campos do Storm
        if mapping_result:
            # 1. ORGÃƒO padronizado (Storm)
            if mapping_result.get('orgao_storm'):
                normalized_row["ORGAO"] = mapping_result.get('orgao_storm', '')
            
            # 2. CODIGO TABELA (Storm) - SEMPRE substituir se encontrou mapeamento
            if mapping_result.get('codigo_tabela'):
                normalized_row["CODIGO_TABELA"] = mapping_result.get('codigo_tabela', '')
            
            # 3. TAXA (Storm) - SEMPRE substituir se encontrou mapeamento
            if mapping_result.get('taxa_storm'):
                taxa_mapeada = mapping_result.get('taxa_storm', "0,00%")
                # Garantir que tem formato percentual
                if taxa_mapeada and '%' not in taxa_mapeada:
                    taxa_mapeada = taxa_mapeada + '%'
                normalized_row["TAXA"] = taxa_mapeada if taxa_mapeada else "0,00%"
            
            # 4. TIPO DE OPERACAO (Storm) - SEMPRE substituir se encontrou mapeamento
            if mapping_result.get('operacao_storm'):
                normalized_row["TIPO_OPERACAO"] = mapping_result.get('operacao_storm', "")
        else:
            # Se NÃƒO encontrou mapeamento, manter valores do banco mas garantir que TAXA existe
            logging.warning(f"âš ï¸ PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: Mapeamento NÃƒO encontrado! Usando valores do banco")
            # Garantir que TAXA tenha valor mesmo sem mapeamento
            if not normalized_row.get("TAXA") or normalized_row.get("TAXA") == "":
                normalized_row["TAXA"] = "0,00%"
            elif '%' not in normalized_row.get("TAXA", ""):
                normalized_row["TAXA"] = normalized_row.get("TAXA") + '%'
        
        # VALIDAÃ‡ÃƒO FINAL: Garantir que TAXA e CODIGO_TABELA nunca fiquem vazios
        if not normalized_row.get("TAXA") or normalized_row.get("TAXA").strip() == "":
            normalized_row["TAXA"] = "0,00%"
            logging.warning(f"âš ï¸ PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: TAXA vazia, definida como 0,00%")
        
        if not normalized_row.get("CODIGO_TABELA") or normalized_row.get("CODIGO_TABELA").strip() == "":
            normalized_row["CODIGO_TABELA"] = "SEM_CODIGO"
            logging.warning(f"âš ï¸ PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: CODIGO_TABELA vazio, definido como SEM_CODIGO")
        
        # ðŸ” PRESERVAR DATAS ORIGINAIS - nÃ£o deixar o mapeamento alterar
        data_cadastro_original = normalized_row.get('DATA_CADASTRO', '')
        data_pagamento_original = normalized_row.get('DATA_PAGAMENTO', '')
        
        # Log DEPOIS do mapeamento
        logging.info(f"ðŸ“— DEPOIS do mapeamento - PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: ORGAO={normalized_row.get('ORGAO', '')}, CODIGO_TABELA={normalized_row.get('CODIGO_TABELA', '')}, TAXA={normalized_row.get('TAXA', '')}, OPERACAO={normalized_row.get('TIPO_OPERACAO', '')}")
        
        # âœ… GARANTIR que as datas originais sejam mantidas
        if data_cadastro_original:
            normalized_row['DATA_CADASTRO'] = data_cadastro_original
        if data_pagamento_original: 
            normalized_row['DATA_PAGAMENTO'] = data_pagamento_original
            
        logging.info(f"ðŸ“… DATAS FINAIS PRESERVADAS - PROPOSTA {normalized_row.get('PROPOSTA', 'N/A')}: CADASTRO='{normalized_row.get('DATA_CADASTRO')}' | PAGAMENTO='{normalized_row.get('DATA_PAGAMENTO')}'")

        
        # VALIDAÃ‡ÃƒO MELHORADA: SÃ³ adicionar se tiver dados essenciais vÃ¡lidos
        proposta = str(normalized_row.get("PROPOSTA", "")).strip()
        nome = str(normalized_row.get("NOME", "")).strip()
        cpf = str(normalized_row.get("CPF", "")).strip()
        
        logging.info(f"ðŸ” VALIDAÃ‡ÃƒO FINAL - Proposta: '{proposta}', Nome: '{nome[:30] if nome else 'N/A'}', CPF: '{cpf}'")
        
        # Palavras-chave que indicam linha invÃ¡lida (EXATAS, nÃ£o substring)
        invalid_exact_keywords = [
            "nan", "none", "null", "unnamed", "relatÃ³rio", "relatorio",
            "total", "pÃ¡gina", "pagina"
        ]
        
        # Palavras que sÃ³ sÃ£o invÃ¡lidas se forem a palavra COMPLETA
        invalid_whole_words = [
            "proposta", "nome", "cliente", "cpf", "banco", "cÃ³digo", "codigo", "data"
        ]
        
        # Verificar se proposta Ã© vÃ¡lida
        proposta_lower = proposta.lower()
        
        # VerificaÃ§Ã£o melhorada: nÃ£o rejeitar se Ã© uma proposta real que contÃ©m essas palavras
        has_invalid_exact = any(keyword == proposta_lower for keyword in invalid_exact_keywords)
        has_invalid_whole = any(proposta_lower == keyword for keyword in invalid_whole_words)
        
        # ValidaÃ§Ã£o mais leniente para diagnÃ³stico
        is_valid_proposta = (
            proposta and 
            proposta.strip() not in ["", "nan", "None", "NULL", "NaN"] and
            not has_invalid_exact and
            not has_invalid_whole and
            len(proposta.strip()) >= 1  # Qualquer proposta com pelo menos 1 caractere
        )
        
        logging.info(f"   is_valid_proposta={is_valid_proposta} (has_invalid_exact={has_invalid_exact}, has_invalid_whole={has_invalid_whole})")
        
        # Verificar se tem pelo menos nome OU cpf vÃ¡lido
        nome_lower = nome.lower() if nome else ""
        nome_is_invalid = any(nome_lower == keyword for keyword in invalid_whole_words + invalid_exact_keywords)
        
        # Relaxar validaÃ§Ã£o de CPF para aceitar formatados (XXX.XXX.XXX-XX)
        cpf_clean = ''.join(filter(str.isdigit, cpf)) if cpf else ""
        cpf_valid = len(cpf_clean) >= 11 or (cpf and len(cpf) >= 11)
        
        # VALIDAÃ‡ÃƒO ESPECÃFICA PARA DAYCOVAL - Mais flexÃ­vel sÃ³ para este banco
        if bank_type == "DAYCOVAL":
            # DAYCOVAL: Se tem proposta numÃ©rica, Ã© praticamente sempre vÃ¡lido
            has_numeric_proposta = proposta and any(c.isdigit() for c in proposta)
            has_valid_data = (
                (nome and len(nome) > 2 and not nome_is_invalid) or  # Nome com 3+ chars
                (cpf_clean and len(cpf_clean) >= 8) or  # CPF mais flexÃ­vel para DAYCOVAL
                has_numeric_proposta or  # Proposta com nÃºmeros
                (proposta and len(proposta) >= 5)  # Qualquer proposta longa o suficiente
            )
        else:
            # VALIDAÃ‡ÃƒO NORMAL PARA OUTROS BANCOS (Storm, Santander, etc.)
            has_valid_data = (
                (nome and len(nome) > 3 and not nome_is_invalid) or
                cpf_valid or
                proposta  # Se tem proposta, jÃ¡ Ã© vÃ¡lido
            )
        
        logging.info(f"   has_valid_data={has_valid_data} (nome_valid={nome and len(nome) > 3 and not nome_is_invalid}, cpf_valid={cpf_valid})")
        
        if is_valid_proposta and has_valid_data:
            normalized_data.append(normalized_row)
            logging.info(f"âœ…âœ…âœ… Linha ADICIONADA com sucesso: Proposta={proposta}, Nome={nome[:20] if nome else 'N/A'}, CPF={cpf[:6] if cpf else 'N/A'}...")
        else:
            logging.warning(f"âŒâŒâŒ Linha IGNORADA [{bank_type}] - Proposta='{proposta}' (len={len(proposta)}), Nome='{nome[:20] if nome else 'N/A'}' (len={len(nome)}), CPF='{cpf}' (len={len(cpf)}), is_valid_proposta={is_valid_proposta}, has_valid_data={has_valid_data}")
            # Log detalhado para debug
            if not is_valid_proposta:
                logging.warning(f"  ðŸ” Proposta invÃ¡lida: has_invalid_exact={has_invalid_exact}, has_invalid_whole={has_invalid_whole}")
            if not has_valid_data:
                logging.warning(f"  ðŸ” Dados invÃ¡lidos: nome_valid={nome and len(nome) > 3 and not nome_is_invalid}, cpf_valid={cpf_valid}")
    
    logging.info(f"ðŸ“Š [{bank_type}] RESUMO: {len(normalized_data)} registros vÃ¡lidos de {len(df)} linhas processadas")
    
    # Log detalhado se nÃ£o temos dados
    if len(normalized_data) == 0:
        logging.error(f"âŒ [{bank_type}] NENHUM dado vÃ¡lido foi extraÃ­do!")
        logging.error(f"   ðŸ“‹ Colunas do DataFrame: {list(df.columns)[:10]}...")
        if not df.empty:
            logging.error(f"   ðŸ“„ Primeira linha: {dict(df.iloc[0])}") 
        return pd.DataFrame()
    
    # ðŸ§¹ FILTRO DE SEGURANÃ‡A: Remover qualquer None que possa ter escapado
    normalized_data_clean = [row for row in normalized_data if row is not None and isinstance(row, dict)]
    
    if len(normalized_data_clean) != len(normalized_data):
        logging.warning(f"âš ï¸ [{bank_type}] Removidos {len(normalized_data) - len(normalized_data_clean)} registros None da lista")
    
    if len(normalized_data_clean) == 0:
        logging.error(f"âŒ [{bank_type}] ApÃ³s filtrar None, nenhum dado restou!")
        return pd.DataFrame()
    
    return pd.DataFrame(normalized_data_clean)

def _get_daycoval_operation_type(table_description: str) -> str:
    """Determina o tipo de operaÃ§Ã£o baseado na descriÃ§Ã£o da tabela do Daycoval"""
    table_lower = table_description.lower()
    if 'rfn' in table_lower or 'refin' in table_lower:
        if 'portab' in table_lower:
            return "REFINANCIAMENTO DA PORTABILIDADE"
        else:
            return "REFINANCIAMENTO"
    elif 'portab' in table_lower:
        return "PORTABILIDADE + REFIN"
    else:
        return "MARGEM LIVRE (NOVO)"

def map_to_final_format(df: pd.DataFrame, bank_type: str) -> tuple[pd.DataFrame, int]:
    """Mapear dados para o formato final de 24 colunas com estatÃ­sticas de mapeamento"""
    try:
        # Debug especÃ­fico para PAULISTA
        if bank_type == "PAULISTA":
            logging.info(f"ðŸ¦ map_to_final_format: PAULISTA com {len(df)} linhas")
            logging.info(f"ðŸ¦ Primeiras 3 linhas do DF:")
            for i, (idx, row) in enumerate(df.head(3).iterrows()):
                logging.info(f"   Linha {idx}: Unnamed:0='{row.get('Unnamed: 0', 'N/A')}'")
        
        # Primeiro normalizar os dados
        normalized_df = normalize_bank_data(df, bank_type)
        
        if normalized_df.empty:
            logging.warning(f"Dados normalizados vazios para banco {bank_type}")
            return pd.DataFrame(), 0
        
        # Remover duplicatas especÃ­ficas para QUERO MAIS (propostas duplicadas)
        if bank_type == "QUERO_MAIS":
            original_count = len(normalized_df)
            # Remover duplicatas baseado na PROPOSTA (campo Ãºnico) mantendo o primeiro registro
            normalized_df = normalized_df.drop_duplicates(subset=['PROPOSTA'], keep='first')
            removed_count = original_count - len(normalized_df)
            if removed_count > 0:
                logging.info(f"ðŸ§¹ QUERO MAIS: Removidas {removed_count} propostas duplicadas ({original_count} â†’ {len(normalized_df)})")
        
        final_data = []
        mapped_count = 0
        
        for _, row in normalized_df.iterrows():
            situacao = row.get("SITUACAO", "")
            data_pagamento = row.get("DATA_PAGAMENTO", "")
            
            # DATA DE PAGAMENTO sÃ³ deve ter valor se STATUS for exatamente PAGO (apÃ³s normalizaÃ§Ã£o)
            if situacao.upper() != "PAGO":
                data_pagamento = ""
            
            # ðŸŒŽ APLICAR FORMATAÃ‡ÃƒO GLOBAL BRASILEIRA (CPF + Valores MonetÃ¡rios)
            cpf_raw = row.get("CPF", "")
            cpf_formatted = format_cpf_global(cpf_raw)
            
            valor_parcelas_raw = row.get("VALOR_PARCELAS", "")
            valor_parcelas_formatted = format_value_brazilian(valor_parcelas_raw)
            
            valor_operacao_raw = row.get("VALOR_OPERACAO", "")
            valor_operacao_formatted = format_value_brazilian(valor_operacao_raw)
            
            valor_liberado_raw = row.get("VALOR_LIBERADO", "")
            valor_liberado_formatted = format_value_brazilian(valor_liberado_raw)
            
            final_row = {
                "PROPOSTA": row.get("PROPOSTA", ""),
                "DATA CADASTRO": row.get("DATA_CADASTRO", ""),
                "BANCO": row.get("BANCO", ""),
                "ORGAO": row.get("ORGAO", ""),
                "CODIGO TABELA": row.get("CODIGO_TABELA", ""),
                "TIPO DE OPERACAO": row.get("TIPO_OPERACAO", ""),
                "NUMERO PARCELAS": row.get("NUMERO_PARCELAS", ""),
                "VALOR PARCELAS": valor_parcelas_formatted,  # âœ… Formatado em padrÃ£o brasileiro
                "VALOR OPERACAO": valor_operacao_formatted,  # âœ… Formatado em padrÃ£o brasileiro
                "VALOR LIBERADO": valor_liberado_formatted,  # âœ… Formatado em padrÃ£o brasileiro
                "VALOR QUITAR": "",
                "USUARIO BANCO": row.get("USUARIO_BANCO", ""),
                "CODIGO LOJA": "",
                "SITUACAO": situacao,
                "DATA DE PAGAMENTO": data_pagamento,
                "CPF": cpf_formatted,  # âœ… Formatado em padrÃ£o brasileiro (XXX.XXX.XXX-XX)
                "NOME": row.get("NOME", ""),
                "DATA DE NASCIMENTO": row.get("DATA_NASCIMENTO", ""),
                "TIPO DE CONTA": "",
                "TIPO DE PAGAMENTO": "",
                "AGENCIA CLIENTE": "",
                "CONTA CLIENTE": "",
                "FORMALIZACAO DIGITAL": "SIM",
                "TAXA": row.get("TAXA", ""),  # TAXA jÃ¡ vem do relat_orgaos.csv (aplicada em normalize_bank_data)
                "OBSERVACOES": row.get("OBSERVACOES", "")  # Campo de observaÃ§Ãµes (principalmente VCTEX)
            }
            
            # Contar se foi mapeado o cÃ³digo de tabela
            if final_row["CODIGO TABELA"]:
                mapped_count += 1
            
            # Limpar valores 'nan'
            for key, value in final_row.items():
                if str(value).lower() in ['nan', 'none', 'null', '']:
                    final_row[key] = ""
            
            final_data.append(final_row)
        
        result_df = pd.DataFrame(final_data)
        logging.info(f"Mapeamento concluÃ­do para {bank_type}: {len(result_df)} registros, {mapped_count} mapeados")
        return result_df, mapped_count
        
    except Exception as e:
        logging.error(f"Erro no mapeamento para {bank_type}: {str(e)}")
        return pd.DataFrame(), 0

def remove_duplicates_enhanced(df: pd.DataFrame, storm_data: Dict[str, str]) -> pd.DataFrame:
    """RemoÃ§Ã£o aprimorada de duplicatas baseada na Storm"""
    if df.empty:
        return df
    
    filtered_data = []
    removed_count = 0
    
    for _, row in df.iterrows():
        proposta = str(row.get('PROPOSTA', '')).strip()
        
        if not proposta or proposta.lower() in ['nan', 'null', '']:
            continue
        
        # Verificar se proposta existe na Storm
        skip_record = False
        for storm_proposta, storm_status in storm_data.items():
            if proposta == storm_proposta or proposta in storm_proposta or storm_proposta in proposta:
                if storm_status in ["PAGO", "CANCELADO"]:
                    skip_record = True
                    removed_count += 1
                    break
        
        if not skip_record:
            filtered_data.append(row.to_dict())
    
    logging.info(f"Duplicatas removidas: {removed_count}")
    return pd.DataFrame(filtered_data)

def format_csv_for_storm(df: pd.DataFrame) -> str:
    """Formatar CSV otimizado para importaÃ§Ã£o na Storm com separador ';'"""
    if df.empty:
        return ""
    
    # Garantir que todas as colunas estÃ£o presentes na ordem correta
    required_columns = [
        "PROPOSTA", "DATA CADASTRO", "BANCO", "ORGAO", "CODIGO TABELA",
        "TIPO DE OPERACAO", "NUMERO PARCELAS", "VALOR PARCELAS", "VALOR OPERACAO",
        "VALOR LIBERADO", "VALOR QUITAR", "USUARIO BANCO", "CODIGO LOJA",
        "SITUACAO", "DATA DE PAGAMENTO", "CPF", "NOME", "DATA DE NASCIMENTO",
        "TIPO DE CONTA", "TIPO DE PAGAMENTO", "AGENCIA CLIENTE", "CONTA CLIENTE",
        "FORMALIZACAO DIGITAL", "TAXA", "OBSERVACOES"
    ]
    
    # Reordenar colunas
    df_ordered = df.reindex(columns=required_columns, fill_value="")
    
    # Limpar dados e garantir formataÃ§Ã£o consistente
    for col in df_ordered.columns:
        df_ordered[col] = df_ordered[col].astype(str).str.strip()
        df_ordered[col] = df_ordered[col].replace(['nan', 'None', 'null', 'NaN'], '')
    
    # ðŸ”§ FIX: Corrigir formataÃ§Ã£o do CPF digitador (USUARIO BANCO) no relatÃ³rio final
    if "USUARIO BANCO" in df_ordered.columns:
        def format_cpf_usuario_banco(cpf_str):
            """Formatar CPF do digitador no padrÃ£o XXX.XXX.XXX-XX"""
            if not cpf_str or cpf_str in ['', '0', '000.000.000-00']:
                return '000.000.000-00'
            
            # Remover qualquer formataÃ§Ã£o existente e extrair apenas nÃºmeros
            cpf_digits = ''.join(filter(str.isdigit, str(cpf_str)))
            
            # Se tem mais de 11 dÃ­gitos (caso SANTANDER), pegar apenas os primeiros 11
            if len(cpf_digits) >= 11:
                cpf_clean = cpf_digits[:11]
                # Formatar no padrÃ£o brasileiro
                return f"{cpf_clean[:3]}.{cpf_clean[3:6]}.{cpf_clean[6:9]}-{cpf_clean[9:11]}"
            elif len(cpf_digits) == 11:
                # CPF jÃ¡ tem 11 dÃ­gitos, apenas formatar
                return f"{cpf_digits[:3]}.{cpf_digits[3:6]}.{cpf_digits[6:9]}-{cpf_digits[9:11]}"
            else:
                # CPF invÃ¡lido, retornar original
                return str(cpf_str)
        
        df_ordered["USUARIO BANCO"] = df_ordered["USUARIO BANCO"].apply(format_cpf_usuario_banco)
    
    # Formatar datas para DD/MM/YYYY (padrÃ£o brasileiro)
    date_columns = ["DATA CADASTRO", "DATA DE PAGAMENTO", "DATA DE NASCIMENTO"]
    for date_col in date_columns:
        if date_col in df_ordered.columns:
            df_ordered[date_col] = df_ordered[date_col].apply(lambda x: format_date_to_brazilian(x))
    
    # Usar separador ';' como solicitado
    return df_ordered.to_csv(index=False, sep=';', encoding='utf-8', lineterminator='\n')

def format_date_to_brazilian(date_str: str) -> str:
    """Converte data para formato DD/MM/YYYY"""
    if not date_str or date_str in ['', 'nan', 'None', 'null']:
        return ""
    
    date_str = str(date_str).strip()
    
    # JÃ¡ estÃ¡ no formato DD/MM/YYYY
    if '/' in date_str and len(date_str.split('/')) == 3:
        parts = date_str.split('/')
        if len(parts[0]) == 2 and len(parts[2]) == 4:  # DD/MM/YYYY
            return date_str
    
    # Tentar converter de outros formatos comuns
    try:
        # Formato YYYY-MM-DD
        if '-' in date_str:
            date_obj = pd.to_datetime(date_str, format='%Y-%m-%d', errors='coerce')
            if pd.notna(date_obj):
                return date_obj.strftime('%d/%m/%Y')
        
        # Formato DD/MM/YY (ano com 2 dÃ­gitos)
        if '/' in date_str:
            parts = date_str.split('/')
            if len(parts) == 3 and len(parts[2]) == 2:
                day, month, year = parts
                year = '20' + year if int(year) < 50 else '19' + year
                return f"{day.zfill(2)}/{month.zfill(2)}/{year}"
        
        # Tentar parsing automÃ¡tico do pandas
        date_obj = pd.to_datetime(date_str, errors='coerce', dayfirst=True)
        if pd.notna(date_obj):
            return date_obj.strftime('%d/%m/%Y')
    except:
        pass
    
    return date_str  # Retorna original se nÃ£o conseguir converter

@api_router.post("/upload-storm")
async def upload_storm_report(file: UploadFile = File(...)):
    """Upload e processamento do relatÃ³rio da Storm"""
    try:
        if not file.filename:
            raise HTTPException(status_code=400, detail="Nome do arquivo Ã© obrigatÃ³rio")
        
        content = await file.read()
        if len(content) == 0:
            raise HTTPException(status_code=400, detail="Arquivo estÃ¡ vazio")
        
        df = read_file_optimized(content, file.filename)
        
        # Detectar tipo de banco
        bank_type = detect_bank_type_enhanced(df, file.filename)
        if bank_type != "STORM":
            raise HTTPException(status_code=400, detail="Este nÃ£o Ã© um arquivo da Storm vÃ¡lido")
        
        # Processar dados da Storm
        storm_proposals = process_storm_data_enhanced(df)
        
        # Armazenar globalmente
        global storm_data_global
        storm_data_global = storm_proposals
        
        return {
            "message": "Arquivo da Storm processado com sucesso",
            "total_proposals": len(storm_proposals),
            "paid_cancelled": sum(1 for status in storm_proposals.values() if status in ["PAGO", "CANCELADO"]),
            "filename": file.filename
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Erro ao processar Storm: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@api_router.post("/process-banks")
async def process_bank_reports(files: List[UploadFile] = File(...)):
    """Processamento aprimorado de mÃºltiplos relatÃ³rios de bancos"""
    try:
        if not storm_data_global:
            raise HTTPException(status_code=400, detail="Primeiro faÃ§a upload do relatÃ³rio da Storm")
        
        if not files or len(files) == 0:
            raise HTTPException(status_code=400, detail="Nenhum arquivo de banco foi enviado")
        
        job_id = str(uuid.uuid4())
        job = ProcessingJob(id=job_id, total_records=0, processed_records=0)
        processing_jobs[job_id] = job
        
        all_final_data = []
        bank_summaries = []
        
        for file in files:
            try:
                if not file.filename:
                    continue
                
                content = await file.read()
                if len(content) == 0:
                    continue
                
                logging.info(f"Processando arquivo: {file.filename}")
                
                # Ler arquivo com tratamento de erros melhorado
                try:
                    df = read_file_optimized(content, file.filename)
                except Exception as read_error:
                    logging.error(f"âŒ Erro ao ler arquivo {file.filename}: {str(read_error)}")
                    continue
                
                # Validar DataFrame
                if df is None or df.empty:
                    logging.warning(f"âš ï¸ Arquivo {file.filename} resultou em DataFrame vazio")
                    continue
                
                # Limpar DataFrame - remover linhas completamente vazias
                df = df.dropna(how='all')
                
                if df.empty:
                    logging.warning(f"âš ï¸ Arquivo {file.filename} nÃ£o contÃ©m dados vÃ¡lidos apÃ³s limpeza")
                    continue
                
                # Detectar tipo de banco
                try:
                    bank_type = detect_bank_type_enhanced(df, file.filename)
                except Exception as detect_error:
                    logging.error(f"âŒ Erro ao detectar banco em {file.filename}: {str(detect_error)}")
                    continue
                
                if bank_type == "STORM":
                    continue  # Pular arquivos da Storm
                
                logging.info(f"âœ… Banco detectado: {bank_type}, Registros originais: {len(df)}, Colunas: {len(df.columns)}")
                
                # DEBUG: Log adicional para PAULISTA
                if 'AF5EEBB7' in file.filename or 'paulista' in file.filename.lower():
                    logging.error(f"ðŸ” DEBUG PAULISTA: Arquivo={file.filename}, Banco detectado={bank_type}")
                    logging.error(f"ðŸ” DEBUG PAULISTA: Primeiras colunas: {list(df.columns)[:10]}")
                    if not df.empty:
                        first_row = df.iloc[0].to_dict()
                        logging.error(f"ðŸ” DEBUG PAULISTA: Primeira linha: {first_row}")
                
                # Mapear para formato final
                if bank_type == "PAULISTA":
                    logging.error(f"ðŸ¦ PAULISTA: Chamando map_to_final_format com {len(df)} linhas")
                
                mapped_df, mapped_count = map_to_final_format(df, bank_type)
                
                if mapped_df.empty:
                    logging.warning(f"Nenhum dado mapeado para {file.filename}")
                    continue
                
                # Remover duplicatas baseado na Storm
                original_count = len(mapped_df)
                filtered_df = remove_duplicates_enhanced(mapped_df, storm_data_global)
                duplicates_removed = original_count - len(filtered_df)
                
                logging.info(f"ApÃ³s remoÃ§Ã£o de duplicatas: {len(filtered_df)} registros")
                
                if not filtered_df.empty:
                    all_final_data.append(filtered_df)
                
                # Criar resumo
                status_dist = {}
                if not filtered_df.empty and "SITUACAO" in filtered_df.columns:
                    status_dist = filtered_df["SITUACAO"].value_counts().to_dict()
                
                bank_summaries.append(ReportSummary(
                    bank_name=bank_type,
                    total_records=len(filtered_df),
                    duplicates_removed=duplicates_removed,
                    status_distribution=status_dist,
                    mapped_records=mapped_count,
                    unmapped_records=original_count - mapped_count
                ))
                
            except Exception as e:
                logging.error(f"Erro ao processar arquivo {file.filename}: {str(e)}")
                continue
        
        # Combinar todos os dados
        if not all_final_data:
            logging.error("ðŸš« ERRO CRÃTICO: Nenhum DataFrame vÃ¡lido foi processado!")
            for i, file in enumerate(files):
                logging.error(f"   ðŸ“‚ Arquivo {i+1}: {file.filename}")
            raise HTTPException(status_code=400, detail="Nenhum dado vÃ¡lido foi processado. Verifique se os arquivos tÃªm o formato correto e contÃªm dados vÃ¡lidos.")
        
        final_df = pd.concat(all_final_data, ignore_index=True)
        
        # **FORMATAÃ‡ÃƒO OTIMIZADA PARA STORM COM SEPARADOR ';'**
        csv_content = format_csv_for_storm(final_df)
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8')
        temp_file.write(csv_content)
        temp_file.close()
        
        # Atualizar job
        job.status = "completed"
        job.completed_at = datetime.utcnow()
        job.message = f"Processamento concluÃ­do: {len(final_df)} registros"
        job.total_records = len(final_df)
        job.result_file = temp_file.name
        
        return {
            "job_id": job_id,
            "message": "Processamento concluÃ­do com sucesso",
            "total_records": len(final_df),
            "bank_summaries": [summary.dict() for summary in bank_summaries],
            "download_url": f"/api/download-result/{job_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Erro no processamento: {str(e)}")
        if 'job' in locals():
            job.status = "failed"
            job.message = str(e)
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@api_router.get("/download-result/{job_id}")
async def download_result(job_id: str):
    """Download do resultado processado"""
    if job_id not in processing_jobs:
        raise HTTPException(status_code=404, detail="Job nÃ£o encontrado")
    
    job = processing_jobs[job_id]
    if job.status != "completed":
        raise HTTPException(status_code=400, detail="Processamento ainda nÃ£o concluÃ­do")
    
    if not hasattr(job, 'result_file') or not os.path.exists(job.result_file):
        raise HTTPException(status_code=404, detail="Arquivo de resultado nÃ£o encontrado")
    
    return FileResponse(
        path=job.result_file,
        filename=f"relatorio_final_storm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        media_type='text/csv'
    )

@api_router.get("/processing-status/{job_id}")
async def get_processing_status(job_id: str):
    """Status do processamento"""
    if job_id not in processing_jobs:
        raise HTTPException(status_code=404, detail="Job nÃ£o encontrado")
    
    job = processing_jobs[job_id]
    return job.dict()

@api_router.get("/")
async def root():
    return {"message": "Sistema de Processamento de RelatÃ³rios Financeiros - V6.6.0 Melhorias Completas DIGIO, VCTEX e AVERBAI"}

@api_router.post("/reload-mapping")
async def reload_mapping():
    """Endpoint para recarregar o mapeamento de Ã³rgÃ£os quando novos cÃ³digos sÃ£o adicionados"""
    try:
        success = reload_organ_mapping()
        if success:
            return {
                "message": "âœ… Mapeamento recarregado com sucesso!",
                "total_bancos": len(ORGAN_MAPPING),
                "total_combinacoes": len(DETAILED_MAPPING),
                "total_tabelas": len(TABELA_MAPPING),
                "total_fallback": len(BANK_ORGAN_MAPPING)
            }
        else:
            raise HTTPException(status_code=500, detail="Erro ao recarregar mapeamento")
    except Exception as e:
        logging.error(f"Erro no reload: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@api_router.get("/averbai-codes")
async def get_averbai_codes():
    """Endpoint para listar todos os cÃ³digos AVERBAI reconhecidos pelo sistema"""
    try:
        averbai_codes = {
            "FGTS": [],
            "INSS": [],
            "CREDITO_DO_TRABALHADOR": []
        }
        
        # Buscar todos os cÃ³digos AVERBAI no mapeamento
        for key, details in TABELA_MAPPING.items():
            parts = key.split('|')
            if len(parts) >= 2 and parts[0] == "AVERBAI":
                orgao = parts[1]
                codigo = details.get('codigo_tabela', '')
                tabela = details.get('tabela_banco', '')
                taxa = details.get('taxa_storm', '')
                
                code_info = {
                    "codigo": codigo,
                    "tabela": tabela,
                    "taxa": taxa,
                    "operacao": details.get('operacao_storm', '')
                }
                
                if orgao == "FGTS":
                    averbai_codes["FGTS"].append(code_info)
                elif orgao == "INSS":
                    averbai_codes["INSS"].append(code_info)
                elif orgao == "CRÃ‰DITO DO TRABALHADOR":
                    averbai_codes["CREDITO_DO_TRABALHADOR"].append(code_info)
        
        # Ordenar por cÃ³digo
        for orgao in averbai_codes:
            averbai_codes[orgao] = sorted(averbai_codes[orgao], key=lambda x: int(x["codigo"]) if x["codigo"].isdigit() else 9999)
        
        return {
            "message": "CÃ³digos AVERBAI reconhecidos pelo sistema",
            "total_fgts": len(averbai_codes["FGTS"]),
            "total_inss": len(averbai_codes["INSS"]), 
            "total_clt": len(averbai_codes["CREDITO_DO_TRABALHADOR"]),
            "codes": averbai_codes
        }
        
    except Exception as e:
        logging.error(f"Erro ao listar cÃ³digos AVERBAI: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro interno: {str(e)}")

@api_router.post("/debug-file")
async def debug_file(file: UploadFile = File(...)):
    """Endpoint de debug para testar leitura e detecÃ§Ã£o de arquivos"""
    try:
        content = await file.read()
        
        # Tentar ler arquivo
        try:
            df = read_file_optimized(content, file.filename)
            
            debug_info = {
                "filename": file.filename,
                "file_size": len(content),
                "success": True,
                "rows": len(df),
                "columns": len(df.columns),
                "column_names": list(df.columns)[:20],  # Primeiras 20 colunas
                "first_row_sample": {}
            }
            
            # Pegar primeira linha como exemplo
            if not df.empty:
                first_row = df.iloc[0]
                for col in list(df.columns)[:10]:  # Primeiras 10 colunas
                    debug_info["first_row_sample"][col] = str(first_row[col])[:100]
            
            # Tentar detectar banco
            try:
                bank_type = detect_bank_type_enhanced(df, file.filename)
                debug_info["detected_bank"] = bank_type
            except Exception as detect_error:
                debug_info["detected_bank"] = f"ERRO: {str(detect_error)}"
            
            return debug_info
            
        except Exception as read_error:
            return {
                "filename": file.filename,
                "file_size": len(content),
                "success": False,
                "error": str(read_error),
                "error_type": type(read_error).__name__
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erro no debug: {str(e)}")

# Include the router in the main app
app.include_router(api_router)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=os.environ.get('CORS_ORIGINS', '*').split(','),
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@app.on_event("shutdown")
async def shutdown_db_client():
    client.close()